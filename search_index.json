[["index.html", "Apunts d’Àlgebra Lineal Introducció", " Apunts d’Àlgebra Lineal Marc Masdeu, Albert Ruiz 2023-09-28 Introducció Aquests apunts corresponen a l’assignatura d’Àlgebra Lineal que s’ha fet als cursos 2018/2019 i 2019/2020 del Grau en Matemàtica Computacional i Analítica de Dades a la UAB. És un curs de 52 hores docents on es barregen les classes teòriques i pràctiques. Per a les classes teòriques (i aquests apunts) s’han utilitzat les referències [1] i [2]. Cada capítol acaba amb una selecció d’exercicis proposats, agafant la numeració de [1]. A més a més, aquest curs s’ha complementat amb tres sessions de Sage que mostren aplicacions d’aquests resultats. Encara que aquests apunts intenten ser força autocontinguts, es requereix que l’alumne conegui la resolució de sistemes d’equacions lineals, l’aritmètica bàsica de números i polinomis, i que tingui destresa de càlcul amb expressions algebraiques simbòliques. A tot aquest curs suposem que treballem sobre un cos commutatiu \\(\\K\\) fixat, que podeu pensar és \\(\\Q\\), \\(\\R\\) o \\(\\CC\\). Els elements de \\(\\K\\) els anomenarem nombres o escalars. Les propietats que utilitzarem són: És commutatiu amb la suma: \\(a+b=b+a\\) \\(\\forall a,b\\in \\K\\). És commutatiu amb el producte: \\(ab=ba\\) \\(\\forall a,b\\in \\K\\). La suma té un element neutre que anomenem zero: \\(0+a=a\\) \\(\\forall a\\in\\K\\). El producte té un elements neutre que anomenem u: \\(1a=a\\) \\(\\forall a\\in\\K\\). Tot element \\(a\\in\\K\\) té un invers per la suma que anomenem \\(-a\\): \\(a+(-a)=0\\). Tot element \\(a\\) diferent de zero té un invers per la multiplicació que anomenem \\(1/a\\) o bé \\(a^{-1}\\): \\(a a^{-1}=1\\). Hi ha les propietats associatives a la suma i al producte: \\((a+b)+c=a+(b+c)\\) i \\((ab)c=a(bc)\\) \\(\\forall a,b,c \\in \\K\\). Hi ha la propietat distributiva: \\(a(b+c)=ab+ac\\) \\(\\forall a,b,c \\in \\K\\). També suposem certa familiaritat amb el llenguatge dels conjunts. Si \\(A\\) és un conjunt, escriurem \\(B\\subset A\\) per denotar que \\(B\\) és un subconjunt d’\\(A\\). Escriurem \\(a\\in A\\) per dir que \\(a\\) és un element d’\\(A\\). També escriurem \\(A\\setminus B=\\{a \\in A \\mid a \\not\\in B\\}\\) i llegirem els \\(a\\) que pertanyen a \\(A\\) i que no pertanyen a \\(B\\) (o bé el complementari de \\(B\\) en \\(A\\)). Important: Si aquí sota no hi veieu un \\(O-K\\), heu d’activar l’opció HTML-CSS de MathJax al vostre navegador: \\[ \\begin{xy} \\xymatrix@C=5pt{O\\ar@{-}[r]&amp;K} \\end{xy} \\] References "],["matrius-i-equacions-lineals.html", "Capítol 1. Matrius i equacions lineals 1.1 Matrius 1.2 Operacions amb matrius. Matriu invertible 1.3 Transformacions elementals en matrius 1.4 Criteri d’invertibilitat. Rang d’una matriu 1.5 Resolució de sistemes d’equacions lineals 1.6 Exercicis recomanats", " Capítol 1. Matrius i equacions lineals El contingut d’aquesta secció el podem trobar a [1 Tema 1] i a [2 Tema 2]. 1.1 Matrius Definició 1.1 Si \\(m\\) i \\(n\\) són dos nombres naturals, una matriu \\(m\\times n\\) amb entrades a \\(\\K\\) és una taula rectangular d’elements de \\(\\K\\) amb \\(m\\) files i \\(n\\) columnes. Denotem \\(M_{m\\times n}(\\K)\\) al conjunt de matrius que tenen \\(m\\) files i \\(n\\) columnes i els seus elements són de \\(\\K\\). Denotarem amb lletres majúscules el nom de les matrius i amb la mateixa lletra i subíndexs cadascun dels coeficients: si \\(A\\) és una matriu, anomenarem \\(a_{ij}\\) al nombre de la fila \\(i\\), columna \\(j\\). En el producte de matrius, a vegades utilitzarem la notació \\((AB)_{ij}\\) per a fer referència al coeficient de la posició \\((i,j)\\) després de fer el producte. Exemple 1.1 Si \\(A=\\big(\\begin{smallmatrix} 1 &amp; 3 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 \\end{smallmatrix}\\big) \\in M_{2\\times3}(\\Q)\\), llavors \\(a_{11}=1\\), \\(a_{12}=3\\), \\(a_{13}=0\\), \\(a_{21}=0\\), \\(a_{22}=-1\\) i \\(a_{23}=1\\). A continuació fixem algunes notacions i definicions de casos particulars: Una matriu quadrada és una matriu amb el nombre de columnes igual al nombre de files. Denotarem per \\(M_n(\\K)=M_{n\\times n}(\\K)\\). Un element està a la diagonal d’una matriu quadrada si la posició que ocupa té el mateix nombre de fila que de columna: si la matriu és \\(A\\), els elements de la diagonal són els \\(a_{ii}\\). Una matriu diagonal és una matriu quadrada on els únics elements no nuls estan a la diagonal: \\(A\\), matriu quadrada, és diagonal si \\(a_{ij}=0\\) \\(\\forall i\\neq j\\). La matriu identitat \\(n\\times n\\) és una matriu diagonal on tots els elements de la diagonal valen \\(1\\) (i per tant els altres valen \\(0\\)). La denotem per \\(\\1_n\\) la matriu identitat \\(n\\times n\\). En general, escriurem els vectors per columnes: un vector de \\(\\K^n\\) és una matriu amb \\(n\\) files i \\(1\\) columna. Direm que una matriu quadrada \\(A\\) és triangular superior si tots els coeficients per sota de la diagonal valen \\(0\\), o sigui, \\(a_ {ij}=0\\) si \\(i&gt;j\\). Direm que una matriu quadrada \\(A\\) és triangular inferior si tots els coeficients per sobre de la diagonal valen \\(0\\), o sigui, \\(a_ {ij}=0\\) si \\(i&lt;j\\). Donada una matriu \\(A \\in M_{m\\times n}(\\K)\\), definim la transposada d’\\(A\\) i la denotem per \\(A^T\\) com la matriu de \\(M_{n\\times m}(\\K)\\) que té per columnes les files d’\\(A\\), o sigui, que a la posició \\((i,j)\\) té el coeficient \\(a_{ji}\\). Tenim la propietat: \\[(A^T)^T=A \\,.\\] Diem que una matriu \\(A\\) és simètrica si \\(A=A^T\\) (en particular, ha de ser quadrada). Per exemple, la matriu identitat \\(\\1_n\\) és simètrica. Exemple 1.2 Considerem un sistema d’equacions amb \\(m\\) equacions i \\(n\\) incògnites: \\[\\begin{align*} a_{11}x_1+a_{12}x_2+ \\cdots + a_{1n}x_n &amp;= b_1 \\\\ a_{21}x_1+a_{22}x_2+ \\cdots + a_{2n}x_n &amp;= b_2 \\\\ &amp;\\vdots \\\\ a_{m1}x_1+a_{m2}x_2+ \\cdots + a_{mn}x_n &amp;= b_m \\end{align*}\\] D’aquí podem treure la matriu associada als coeficients del sistema: \\[A= \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}\\] el vector de termes independents: \\[B= \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix}\\] o bé escriure-ho tot en una sola matriu (matriu ampliada), on habitualment separem els termes independents: \\[\\begin{amatrix}{4} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; b_1 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; b_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} &amp; b_m \\end{amatrix}\\] 1.2 Operacions amb matrius. Matriu invertible Considerem \\(\\lambda \\in \\K\\) i \\(A,B \\in M_{m\\times n}(\\K)\\). Les primeres operacions que podem fer són les que corresponen a veure \\(M_{m\\times n} (\\K)\\) com \\(\\K\\)-espai vectorial (més endavant veurem què vol dir): Definim la matriu \\(\\lambda A\\) com la matriu que té per coeficients \\(\\lambda a_{ij}\\). Definim la matriu \\(A+B\\) com la matriu que té per coeficients \\(a_{ij}+b_{ij}\\). Exemple 1.3 \\[\\begin{pmatrix} -1 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\end{pmatrix} + \\begin{pmatrix} 0 &amp; -1 &amp; 2 \\\\ -1 &amp; 3 &amp; 2 \\end{pmatrix} = \\begin{pmatrix} -1 &amp; 0 &amp; 2 \\\\ 0 &amp; 5 &amp; 3 \\end{pmatrix}\\] Exemple 1.4 \\[2 \\begin{pmatrix} -1 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\end{pmatrix} = \\begin{pmatrix} -2 &amp; 2 &amp; 0 \\\\ 2 &amp; 4 &amp; 2 \\end{pmatrix}\\] Aquestes definicions, més les propietats dels elements de \\(\\K\\) impliquen: Existeix una matriu \\(\\0_{mn}\\) complint \\(A+\\0_{mn}=A\\), \\(\\forall A \\in M_{m\\times n}(\\K)\\) (\\(\\0_{mn}\\) té tots els coeficients zero). \\(0 A = \\0_{mn}\\), \\(\\forall A \\in M_{m\\times n}(\\K)\\). \\(A+B=B+A\\), \\(\\forall A,B \\in M_{m\\times n}(\\K)\\). \\(1A=A\\), \\(\\forall A \\in M_{m\\times n}(\\K)\\). \\(\\lambda (\\mu A)= (\\lambda \\mu) A\\), \\(\\forall \\lambda,\\mu \\in \\K\\) i \\(\\forall A \\in M_{m\\times n}(\\K)\\) Abans de la definició del producte de matrius és convenient utilitzar el llenguatge següent: Definició 1.2 Diem que un vector \\(\\vec{w}\\) és combinació lineal de \\(\\{\\vec{v}_1,\\dots,\\vec{v}_n\\}\\) si existeixen escalars \\(\\lambda_1, \\dots , \\lambda_n\\) tals que \\(w=\\lambda_1\\vec{v}_1+\\cdots+\\lambda_n\\vec{v}_n\\). Si un dels vectors de \\(\\{\\vec{v}_1,\\dots,\\vec{v}_n\\}\\) es pot escriure com a combinació lineal dels altres, diem que la família de vectors \\(\\{\\vec{v}_1,\\dots,\\vec{v}_n\\}\\) és linealment dependent. En cas contrari, diem que la família de vectors \\(\\{\\vec{v}_1,\\dots,\\vec{v}_n\\}\\) és linealment independent. Parlarem de files (o columnes) linealment dependents o independents d’una matriu \\(A\\) pensades com a vectors amb tantes components com columnes (o files) tingui \\(A\\). A més, a més, podem definir: Definició 1.3 Si \\(A \\in M_{m\\times n}(\\K)\\), \\(B \\in M_{n\\times r}(\\K)\\) (o sigui, el nombre de columnes de \\(A\\) és igual al nombre de files de \\(B\\)) podem definir el producte \\(AB\\) com la matriu \\(C\\in M_{m\\times r}\\) que té per coeficients: \\[c_{ij}=\\sum_{k=1}^{n} a_{ik}b_{kj} \\,.\\] La matriu \\(C\\) és pot pensar que té per columnes combinacions lineals de columnes de \\(A\\) (\\(B\\) ens diu quines són aquestes combinacions lineals): la columna \\(j\\) de la matriu \\(C\\) és: \\[\\begin{pmatrix} c_{1j}\\\\c_{2j}\\\\ \\vdots \\\\ c_{mj} \\end{pmatrix} = b_{1j} \\begin{pmatrix} a_{11}\\\\a_{21}\\\\ \\vdots \\\\ a_{m1} \\end{pmatrix} + b_{2j} \\begin{pmatrix} a_{12}\\\\a_{22}\\\\ \\vdots \\\\ a_{m2} \\end{pmatrix} + \\cdots + b_{nj} \\begin{pmatrix} a_{1n}\\\\a_{2n}\\\\ \\vdots \\\\ a_{mn} \\end{pmatrix}\\] Anàlogament, la matriu \\(C\\) es pot pensar que té per files combinacions lineals de files de \\(B\\) (\\(A\\) ens diu quines són aquestes combinacions lineals): la fila \\(i\\) de la matriu \\(C\\) és (separem amb comes per a que quedi més clar): \\[(c_{i1},c_{i2},\\cdots,c_{ir})= a_{i1} (b_{11},b_{12},\\dots,b_{1r})+ a_{i2} (b_{21},b_{22},\\dots,b_{2r})+ \\cdots + a_{in} (b_{n1},b_{n2},\\dots,b_{nr})\\] Exemple 1.5 \\[\\begin{pmatrix} -1 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{pmatrix} = \\begin{pmatrix} 2 &amp; 2 \\\\ 12 &amp; 16 \\end{pmatrix}\\] \\[\\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{pmatrix} \\begin{pmatrix} -1 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 5 &amp; 2 \\\\ 1 &amp; 11 &amp; 4 \\\\ 1 &amp; 17 &amp; 6 \\end{pmatrix}\\] Exemple 1.6 Podem escriure el sistema d’equacions \\[\\begin{align*} a_{11}x_1+a_{12}x_2+ \\cdots + a_{1n}x_n &amp;= b_1 \\\\ a_{21}x_1+a_{22}x_2+ \\cdots + a_{2n}x_n &amp;= b_2 \\\\ &amp;\\vdots \\\\ a_{m1}x_1+a_{m2}x_2+ \\cdots + a_{mn}x_n &amp;= b_m \\end{align*}\\] com \\(AX=B\\), on \\[A= \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix},\\quad X= \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\text{ i}\\quad B= \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix}.\\] Proposició 1.1 El producte de matrius té les propietats següents: Element neutre: si \\(A \\in M_{m\\times n}(\\K)\\), \\(\\1_m A = A \\1_n = A\\). Propietat associativa: si \\(A \\in M_{m\\times n}(\\K)\\), \\(B \\in M_{n\\times r}(\\K)\\), \\(C \\in M_{r\\times s}(\\K)\\), llavors \\[(AB)C=A(BC).\\] Distributiva respecte el producte: si \\(A \\in M_{m\\times n}(\\K)\\), \\(B, C \\in M_{n\\times r}(\\K)\\) i \\(D \\in M_{r\\times s}(\\K)\\), llavors \\(A(B+C)=AB+AC\\) i \\((B+C)D=BD+CD\\). Prova. Escriure les fórmules amb els coeficients i surt. Fem la propietat associativa com exemple: volem comparar el coeficient a la posició \\((i,j)\\) d’\\((AB)C\\) amb el d’\\(A(BC)\\), que denotem \\(((AB)C)_{ij}\\) i \\((A(BC))_{ij}\\) respectivament: \\[((AB)C)_{ij}=\\sum_{k=1}^r(AB)_{ik}c_{kj}=\\sum_{k=1}^r(\\sum_{l=1}^na_{il}b_{lk})c_{kj}=\\sum_{k=1}^r\\sum_{l=1}^na_{il}b_{lk}c_{kj}\\] Mentre que: \\[(A(BC))_{ij}=\\sum_{l=1}^na_{il}(BC)_{lj}=\\sum_{l=1}^na_{il}(\\sum_{k=1}^rb_{lk}c_{kj})=\\sum_{l=1}^n\\sum_{k=1}^ra_{il}b_{lk}c_{kj}\\] I els dos resultats són el mateix ja que podem commutar els sumatoris. Observació. El producte de matrius, en general, no és commutatiu (veure l’Exemple 1.5). Proposició 1.2 Si \\(A\\in M_{m\\times n}(\\K)\\) i \\(B\\in M_{n\\times r}(\\K)\\), llavors tenim la relació següent entre productes i transposades \\[(AB)^T=B^T A^T\\] Prova. Escrivim les fórmules dels coeficients: \\[((AB)^T)_{ij}=(AB)_{ji}=\\sum_{k=1}^n a_{jk}b_{ki}\\] mentre que \\[(B^TA^T)_{ij}=\\sum_{k=1}^n (B^T)_{ik}(A^T)_{kj}=\\sum_{k=1}^n b_{ki}a_{jk}\\] i són iguals per la propietat commutativa del producte a \\(\\K\\). Definició 1.4 Si considerem \\(\\vec{v}\\) i \\(\\vec{w}\\) vectors de \\(\\K^n\\), que escrivim com una columna cadascun, definim el producte escalar \\(\\vec{v}\\cdot\\vec{w}\\) com: \\[\\vec{v}\\cdot \\vec{w}=\\vec{v}^T \\vec{w}\\] Per tant, si les coordenades són: \\[\\vec{v}=\\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix}, \\vec{w}=\\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{pmatrix} \\text{ llavors } \\vec{v}\\cdot\\vec{w}=\\sum_{i=1}^n v_iw_i \\,.\\] Volem definir la inversa d’una matriu quadrada, però com que el producte no és commutatiu, hauríem de parlar d’inversa per l’esquerra o per la dreta. Definició 1.5 Diem que una matriu quadrada \\(A \\in M_{n}(\\K)\\) és invertible si existeix una matriu \\(B \\in M_n(\\K)\\) tal que \\(AB=BA=\\1_n\\). Teorema 1.1 Siguin \\(A\\) i \\(B\\) matrius quadrades \\(n\\times n\\). Llavors \\(AB=\\1_n\\) si i només si \\(BA=\\1_n\\). En particular, si això passa aleshores \\(A\\) és invertible i la inversa, que és única, la denotarem \\(A^{-1}\\). Prova. La primera part la demostrarem quan tinguem el concepte de rang d’una matriu (Corol·lari 2.2). Vegem ara que si \\(B\\) és tal que \\(AB=\\1_n\\), i \\(C\\) tal que \\(CA=\\1_n\\), llavors \\(B=C\\): \\[C = C \\1_n = C(AB)=(CA)B=\\1_n B=B \\,.\\] Això implica que la inversa és única: Suposem \\(B&#39;\\) tal que \\(AB&#39;=\\1_n\\), llavors, pel raonament d’abans, \\(B&#39;=C=B\\). Proposició 1.3 Si \\(A \\in M_{n\\times n}(\\K)\\) és invertible, llavors \\(A^T\\) també ho és i \\((A^T)^{-1}=(A^{-1})^T\\). Si \\(A,B \\in M_{n\\times n}(\\K)\\) són matrius invertibles, llavors el producte \\(AB\\) també ho és i \\((AB)^{-1}=B^{-1}A^{-1}\\). Prova. Demostrem primer (a): com que ens proposen una inversa, tant sols cal comprovar que ho és: \\[(A^T)(A^{-1})^T=(A^{-1}A)^T=\\1_n^T=\\1_n \\,.\\] per tant, \\((A^{-1})^T\\) és inversa d’\\(A^T\\) (com que \\(A^T\\) és quadrada, pel Teorema 1.1, tant sols cal comprovar-ho per un dels costats). Demostrem ara (b): com que \\(A\\) i \\(B\\) són invertibles, existeixen les matrius inverses \\(A^{-1}\\) i \\(B^{-1}\\in M_{n\\times n}(\\K)\\) respectivament. Llavors: \\[(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=A\\1_n A^{-1}=A A^{-1}=\\1_n\\] Igual que (a), això ja demostra que \\(B^{-1}A^{-1}\\) és inversa d’\\(AB\\). Observació. D’aquí es dedueix que si \\(A_1, \\dots, A_r \\in M_{n\\times n}(\\K)\\) són invertibles, el seu producte també ho és i \\((A_1\\cdots A_r)^{-1}=A_r^{-1}\\cdots A_1^{-1}\\). 1.3 Transformacions elementals en matrius Considerem una matriu organitzada per files (encara que tot el que farem aquí també es pot fer per columnes) i definim les transformacions elementals: Observació. Si considerem la matriu d’un sistema d’equacions i apliquem qualsevol de les transformacions elementals, no modifiquem les solucions. Observació. Les tres transformacions elementals es poden desfer mitjançant una transformació elemental: Multiplicar una de les files per \\(1/\\lambda\\neq 0\\). Sumar a una de les files \\(-\\mu\\) vegades una altra fila. Intercanviar dues files. Aquestes transformacions elementals es poden fer (i desfer) multiplicant per una matriu invertible \\(P\\) per l’esquerra. Suposem que \\(A\\) és una matriu amb \\(m\\) files: si apliquem aquest canvi a la fila \\(i\\) d’\\(\\1_m\\) tindrem una matriu \\(P\\) on hem modificat l’\\(1\\) de la fila \\(i\\) per \\(\\lambda\\neq 0\\). Llavors \\(PA\\) té els mateixos valors que \\(A\\), però amb la fila \\(i\\) multiplicada per \\(\\lambda\\). En aquest cas, \\(P^{-1}\\) és una matriu identitat amb un \\(1/\\lambda\\) a la posició \\((i,i)\\). si sumem a la fila \\(i\\) de la matriu \\(\\1_n\\) \\(\\mu\\) vegades la fila \\(k\\neq i\\) tindrem una matriu \\(P\\) tal que \\(PA\\) té a la fila \\(i\\) la fila \\(i\\) d’\\(A\\) més \\(\\mu\\) vegades la fila \\(k\\) d’\\(A\\). En aquest cas, \\(P^{-1}\\) és una matriu amb \\(1\\) a la diagonal, \\(0\\) fora, excepte la posició \\((k,i)\\), que val \\(-\\mu\\). si intercanviem les files \\(i\\) i \\(k\\) de la matriu \\(\\1_n\\) (\\(i\\neq k\\)) obtenim una matriu \\(P\\) tal que \\(PA\\) és el resultat d’intercanviar les files \\(i\\) i \\(k\\) d’\\(A\\). En aquest cas, \\(P^{-1}=P\\). Amb aquests raonaments hem demostrat: Proposició 1.4 Si considerem la matriu identitat \\(\\1_m\\) i li apliquem transformacions elementals per files, la matriu \\(P\\) que obtenim és invertible. Si apliquem exactament les mateixes transformacions elementals a una altra matriu \\(A\\in M_{m\\times n}(\\K)\\), la matriu que resulta és exactament \\(PA\\). Definició 1.6 Diem que dues matrius \\(A\\) i \\(B\\) són equivalents per files si es pot passar d’\\(A\\) a \\(B\\) mitjançant transformacions elementals per files. Escriurem \\(A \\sim B\\). Proposició 1.5 Si \\(A\\), \\(B\\) i \\(C\\) són matrius de dimensions iguals, aleshores es té: \\(A \\sim A\\) (reflexiva), \\(A \\sim B\\) si i només si \\(B \\sim A\\) (simètrica), \\(A \\sim B\\) i \\(B \\sim C\\) implica \\(A \\sim C\\) (transitiva). Prova. La primera és T2 per a qualsevol fila i \\(\\mu=0\\). La segona és que la inversa d’una transformació elemental és una transformació elemental (i les composem en ordre invers): si \\(P_1\\), …\\(P_r\\) són les transformacions elementals que apliquem a \\(A\\) per obtenir \\(B\\), resulta que: \\[P_r \\cdots P_1 A=B\\] llavors \\[A= P_1^{-1} \\cdots P_r^{-1} B\\] Però si \\(P_i\\) és una transformació elemental, \\(P_i^{-1}\\) també i per tant \\(B\\sim A\\). La tercera és per definició de \\(\\sim\\), ja que passem de \\(A\\) a \\(C\\) fent primer els canvis elementals que transformen \\(A\\) en \\(B\\) i després els que transformen \\(B\\) en \\(C\\). Observació. Si tenim un conjunt \\(S\\) i una relació \\(\\sim\\) entre els seus elements complint les propietats de la Proposició 1.5 (reflexiva, simètrica i transitiva) diem que és una relació d’equivalència. Definició 1.7 Diem que una matriu \\(A\\) està en forma esglaonada (per files) si compleix que: El primer element no nul de cada fila val \\(1\\), i l’anomenem pivot. Si una fila conté un pivot a la columna \\(j\\), les files superiors també tenen un pivot a una columna \\(j&#39;&lt;j\\). Totes les entrades per sota d’un pivot valen \\(0\\). Diem que una matriu \\(A\\) està en forma reduïda (per files) si està esglaonada i a més compleix que: Si una columna té un pivot, aquest és l’únic element no nul de la seva columna. Exemple 1.7 De les matrius següents: \\[A=\\begin{pmatrix} 1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}, B=\\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}, C=\\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\end{pmatrix} \\text{ i } D = \\begin{pmatrix} 1&amp;2&amp;3\\\\ 0&amp;1&amp;5 \\end{pmatrix}\\] les matrius \\(A\\), \\(B\\) i \\(D\\) estan esglaonades per files (i la matriu \\(C\\) no). De les matrius esglaonades per files, \\(A\\) està en forma reduïda i les altres no: per sobre del segon pivot no hi ha zero. Considerem ara l’algorisme següent (Mètode de Gauss o Mètode de Gauss-Jordan), que aplica canvis elementals per files a una matriu \\(A\\) fins que obtenim una matriu en forma reduïda per files. A aquest mètode també li diem triangular la matriu per files. Com que la matriu té un nombre finit de files, aquest algorisme sempre acaba. A més, acabem de demostrar que: Teorema 1.2 Tota matriu \\(A\\in M_{m\\times n}(\\K)\\) és equivalent a una matriu reduïda. Exemple 1.8 Considerem la matriu: \\[A=\\begin{pmatrix} 0 &amp; 1 &amp; 3 &amp; 0 \\\\ 1 &amp; -2 &amp; -5 &amp; 4\\\\ 2 &amp; -3 &amp; -7 &amp; 7 \\end{pmatrix}\\] Apliquem el canvi T3, canviant la primera fila per la segona, obtenint: \\[\\begin{pmatrix} 1 &amp; -2 &amp; -5 &amp; 4\\\\0 &amp; 1 &amp; 3 &amp; 0 \\\\ 2 &amp; -3 &amp; -7 &amp; 7 \\end{pmatrix}\\] Apliquem el canvi T2, restant a la tercera fila \\(2\\) cops la primera: \\[\\begin{pmatrix} 1 &amp; -2 &amp; -5 &amp; 4\\\\0 &amp; 1 &amp; 3 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 &amp; -1 \\end{pmatrix}\\] Com que a la segona fila, la primera posició ja és \\(1\\), podem utilitzar-la directament de pivot i sumar-la \\(2\\) cops a la primera fila, i restar-la a la tercera: \\[\\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 4\\\\0 &amp; 1 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -1 \\end{pmatrix}\\] Les dues primeres files ja estan en forma reduïda, pel que podem considerar la tercera fila, i multiplicar-la per \\(-1\\) per a que l’única posició no nua sigui un pivot: \\[\\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 4\\\\0 &amp; 1 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] Finalment restem la tercera fila a la primera multiplicada per \\(4\\): \\[\\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 0\\\\0 &amp; 1 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] 1.4 Criteri d’invertibilitat. Rang d’una matriu Considerem primer el resultat següent: Teorema 1.3 Donada una matriu quadrada \\(A \\in M_{n\\times n}(\\K)\\), les condicions següents són equivalents: \\(A\\) és equivalent a \\(\\1_n\\). \\(A\\) és invertible. Prova. Vegem primer (a) implica (b): si \\(A\\) és equivalent a \\(\\1_n\\), existeix \\(P\\) tal que \\(PA=\\1_n\\). Com que \\(P\\) és invertible, existeix \\(P^{-1}\\) i podem fer: \\[PA=\\1_n \\Rightarrow P ^{-1}PA=P^{-1} \\Rightarrow A=P^{-1} \\Rightarrow AP = P^{-1}P=\\1_n\\] per tant \\(P\\) és la inversa d’\\(A\\). Vegem (b) implica (a): suposem que \\(A\\) no és equivalent a \\(\\1_n\\). Llavors, forçosament, quan l’esglaonem hi haurà una fila sense pivot, i per tant, tot zeros. Llavors obtenim \\(PA=A&#39;\\), amb \\(P\\) invertible i \\(A&#39;\\) una matriu que té l’última fila tot zeros. Si \\(A\\) fos invertible, voldria dir que existeix \\(Q\\) tal que \\(AQ=\\1_n\\), llavors, però llavors: \\[AQ=\\1_n \\Rightarrow PAQ=P \\Rightarrow A&#39;Q=P \\Rightarrow A&#39;(QP^{-1})=\\1_n\\] Observem ara que si \\(A&#39;\\) té l’última fila tot zeros \\(A&#39;(QP^{-1})\\) també, i per tant no pot ser \\(\\1_n\\). Observació. Si considerem \\(A \\in M_{n\\times n}(\\K)\\) que es pot subdividir en 4 submatrius: \\[A=\\left(\\begin{array}{c|c} B &amp; C \\\\ \\hline \\0 &amp; D \\end{array}\\right)\\] on la matriu \\(\\0\\) està formada per zeros i toca la diagonal (conté un coeficient amb coordenades \\((i,i)\\)), llavors \\(A\\) no és invertible. Prova. Primer observem que per a que una matriu sigui invertible, l’esglaonament ha de fer que a totes les seves columnes (i files) hi hagi un pivot (es dedueix del Teorema 1.3). Si fos invertible podríem fer els canvis elementals per files i obtenir \\(\\1_n\\). Com que la submatriu \\(\\0\\) toca la diagonal, llavors \\(B\\) té més columnes que files i \\(D\\) més files que columnes. Quan esglaonem la matriu \\(A\\), el pivot de les primeres columnes ha de ser a \\(B\\) (sota hi ha zeros) i per tant no hi pot haver més pivots que files a \\(B\\) (a les columnes de \\(B\\)), per tant hi ha columnes de \\(A\\) sense pivot, i per tant no pot ser invertible. Podem utilitzar el concepte de matriu reduïda equivalent per a definir el rang d’una matriu \\(A\\). Cal tenir en compte que necessitarem demostrar que la definició no depèn de quina matriu reduïda equivalent a \\(A\\) considerem. Proposició 1.6 Si \\(A\\) i \\(B\\) són dues matrius en forma reduïda que són equivalents, llavors \\(A=B\\). Prova. Suposem que \\(A\\) i \\(B\\) són diferents. Considerem submatrius \\(A&#39;\\) i \\(B&#39;\\) formades per la primera columna on difereixin, així com per totes les columnes a l’esquerra d’aquesta que continguin pivots. Observem que \\(A&#39;\\) i \\(B&#39;\\) segueixen essent equivalents, mitjançant les mateixes transformacions elementals associades a \\(A\\) i \\(B\\). Podem interpretar \\(A&#39;\\) i \\(B&#39;\\) com matrius augmentades de sistemes lineals equivalents, posem en \\(r\\) incògnites. Si aquests són compatibles, aleshores necessàriament els termes independents han de coincidir i, per tant obtenim una contradicció. Si aquests dos sistemes són incompatibles, això significa que l’última columna conté zeros en les primeres files (tantes com pivots) i necessàriament tindrem que aquesta última columna tindrà un pivot a l’entrada \\(r\\)-èssima. Per tant, obtenim una nova contradicció. D’aquí és dedueix que donada una matriu \\(A\\in M_{n\\times n}(\\K)\\), existeix una sola matriu reduïda equivalent a \\(A\\), i l’anomenem \\(\\rref(A)\\) (reduced row-echelon form) . Ara ja podem definir el rang d’una matriu: Definició 1.8 Donada una matriu \\(A\\), definim el rang d’\\(A\\) com el nombre de files diferents de zero (igual al nombre de pivots) de \\(\\rref(A)\\). Corol·lary 1.1 Si fem transformacions elementals a una matriu \\(A\\in M_{m\\times n}(\\K)\\), el rang de la matriu resultant és el mateix. Això també ho podem enunciar dient que si \\(P\\in M_{m\\times m}(\\K)\\) és una matriu invertible i \\(A\\in M_{m\\times n}(\\K)\\), llavors \\(\\Rang(A)=\\Rang(PA)\\). Corol·lary 1.2 Una matriu quadrada \\(A\\in M_{n\\times n}(\\K)\\) és invertible si i només si té rang \\(n\\). Prova. Considerem el Teorema 1.3, i per tant és invertible si i només si és equivalent a \\(\\1_n\\), per tant, si i només si té rang \\(n\\). El Mètode de Gauss ens dona una manera de calcular la inversa d’una matriu: suposem que ens donen una matriu \\(A \\in M_{n\\times n}(\\K)\\). Considerem la matriu formada per una matriu identitat \\(n\\times n\\) al costat de la matriu \\(A\\): \\[\\left(\\begin{array}{c|c}A &amp; \\1_n \\end{array}\\right) \\in M_{n\\times 2n}(\\K)\\] Si apliquem canvis per files a aquesta matriu (cada fila té \\(2n\\) coeficients) obtindrem matrius que podem escriure com: \\[\\left(\\begin{array}{c|c}A&#39;&amp; P \\end{array}\\right) \\in M_{n\\times 2n}(\\K)\\] amb \\(A&#39;,P\\in M_{n\\times n}(\\K)\\) i que compleixen que \\(PA=A&#39;\\). Com a cas particular tenim que, si \\(A\\) és invertible, podem arribar a la situació \\[\\begin{align*} (\\#eq:inversa) \\left(\\begin{array}{c|c}\\1_n &amp; P \\end{array}\\right) \\in M_{n\\times 2n}(\\K) \\end{align*}\\] i tindrem que \\(P=A^{-1}\\). Si la matriu \\(A\\) no fos invertible, el Teorema 1.3 ens diu que no seria possible arribar a la situació de l’Equació (??). Exemple 1.9 Considerem la matriu: \\[A=\\begin{pmatrix} 1 &amp; 2 &amp; 6 \\\\ 0 &amp; -1 &amp; -8 \\\\ 5 &amp; 6 &amp; 0 \\end{pmatrix}\\] Escrivim la matriu amb una còpia de la matriu identitat a la dreta: \\[\\left(\\begin{array}{rrr|rrr} 1 &amp; 2 &amp; 6 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; -8&amp; 0 &amp; 1 &amp; 0 \\\\ 5 &amp; 6 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right)\\] Esglaonem la matriu resultant segon el mètode de Gauss: \\[\\left(\\begin{array}{rrr|rrr} 1 &amp; 2 &amp; 6 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; -1 &amp; -8 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; -4 &amp; -30 &amp; -5 &amp; 0 &amp; 1 \\end{array}\\right) \\rightsquigarrow \\left(\\begin{array}{rrr|rrr} 1 &amp; 2 &amp; 6 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 8 &amp; 0 &amp; -1 &amp; 0\\\\ 0 &amp; -4 &amp; -30 &amp; -5 &amp; 0 &amp; 1 \\end{array}\\right)\\] \\[\\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; -10 &amp; 1 &amp; 2 &amp; 0\\\\ 0 &amp;1 &amp; 8 &amp; 0 &amp; -1 &amp; 0\\\\ 0 &amp; 0 &amp; 2 &amp; -5 &amp; -4 &amp; 1 \\end{array}\\right) \\rightsquigarrow \\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 6 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 8 &amp; 0 &amp; -1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; -5/2 &amp; -2 &amp; 1/2 \\end{array}\\right)\\] \\[\\rightsquigarrow\\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 0 &amp; -24 &amp; -18 &amp; 5\\\\ 0 &amp; 1 &amp; 0 &amp; 20 &amp; 15 &amp; -4\\\\ 0 &amp; 0 &amp; 1 &amp; -5/2 &amp; -2 &amp; 1/2 \\end{array}\\right)\\] Per tant: \\[A^{-1}=\\begin{pmatrix} -24 &amp; -18 &amp; 5 \\\\ 20 &amp; 15 &amp; -4 \\\\ -5/2 &amp; -2 &amp; 1/2 \\end{pmatrix}\\] Exercici 1.1 Tot el que hem fet per files té el seu anàleg per columnes. Una manera senzilla d’adaptar totes les definicions i resultats és dir que la transposada compleix la definició per files. Per exemple: La matriu \\(A\\) és en forma reduïda per columnes si \\(A^T\\) és en forma reduïda per files. Enuncieu l’anàleg per columnes de cada resultat que s’ha vist a aquest capítol per files. Demostreu que \\(\\Rang(A)=\\Rang(A^T)\\). Exercici 1.2 Considerem \\(A\\in M_{m\\times n}(\\K)\\) i hi afegim una fila, obtenint \\(A&#39;\\in M_{(m+1)\\times n}(\\K)\\). Demostreu \\(\\Rang(A)=\\Rang(A&#39;)\\) si i només si la fila que hem afegit és combinació lineal de les files d’\\(A\\). Exercici 1.3 Demostreu que el rang d’una matriu \\(A\\) és el nombre màxim de files (o columnes) linealment independents que conté \\(A\\). 1.5 Resolució de sistemes d’equacions lineals Recordem la notació d’un sistema d’equacions: \\[\\begin{align*} a_{11}x_1+a_{12}x_2+ \\cdots + a_{1n}x_n &amp;= b_1 \\\\ a_{21}x_1+a_{22}x_2+ \\cdots + a_{2n}x_n &amp;= b_2 \\\\ &amp;\\vdots \\\\ a_{m1}x_1+a_{m2}x_2+ \\cdots + a_{mn}x_n &amp;= b_m \\end{align*}\\] Que també escrivim com \\(AX=B\\), on \\[A= \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}\\text{, } X= \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\text{ i } B= \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix}\\] Volem esbrinar si el sistema té solució o no, i, en cas de tenir-ne, saber quantes en té i calcular-les. Una primera interpretació és considerar que tenim una solució \\(X\\) i fer el càlcul següent: \\[\\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix} = B = AX = x_1 \\begin{pmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1} \\end{pmatrix} + x_2 \\begin{pmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\end{pmatrix} + \\cdots + x_n \\begin{pmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\end{pmatrix}\\] Per tant, una solució ens dóna \\(B\\) com a combinació lineal de les columnes d’\\(A\\), i al revés: si podem escriure \\(B\\) com a combinació lineal de les columnes d’\\(A\\), tenim una solució. Argumentant amb el rang per columnes, ja tenim un criteri per saber si un sistema d’equacions lineals té solució o no: Proposició 1.7 El sistema d’equacions: \\[\\begin{align*} a_{11}x_1+a_{12}x_2+ \\cdots + a_{1n}x_n &amp;= b_1 \\\\ a_{21}x_1+a_{22}x_2+ \\cdots + a_{2n}x_n &amp;= b_2 \\\\ &amp;\\vdots \\\\ a_{m1}x_1+a_{m2}x_2+ \\cdots + a_{mn}x_n &amp;= b_m \\end{align*}\\] té solució si i només si: \\[\\Rang \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} = \\Rang \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; b_1 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; b_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} &amp; b_m \\end{pmatrix}\\] Prova. Denotem per \\(A\\) la matriu associada al sistema i per \\(\\overline{A}\\) la matriu ampliada. Esglaonem per files la matriu \\(\\overline{A}\\) fins a tenir una matriu reduïda \\(P\\overline{A}\\) i fem exactament les mateixes transformacions a la matriu \\(A\\), obtenint \\(PA\\). Com que les primeres \\(n\\) columnes d’\\(\\overline{A}\\) són precisament les d’\\(A\\), tenim que \\(PA\\) també és una matriu en forma reduïda, i tants sols difereixen de que \\(P\\overline{A}\\) té una columna més. Si \\(\\Rang(A)=\\Rang(\\overline{A})\\), vol dir que a l’última columna de \\(P\\overline{A}\\) no hi ha cap pivot, i per tant es pot escriure l’última columna de \\(P\\overline{A}\\) com a combinació lineal de les \\(n\\) primeres, obtenint una solució del sistema. Si \\(\\Rang(A)\\neq\\Rang(\\overline{A})\\), per força \\(\\Rang(\\overline{A})=\\Rang(A)+1\\) i l’última columna té un pivot. La fila on hi ha el pivot de l’última columna correspon a l’equació \\(0=1\\), que no té solució. El resultat anterior també es pot enunciar dient que si esglaonem per files la matriu ampliada del sistema, no queda cap fila on l’únic element no nul sigui a la columna de termes independents. Com que les transformacions elementals per files a la matriu ampliada no modifiquen les solucions del sistema, suposem que hem esglaonat la matriu inicial del sistema. Llavors, obtenint una matriu reduïda \\(\\overline{A}&#39;\\): El sistema té solució si i només si l’última columna d’\\(\\overline{A}&#39;\\) no té cap pivot. Si el sistema té solució, diem que el sistema és compatible, i quan no té solució, diem que és un sistema incompatible. Suposem que el sistema té solució, llavors cada pivot ens permet aïllar la variable corresponent a la seva columna. Continuem suposant que el sistema té solució: les columnes (de la matriu sense ampliar) que no tenen pivot definiran el que anomenem paràmetres lliures. N’hi haurà tants com \\(k:=n-\\Rang(A)\\), on \\(n\\) és el número de incògnites. En aquest cas direm que la solució té dimensió \\(k\\). En el cas particular que \\(k=0\\) diem que té solució única i diem que és un sistema compatible determinat. Si \\(k&gt;0\\) diem que és un sistema compatible indeterminat amb \\(k\\) paràmetres lliures. Exemple 1.10 Considerem el sistema d’equacions: \\[\\begin{align*} x - y + 2z + 3t &amp;= 21 \\\\ -x+2y+z+5t &amp;= 26\\\\ 3x+y-2z+t &amp;= -9\\\\ 3x+2y+z+9t &amp;= 38 \\end{align*}\\] Considerem la matriu ampliada i esglaonem: \\[\\begin{amatrix}{4} 1 &amp; -1 &amp; 2 &amp; 3 &amp; 21 \\\\ -1 &amp; 2 &amp; 1 &amp; 5 &amp; 26 \\\\ 3 &amp; 1 &amp; -2 &amp; 1 &amp; -9\\\\ 3 &amp; 2 &amp; 1 &amp; 9 &amp; 38 \\end{amatrix} \\rightsquigarrow \\begin{amatrix}{4} 1 &amp; -1 &amp; 2 &amp; 3 &amp; 21 \\\\ 0 &amp; 1 &amp; 3 &amp; 8 &amp; 47 \\\\ 0 &amp; 4 &amp; -8 &amp; -8 &amp; -72\\\\ 0 &amp; 5 &amp; -5 &amp; 0 &amp; -25 \\end{amatrix}\\] \\[\\begin{amatrix}{4} 1 &amp; 0 &amp; 5 &amp; 11 &amp; 68 \\\\ 0 &amp; 1 &amp; 3 &amp; 8 &amp; 47 \\\\ 0 &amp; 0 &amp; -20 &amp; -40 &amp; -260\\\\ 0 &amp; 0 &amp; -20 &amp; -40 &amp; -260 \\end{amatrix}\\rightsquigarrow \\begin{amatrix}{4} 1 &amp; 0 &amp; 5 &amp; 11 &amp; 68 \\\\ 0 &amp; 1 &amp; 3 &amp; 8 &amp; 47 \\\\ 0 &amp; 0 &amp; -20 &amp; -40 &amp; -260\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{amatrix}\\] \\[\\begin{amatrix}{4} 1 &amp; 0 &amp; 5 &amp; 11 &amp; 68 \\\\ 0 &amp; 1 &amp; 3 &amp; 8 &amp; 47 \\\\ 0 &amp; 0 &amp; 1 &amp; 2 &amp; 13\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{amatrix} \\rightsquigarrow \\begin{amatrix}{4} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 3 \\\\ 0 &amp; 1 &amp; 0 &amp; 2 &amp; 8 \\\\ 0 &amp; 0 &amp; 1 &amp; 2 &amp; 13\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{amatrix}\\] Com que el rang de la matriu associada és 3 i el de l’ampliada també, el sistema és compatible. Com que tenim 4 incògnites, té 4-3=1 paràmetre lliures (per tant, sistema compatible indeterminat). Amb l’esglaonament que hem fet, la \\(t\\) és el paràmetre lliure i podem escriure la solució: \\[\\begin{pmatrix} x \\\\ y \\\\ z \\\\ t \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 8 \\\\ 13 \\\\ 0 \\end{pmatrix} +t \\begin{pmatrix} -1 \\\\ -2 \\\\ -2 \\\\ \\phantom{-}1 \\end{pmatrix} \\text{ amb $t\\in\\K$.}\\] Exemple 1.11 Per tal d’aprofitar els càlculs del sistema anterior, tant sols fem una petita modificació a l’últim terme independent: \\[\\begin{align*} x - y + 2z + 3t &amp;= 21 \\\\ -x+2y+z+5t&amp;=26\\\\ 3x+y-2z+t&amp;=-9\\\\ 3x+2y+z+9t&amp;=39 \\end{align*}\\] Considerem la matriu ampliada i esglaonem (són els mateixos passos d’abans, pel que tant sols escrivim la primera i última matriu): \\[\\begin{amatrix}{4} 1 &amp; -1 &amp; 2 &amp; 3 &amp; 21 \\\\ -1 &amp; 2 &amp; 1 &amp; 5 &amp; 26 \\\\ 3 &amp; 1 &amp; -2 &amp; 1 &amp; -9\\\\ 3 &amp; 2 &amp; 1 &amp; 9 &amp; 38 \\end{amatrix} \\rightsquigarrow \\begin{amatrix}{4} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 3 \\\\ 0 &amp; 1 &amp; 0 &amp; 2 &amp; 8 \\\\ 0 &amp; 0 &amp; 1 &amp; 2 &amp; 13\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{amatrix}\\] si som estrictes en el concepte de reduïda, falta utilitzar l’\\(1\\) de l’última fila per a posar zeros a l’última columna. En qualsevol cas, veiem que la matriu ampliada té rang 4, mentre que l’associada té rang 3, pel que el sistema és incompatible. Considerem ara el cas particular en que \\(B=\\0_m\\) (un vector format per zeros). Definició 1.9 Diem que el sistema d’equacions lineals \\(AX=B\\) és homogeni si \\(B=\\0_m\\). Si \\(AX=B\\) és un sistema, parlem de sistema homogeni associat al sistema \\(AX=\\0_m\\). Tenim els resultats següents: Si el sistema és homogeni, llavors \\(X=\\0_n\\) és una solució, per tant el sistema és compatible. Si \\(X\\) és solució del sistema homogeni i \\(\\lambda\\in\\K\\), llavors \\(\\lambda X\\) també és solució del sistema: si ho pensem com a multiplicació de matrius, tenim la igualtat \\(A(\\lambda X)=\\lambda (AX)=\\lambda \\0_m=\\0_m\\), per tant \\(\\lambda X\\) també és solució. Si \\(X\\) i \\(Y\\) són solucions d’un sistema homogeni, llavors \\(X+Y\\) també és solució: tornem a escriure-ho en forma matricial: \\(A(X+Y)=AX+AY=\\0_m+\\0_m=\\0_m\\). Si \\(AX=B\\) és un sistema, amb \\(X\\) i \\(Y\\) una solucions, llavors \\(X-Y\\) és una solució del sistema homogeni associat: \\(A(X-Y)=AX-AY=B-B=\\0_m\\). Si \\(AX=B\\) és un sistema i \\(X\\) una solució particular, qualsevol altre solució \\(Y\\) es pot escriure com \\(Y=X+Z\\), amb \\(Z\\) solució del sistema homogeni associat: això es dedueix de l’apartat anterior: si \\(X\\) i \\(Y\\) són solucions, llavors \\(Z=Y-X\\) és solució de l’homogeni. Si \\(X\\) és solució del sistema i \\(Z\\) de l’homogeni associat, llavors \\(AY=A(X+Z)=AX+AZ=B+\\0_m=B\\). Exemple 1.12 Considerem el sistema d’equacions de l’Exemple 1.10: \\[\\begin{align*} x + y + 2z + 3t &amp;= 21 \\\\ -x+2y+z+5t&amp;=26\\\\ 3x+y-2z+t&amp;=-9\\\\ 3x+2y+z+9t&amp;=38 \\end{align*}\\] Veiem que el sistema homogeni associat és \\[\\begin{align*} x + y + 2z + 3t &amp;= 0\\\\ -x+2y+z+5t&amp;=0\\\\ 3x+y-2z+t&amp;=0\\\\ 3x+2y+z+9t&amp;=0 \\end{align*}\\] i té per solució \\[\\begin{pmatrix} x \\\\ y \\\\ z \\\\ t \\end{pmatrix} = t \\begin{pmatrix} -1 \\\\ -2 \\\\ -2 \\\\ \\phantom{-}1 \\end{pmatrix} \\text{ amb $t\\in\\K$.}\\] Tenint en compte que una solució particular és \\((x,y,z)=(3,8,13)\\), podem escriure: \\[\\begin{pmatrix} x \\\\ y \\\\ z \\\\ t \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 8 \\\\ 13 \\\\ 0 \\end{pmatrix}+ t \\begin{pmatrix} -1 \\\\ -2 \\\\ -2 \\\\ \\phantom{-}1 \\end{pmatrix} \\text{ amb $t\\in\\K$.}\\] 1.6 Exercicis recomanats Els exercicis que segueixen són útils per practicar el material presentat. La numeració és la de [1]. Secció 1.1: 6, 12, 16. Secció 1.2: 10, 18. Secció 1.3: 6, 8, 18, 20, 24, 28. References "],["espais-vectorials-i-aplicacions-lineals.html", "Capítol 2. Espais vectorials i aplicacions lineals 2.1 Matrius com a aplicacions lineals 2.2 Aplicacions lineals i geometria 2.3 Subespais, generadors i bases 2.4 Suma i intersecció de subespais vectorials 2.5 Aplicacions injectives, exhaustives i bijectives 2.6 Coordenades de vectors 2.7 Espais vectorials 2.8 Exercicis recomanats", " Capítol 2. Espais vectorials i aplicacions lineals El contingut d’aquesta secció també es pot trobar a [1 Secció 2.2] i [2 Temes 2, 3, 4]. 2.1 Matrius com a aplicacions lineals En aquest capítol considerarem funcions de l’espai \\(\\K^n\\) a l’espai \\(\\K^m\\). Recordem que aquests espais els hem definit com \\(\\K^n=M_{n\\times 1}(\\K)\\) (i anàlogament per \\(\\K^m\\)), i els seus elements els anomenem vectors. Així, una funció \\(f\\) assigna a cada vector \\(\\vec x\\in \\K^n\\) un vector imatge \\(f(\\vec x)\\in \\K^m\\). Un primer exemple ens els donen les matrius: Exemple 2.1 Considerem una matriu \\(A\\in M_{m\\times n}(\\K)\\). Aleshores l’aplicació associada a \\(A\\) és \\(f_A\\), definida com: \\[f_A(\\vec x) = A\\vec x.\\] Tal i com hem remarcat anteriorment, l’aplicació \\(f_A\\) assigna a cada vector \\(\\vec x\\) una combinació lineal de les columnes d’\\(A\\), on els coeficients venen donats per \\(\\vec x\\). De les propietats del producte de matrius, sabem que \\(f_A\\) satisfà: \\(f_A(\\vec x+\\vec y) = f_A(\\vec x) + f_A(\\vec y)\\) per a tot \\(\\vec x, \\vec y \\in \\K^n\\), i \\(f_A(\\lambda \\vec x) = \\lambda f_A(\\vec x)\\) per a tot \\(\\lambda\\in\\K\\) i tot \\(\\vec x\\in \\K^n\\). Podem prendre l’aplicació \\(f_A\\) com a model de les aplicacions que volem considerar en aquest curs: Definició 2.1 Diem que una aplicació \\(f\\colon \\K^n\\to \\K^m\\) és lineal si satisfà \\(f(\\vec x+\\vec y) = f(\\vec x) + f(\\vec y)\\) per a tot \\(\\vec x, \\vec y \\in \\K^n\\), i \\(f(\\lambda \\vec x) = \\lambda f(\\vec x)\\) per a tot \\(\\lambda\\in\\K\\) i tot \\(\\vec x\\in \\K^n\\). Proposició 2.1 Una aplicació \\(f\\colon \\K^n\\to \\K^m\\) és lineal si i només si existeix una matriu \\(A\\in M_{m\\times n}(\\K)\\) tal que \\(f=f_A\\). Prova. Ja hem vist que si \\(f=f_A\\) aleshores \\(f\\) és lineal. Recíprocament, suposem que \\(f\\) és lineal. Aleshores, com que \\[\\begin{pmatrix} x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_n \\end{pmatrix} =x_1\\begin{pmatrix} 1\\\\0\\\\0\\\\\\vdots\\\\0 \\end{pmatrix} + x_2\\begin{pmatrix} 0\\\\1\\\\0\\\\\\vdots\\\\0 \\end{pmatrix} +\\cdots+ x_n \\begin{pmatrix} 0\\\\0\\\\\\vdots\\\\0\\\\1 \\end{pmatrix},\\] el valor \\(f(\\vec x)\\) es pot calcular com \\[f\\begin{pmatrix} x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_n \\end{pmatrix} =x_1f\\begin{pmatrix} 1\\\\0\\\\0\\\\\\vdots\\\\0 \\end{pmatrix} + x_2f\\begin{pmatrix} 0\\\\1\\\\0\\\\\\vdots\\\\0 \\end{pmatrix} +\\cdots+ x_n f\\begin{pmatrix} 0\\\\0\\\\\\vdots\\\\0\\\\1 \\end{pmatrix}.\\] Definim la matriu \\(A\\) com \\[A=\\begin{pmatrix} \\mid &amp; \\mid &amp;\\mid &amp; \\mid\\\\ f(\\vec e_1) &amp; f(\\vec e_2) &amp;\\cdots&amp;f(\\vec e_n)\\\\ \\mid &amp; \\mid &amp;\\mid &amp; \\mid \\end{pmatrix}\\] on escrivim \\(e_i\\) com el vector format per zeros excepte en la posició \\(i\\), on hi ha un \\(1\\). Aleshores, per la definició de producte d’una matriu per un vector, tenim \\(f(\\vec x) = A\\vec x\\) per a tot \\(\\vec x\\in \\K^n\\), com volíem veure. És important remarcar com hem construït la matriu associada a una aplicació lineal \\(f\\) en la demostració anterior: donada \\(f\\colon \\K^n\\rightarrow \\K^m\\), la seva matriu associada és \\[A=\\begin{pmatrix} \\mid &amp; \\mid &amp;\\mid &amp; \\mid\\\\ f(\\vec e_1) &amp; f(\\vec e_2) &amp;\\cdots&amp;f(\\vec e_n)\\\\ \\mid &amp; \\mid &amp;\\mid &amp; \\mid \\end{pmatrix}.\\] Els vectors \\(\\vec e_i\\) sovint s’anomenen vectors estàndard: \\[\\vec e_i = \\begin{pmatrix} 0\\\\\\vdots\\\\0\\\\1\\\\0\\\\\\vdots\\\\0 \\end{pmatrix} \\begin{matrix} \\phantom{0}\\\\\\phantom{\\vdots}\\\\\\phantom{0}\\\\(\\textrm{posició } i)\\\\\\phantom{0}\\\\\\phantom{\\vdots}\\\\\\phantom{0} \\end{matrix}\\] Com a primera aplicació, considerem ara dues aplicacions lineals \\[\\K^n \\stackrel{g}{\\rightarrow} \\K^r \\stackrel{f}{\\rightarrow} \\K^m,\\] i suposem que \\(f\\) té matriu \\(A\\) i \\(g\\) té matriu \\(B\\). Considerem la composició \\(f\\circ g\\), que és l’aplicació que envia \\(\\vec x\\in \\K^n\\) a \\((f\\circ g)(\\vec x) = f(g(\\vec x))\\). Per definició, sabem que per a tot \\(\\vec x\\) tenim \\[(f\\circ g)(\\vec x) = f(g(\\vec x)) = A(B\\vec x) = (AB)\\vec x.\\] Per tant, obtenim que \\(f\\circ g\\) és una aplicació lineal, amb matriu associada \\(AB\\). Per tant, el producte de matrius “codifica” la composició d’aplicacions lineals. Exemple 2.2 Donades tres aplicacions lineals \\[\\K^t\\stackrel{h}{\\rightarrow}\\K^n \\stackrel{g}{\\rightarrow} \\K^r \\stackrel{f}{\\rightarrow} \\K^m\\] amb matrius respectives \\(f=f_A\\), \\(g=f_B\\), \\(h=f_C\\), el fet que la composició d’aplicacions és associativa (per definició) implica que el producte de matrius és associatiu, sense necessitat de fer cap càlcul: la matriu de la composició \\(f\\circ g \\circ h\\) és \\(A(BC)\\) i \\((AB)C\\) a la vegada, i per tant han de coincidir. 2.2 Aplicacions lineals i geometria En aquesta § veurem algunes aplicacions lineals del pla \\(\\R^2\\) i aprendrem a reconèixer com actuen a partir de la forma de la seva matriu. Podem visualitzar aquestes transformacions per exemple a https://shadanan.github.io/MatVis/. 2.2.1 Homotècies Considerem una matriu \\[\\begin{pmatrix} k &amp; 0\\\\0 &amp; k\\end{pmatrix}, k\\in \\R.\\] Aquesta matriu envia el vector \\(\\vec x\\) a \\(k\\vec x\\). Per tant, multiplica els vectors per un escalar. Si \\(|k|&gt;1\\), es tracta d’una dilació i, si \\(|k|&lt; 1\\) es tracta d’una contracció. Si \\(k\\) és negatiu, aleshores també canvia el signe (es fa primer una simetria respecte l’origen). En general, aquestes transformacions s’anomenen homotècies. Una homotècia de factor 1.7. 2.2.2 Projeccions ortogonals Considerem la matriu \\(\\begin{pmatrix}1&amp;0\\\\0&amp;0\\end{pmatrix}\\). Aquesta matriu envia el vector \\(\\vec x = \\begin{pmatrix}x\\\\y\\end{pmatrix}\\) a \\(\\begin{pmatrix}x\\\\0\\end{pmatrix}\\). És a dir, projecta el vector sobre l’eix de les \\(x\\). Més en general, considerem una recta \\(\\ell\\) del pla, que talli l’origen. Qualsevol vector \\(\\vec x\\) es pot escriure de manera única com \\(\\vec x = \\vec x^{\\parallel} + \\vec x^{\\perp}\\), on el vector \\(\\vec x^\\parallel\\) és sobre la recta \\(\\ell\\), i \\(\\vec x^\\perp\\) és perpendicular a \\(\\ell\\). Definició 2.2 La projecció ortogonal respecte la recta \\(\\ell\\) és l’aplicació \\(\\operatorname{proj}_\\ell\\) definida com \\[\\operatorname{proj}_\\ell(\\vec x) = \\vec x^\\parallel.\\] Observem que \\(\\operatorname{proj}_\\ell\\) és una aplicació lineal (exercici). Proposició 2.2 Sigui \\(\\vec v\\) un vector paral·lel a la recta \\(\\ell\\). Aleshores: \\[\\operatorname{proj}_\\ell(\\vec x) = \\frac{\\vec x\\cdot \\vec v}{\\vec v\\cdot \\vec v} \\vec v.\\] També podem trobar una fórmula per la matriu de la projecció ortogonal. Proposició 2.3 Sigui \\(\\vec v = \\begin{pmatrix}v_1\\\\v_2\\end{pmatrix}\\) un vector paral·lel a la recta \\(\\ell\\). Aleshores la matriu de \\(\\operatorname{proj}_\\ell\\) és \\[\\frac{1}{v_1^2 + v_2^2}\\begin{pmatrix}v_1^2&amp;v_1v_2\\\\v_1v_2&amp;v_2^2\\end{pmatrix}\\] 2.2.3 Reflexions Considerem com abans una recta \\(\\ell\\) que passi per l’origen. Volem definir la reflexió respecte la recta \\(\\ell\\). És fàcil veure que \\[\\operatorname{ref}_\\ell(\\vec x) = \\vec x^\\parallel - \\vec x^\\perp.\\] Com que \\(\\vec x = \\vec x^\\parallel + \\vec x^\\perp\\), podem escriure \\[\\vec x^\\parallel - \\vec x^\\perp = 2\\vec x^\\parallel - \\vec x.\\] Per tant, \\[\\operatorname{ref}_\\ell = 2\\operatorname{proj}_\\ell - \\operatorname{id}.\\] La matriu corresponent és fàcil de trobar a partir d’aquesta fórmula: Proposició 2.4 Sigui \\(\\vec v = \\begin{pmatrix}v_1\\\\v_2\\end{pmatrix}\\) un vector paral·lel a la recta \\(\\ell\\). Aleshores la matriu de \\(\\operatorname{ref}_\\ell\\) és \\[\\frac{1}{v_1^2 + v_2^2}\\begin{pmatrix}v_1^2-v_2^2&amp;2v_1v_2\\\\2v_1v_2&amp;v_2^2-v_1^2\\end{pmatrix}.\\] Fixem-nos que la matriu de \\(\\operatorname{ref}_\\ell\\) té la forma \\(\\begin{pmatrix}a&amp;b\\\\b&amp;-a\\end{pmatrix}\\), i que a més les entrades satisfan \\(a^2 + b^2 = 1\\). De fet, es pot veure que qualsevol matriu d’aquesta forma és la matriu d’una reflexió. Una reflexió respecte una recta. També hi podem observar la projecció a la mateixa recta. 2.2.4 Rotacions Les matrius de la forma \\[\\begin{pmatrix} \\cos\\theta&amp;-\\sin\\theta\\\\ \\sin\\theta&amp;\\cos\\theta \\end{pmatrix}\\] corresponen a una rotació d’angle \\(\\theta\\). Observem que aquesta matriu és de la forma \\[\\begin{pmatrix} a&amp;-b\\\\b&amp;a \\end{pmatrix},\\quad a^2+b^2=1\\] i totes les matrius d’aquesta forma són rotacions. Una rotació de \\(30^\\circ\\). Exercici 2.1 Considereu la matriu de rotació d’angle \\(\\theta_1\\) i la de rotació d’angle \\(\\theta_2\\). Descriviu la composició d’aquestes dues aplicacions i, fent servir el producte de matrius, redescobriu les fórmules per la duplicació d’angles que heu vist a secundària. Proposició 2.5 Una matriu de la forma \\(\\begin{pmatrix}a&amp;-b\\\\b&amp;a\\end{pmatrix}\\) és la composició d’una homotècia de magnitud \\(k=\\sqrt{a^2+b^2}\\) amb una rotació d’angle \\(\\arctan(b/a)\\) (si \\(a\\) és positius) o \\(\\arctan(b/a) + \\pi\\) (si \\(a\\) és negatiu). 2.2.5 Lliscaments Per últim estudiarem les matrius de la forma \\(\\begin{pmatrix} 1&amp;k\\\\0&amp;1\\end{pmatrix}\\), on \\(k\\in \\R\\). Aquestes matrius fixen el vector \\(\\vec e_1\\), mentre envien el vector \\(\\vec e_2\\) a \\(\\begin{pmatrix}k\\\\1\\end{pmatrix}\\). Geomètricament, enviarien un quadrat en el pla a un rombe, on la base es manté fixa però els costats verticals s’esbiaixen. Direm que és un lliscament horitzontal. De manera anàloga, la matriu \\(\\begin{pmatrix}1&amp;0\\\\k&amp;1\\end{pmatrix}\\) representa un lliscament vertical. Lliscament horitzontal de valor 0.5 i lliscament vertical de valor -1. Exercici 2.2 Calculeu, en els casos que es pugui, les inverses de les matrius corresponents a homotècies, projeccions ortogonals, reflexions i lliscaments. 2.3 Subespais, generadors i bases Tornem ara a considerar \\(\\K^n\\) i les aplicacions lineals \\(f\\colon \\K^n\\to\\K^m\\). Definició 2.3 Diem que un subconjunt \\(W\\subset \\K^n\\) és un subespai vectorial si \\(W\\) compleix que: \\(\\vec 0 \\in W\\), \\(\\vec u+\\vec v \\in W\\) per a tot \\(\\vec u\\) i \\(\\vec v\\) de \\(W\\), i \\(\\lambda \\vec u\\in W\\) per a tot \\(\\vec u\\in W\\) i \\(\\lambda\\in\\K\\). Exemple 2.3 A l’\\(\\R\\)-espai vectorial \\(\\R^3\\) hi considerem \\(F=\\{(x,y,z) \\in \\R^3 \\mid z=0\\}\\). Podem comprovar que és un subespai vectorial. Més en general, si tenim un sistema d’equacions lineals homogeni amb \\(n\\) incògnites, podem identificar una solució \\((x_1,\\dots,x_n)\\) (l’hem escrit com una fila per comoditat) amb un vector de \\(\\K^n\\) i les solucions formen un subespai vectorial de \\(\\K^n\\). Com a cas particular de subespai vectorial tenim el següent: Definició 2.4 Si \\(\\{\\vec v_1, \\dots \\vec v_m\\}\\subset \\K^n\\), definim el subespai generat per \\(\\{\\vec v_1, \\dots \\vec v_m\\}\\) com els vectors de \\(\\K^n\\) que es poden escriure com a combinació lineal de \\(\\{\\vec v_1, \\dots \\vec v_m\\}\\). Denotem aquest subconjunt com \\(\\langle \\vec v_1, \\dots \\vec v_m \\rangle\\), i per tant tenim: \\[\\langle \\vec v_1, \\dots \\vec v_m \\rangle = \\{\\lambda_1 \\vec v_1 + \\cdots + \\lambda_m\\vec v_m \\mid \\lambda_i\\in\\K \\}\\] Observació. Si \\(\\{\\vec v_1, \\dots \\vec v_m\\}\\subset \\K^n\\), llavors \\(\\langle \\vec v_1, \\dots \\vec v_m \\rangle\\) és un subespai vectorial. Prova. Per simplificar la notació, definim \\[W=\\langle \\vec v_1, \\dots \\vec v_m \\rangle .\\] Cal veure que si \\(\\vec u, \\vec w\\) pertanyen a \\(W\\), llavors \\(\\vec u+\\vec w\\) també hi pertany. Per definició d’\\(W\\), tenim que existeixen escalars \\(\\lambda_1, \\dots ,\\lambda_m\\) i \\(\\mu_1,\\dots,\\mu_m\\) tals que \\(\\vec u= \\lambda_1 \\vec v_1+\\cdots +\\lambda_m\\vec v_m\\) i \\(\\vec w=\\mu_1\\vec v_1+\\cdots +\\mu_m\\vec v_m\\). Llavors, tenim que: \\[\\vec u+\\vec w = (\\lambda_1+\\mu_1)\\vec v_1 + \\cdots + (\\lambda_m+\\mu_m)\\vec v_m ,\\] obtenint que \\(\\vec u+\\vec v \\in W\\). Ara considerem \\(\\lambda \\in \\K\\) i \\(\\vec u\\in W\\). Volem veure \\(\\lambda \\vec u \\in W\\): com que \\(\\vec u\\in W\\), existeixen \\(\\mu_1, \\dots , \\mu_m\\in\\K\\) tals que \\(\\vec u=\\lambda_1\\vec v_1 + \\cdots + \\lambda_m\\vec v_m\\). Llavors: \\[\\lambda \\vec u = (\\lambda \\mu_1)\\vec v_1 + \\cdots + (\\lambda \\mu_m)\\vec v_m ,\\] pel que \\(\\lambda \\vec u \\in W\\). Exemple 2.4 Considerem \\(\\R^3\\) com a \\(\\R\\)-espai vectorial i els vectors (els escrivim per files) \\(\\{(1,0,0),(0,1,0)\\}\\), tenim que \\(\\langle (1,0,0),(0,1,0)\\rangle\\) són els vectors d’\\(\\R^3\\) amb tercera coordenada zero. Definició 2.5 Sigui \\(f\\colon \\K^n \\to \\K^m\\) una aplicació lineal. Definim el nucli d’\\(f\\) (i el denotem com \\(\\Ker(f)\\)) com el conjunt format pels vectors \\(\\vec v\\in \\K^n\\) tals que \\(f(\\vec v)=\\vec 0\\). Definim la imatge d’\\(f\\) (i la denotem com \\(\\Ima(f)\\)) com el conjunt dels vectors \\(\\vec w\\in\\K^m\\) tals que existeix \\(\\vec v\\in \\K^n\\) tal que \\(f(\\vec v)=\\vec w\\). Proposició 2.6 Si \\(f \\colon \\K^n \\to \\K^m\\) és una aplicació lineal, llavors: \\(\\Ker(f)\\) és un subespai vectorial de \\(\\K^n\\). \\(\\Ima(f)\\) és un subespai vectorial de \\(\\K^m\\). Prova. Comprovem primer que \\(\\Ker(f)\\) compleix les dues condicions de la Definició 2.3: Considerem \\(\\vec u, \\vec v \\in \\Ker(f)\\), per tant \\(f(\\vec u)=f(\\vec v)=\\vec 0\\). Llavors: \\[f(\\vec u+\\vec v)=f(\\vec u)+f(\\vec v)=\\vec 0+\\vec 0 =\\vec 0 ,\\] per tant \\(\\vec u+\\vec v \\in \\Ker(f)\\). També, si \\(\\vec u \\in \\Ker(f)\\) i \\(\\lambda\\in\\K\\), llavors \\(f(\\lambda \\vec u)=\\lambda f(\\vec u)=\\lambda \\vec 0=\\vec 0\\), per tant \\(\\lambda \\vec u\\in\\Ker(f)\\). Comprovem ara que \\(\\Ima(f)\\) també compleix les dues condicions: si \\(\\vec v_1,\\vec v_2 \\in\\Ima(f)\\), llavors existeixen \\(\\vec u_1, \\vec u_2 \\in \\K^n\\) tals que \\(f(\\vec u_1)=\\vec v_1\\) i \\(f(\\vec u_2)=\\vec v_2\\). Llavors es compleix que \\(f(\\vec u_1+\\vec u_2)=f(\\vec u_1)+f(\\vec u_2)=\\vec v_1+\\vec v_2\\), pel que \\(\\vec v_1+\\vec v_2 \\in \\Ima(f)\\). També, si \\(\\vec v\\in \\Ima(f)\\) (per tant existeix \\(\\vec u\\in \\K^n\\) tal que \\(f(\\vec u)=\\vec v\\)) i \\(\\lambda\\in\\K\\), tenim que \\(f(\\lambda \\vec u)=\\lambda f(\\vec u)=\\lambda \\vec v\\), pel que \\(\\lambda \\vec v \\in \\Ima(f)\\). Suposem que l’aplicació \\(f=f_A\\) està associada a una matriu \\(A\\in M_{m\\times n}(\\K)\\) com a la Subsecció 1. En aquest cas tenim que: \\(\\Ker(f_A)\\) són les solucions del sistema homogeni que té per matriu associada \\(A\\). \\(\\Ima(f_A)\\) és el subespai de \\(\\K^m\\) generat per les columnes d’\\(A\\) pensades com a vectors de \\(\\K^m\\). Exemple 2.5 Considerem \\(W=\\{(x,y,z)\\in\\R^3 \\mid 2x+y-z=0 \\text{ i } x -y +2z=0\\}\\). Podem pensar que \\(W\\) és el nucli de \\(f_A\\), on \\(A=\\big(\\begin{smallmatrix} 2 &amp; \\phantom{-}1 &amp; -1 \\\\ 1 &amp; -1 &amp; \\phantom{-}2\\end{smallmatrix}\\big)\\). Tot i que ja hem definit una noció d’independència lineal (Definició 1.2), vegem-ne una d’equivalent: Definició 2.6 Diem que un conjunt de vectors \\(\\{\\vec v_1, \\dots , \\vec v_n\\} \\subset \\K^m\\) és linealment independent si l’única manera d’escriure el vector zero com a combinació lineal de \\(\\vec v_1, \\dots \\vec v_n\\) és amb tots els coeficients zero. És a dir, si: \\[\\vec 0 = \\lambda_1 \\vec v_1+\\cdots +\\lambda_n\\vec v_n \\text{ amb $\\lambda_i\\in\\K$} \\, \\Longrightarrow \\, \\lambda_1=\\cdots =\\lambda_n=0 \\,.\\] Observació. Les definicions 1.2 i 2.6 són equivalents. Prova. En realitat, veurem que la negació d’una de les definicions és equivalent a la negació de l’altra: Considerem \\(\\{\\vec v_1, \\dots , \\vec v_n\\} \\subset \\K^m\\) que no compleixen la definició d’independència lineal de la Definició 1.2, llavors un dels vectors es pot escriure com a combinació lineal dels altres. Com que els podem reordenar, considerem que \\(\\vec v_1=\\lambda_2\\vec v_2+\\cdots \\lambda_n\\vec v_n\\). Llavors \\[\\vec 0 = \\vec v_1- \\lambda_2 \\vec v_2 - \\cdots -\\lambda_n\\vec v_n\\] i el vector zero es pot escriure com a combinació lineal de \\(\\{\\vec v_1, \\dots , \\vec v_n\\}\\) amb, com a mínim, un coeficient no nul (el de \\(\\vec v_1\\)). Considerem \\(\\{\\vec v_1, \\dots , \\vec v_n\\} \\subset \\K^n\\) que no compleixen la definició de independència lineal de la Definició 2.6, llavors podem escriure el vector zero: \\[\\vec 0 = \\lambda_1\\vec v_1+ \\lambda_2 \\vec v_2 + \\cdots +\\lambda_n\\vec v_n\\] amb algun \\(\\lambda_i\\neq 0\\). Com que els podem reordenar, considerem que \\(\\lambda_1\\neq 0\\). Llavors: \\[\\vec v_1 = -(\\lambda_2/\\lambda_1) \\vec v_2 - \\cdots -(\\lambda_n/\\lambda_1)\\vec v_n\\] i el vector \\(\\vec v_1\\) és combinació lineal de la resta. Observació. Si \\(\\{\\vec v_1, \\dots \\vec v_n\\}\\subset \\K^m\\), és un conjunt de vectors linealment independents, llavors qualsevol subconjunt d’aquest també ho serà. Per tant, treure vectors preserva la propietat de ser linealment independents. Podem veure si un conjunt de vectors és linealment independent mirant el nucli d’una aplicació lineal: Proposició 2.7 Siguin \\(\\{\\vec v_1, \\dots, \\vec v_n\\}\\subset \\K^m\\) un conjunt i \\(A\\in M_{m\\times n}(\\K)\\) la matriu que té per columnes els vectors \\(\\vec v_j\\). Llavors \\(\\{\\vec v_1, \\dots, \\vec v_n\\}\\) són linealment independents si, i només si, \\(\\Ker(f_A)=\\{\\vec 0\\}\\). Prova. Observem que donar una combinació lineal \\(x_1\\vec v_1+ \\cdots+x_n \\vec v_n\\) és equivalent a multiplicar \\(Ax\\), on \\(x\\) és el vector columna que té per coeficients \\(x_1, \\dots, x_n\\). Llavors, que \\(\\{\\vec v_1, \\dots, \\vec v_n\\}\\) siguin linealment independents vol dir que la única manera d’obtenir \\(Ax=\\vec 0\\) és amb \\(x=\\vec 0\\), o, el que és el mateix, \\(\\Ker(f_A)=\\vec 0\\). Podem veure quants vectors linealment independents hi ha dins d’un conjunt de vectors mirant el rang de la matriu corresponent: Lema 2.1 Si \\(\\vec v_1, \\dots ,\\vec v_n\\in\\K^m\\) són vectors, llavors el nombre màxim de vectors linealment independents continguts a \\(\\{\\vec v_1, \\dots ,\\vec v_n\\}\\) és el rang de la matriu \\(A\\) que té per columnes els vectors \\(\\vec v_i\\). Prova. Anomenem \\(k\\) al nombre màxim de vectors linealment independents a \\(\\{\\vec v_1, \\dots ,\\vec v_n\\}\\). Com que l’ordre no és important, reordenem els vectors \\(\\{\\vec v_1, \\dots ,\\vec v_n\\}\\) tal que els \\(k\\) primers són linealment independents i que qualsevol altre \\(\\vec v_j\\) amb \\(j\\geq k\\), \\(\\{\\vec v_1, \\dots ,\\vec v_k,\\vec v_j\\}\\) no són linealment independents. Sigui \\(B\\) la matriu que té per columnes els vectors \\(\\vec v_1, \\dots, \\vec v_k\\) i \\(\\overline{B_j}\\) la que té per columnes els vectors \\(\\vec v_1, \\dots, \\vec v_k,\\vec v_j\\). Pensem \\(\\overline{B_j}\\) com la matriu ampliada d’un sistema d’equacions, amb \\(B\\) matriu associada. Com que \\(\\vec v_j\\) és combinació lineals dels \\(\\vec v_1, \\dots, \\vec v_k\\), el sistema té solució i \\(\\Rang(B)=\\Rang(\\overline{B_j})\\). Podem iterar aquest procediment amb els altres vectors \\(\\vec v_{j}\\) amb \\(j\\geq k\\) i obtenim que \\(\\Rang(B)=\\Rang(A)\\). Finalment, com que \\(B\\) té per columnes vectors linealment independents, el sistema d’equacions homogeni que té per matriu associada \\(B\\), té solució única, i per tant \\(\\Rang(B)=k\\) (el número d’incògnites). D’aquí podem deduir una propietat que relaciona el producte de matrius amb el rang: Corol·lary 2.1 Si \\(A\\in M_{m\\times n}(\\K)\\) i \\(B\\in M_{n\\times r}(\\K)\\), llavors: \\[\\Rang(AB)\\leq \\min\\{\\Rang(A),\\Rang(B)\\}\\] Prova. Suposem que \\(C=AB\\) té columnes \\(j_1,\\ldots,j_r\\) que són linealment independents. Aleshores les columnes \\(j_1,\\ldots,j_r\\) de \\(B\\) també ho són: en efecte, si \\[\\lambda_1b_{j_1}+\\cdots\\lambda_rb_{j_r} = 0,\\] com que \\(c_j=Ab_j\\) per a tot \\(j\\), es té \\[\\lambda_1 c_{j_1} +\\cdots \\lambda_r c_{j_r}=\\lambda_1 A b_{j_1} + \\cdots + \\lambda_r A b_{j_r} = A(\\lambda_1b_{j_1}+\\cdots+\\lambda_r b_{j_r})=0,\\] i per tant tots els escalars \\(\\lambda_j=0\\). D’aquí en concloem que \\(\\Rang(AB)\\leq \\Rang(B)\\). Si ara transposem les matrius i fem el mateix raonament, tenim que \\[\\Rang(AB)=\\Rang((AB)^T)=\\Rang(B^TA^T)\\leq \\Rang(A^T)=\\Rang(A)\\] Per tant \\(\\Rang(AB)\\) és més petit que el rang d’\\(A\\) i que el rang de \\(B\\). Corol·lary 2.2 Aquests resultats donen la demostració de la primera part del Teorema 1.1. Corol·lary 2.3 Si \\(\\{\\vec v_1, \\dots, \\vec v_n\\}\\subset \\K^m\\) són linealment independents, llavors \\(n\\leq m\\). Prova. Si \\(\\{\\vec v_1, \\dots, \\vec v_n\\}\\) són linealment independents, pel Lema 2.1, la matriu \\(V\\), una matriu que té per columnes els vectors \\(\\vec v_j\\) té rang \\(n\\). Això només es pot tenir si hi ha com a mínim \\(n\\) files, i per tant \\(m\\geq n\\). Vegem ara el concepte de generadors: Definició 2.7 Si \\(\\{\\vec v_1, \\dots \\vec v_n\\}\\subset \\K^m\\) són tals que \\(\\K^m=\\langle \\vec v_1, \\dots \\vec v_n\\rangle\\), diem que \\(\\{\\vec v_1, \\dots \\vec v_n\\}\\) són un sistema de generadors de \\(\\K^m\\). Si \\(W\\subset \\K^n\\) és un subespai vectorial i \\(\\{\\vec v_1, \\dots \\vec v_n\\}\\subset W\\) són tals que \\(W=\\langle \\vec v_1, \\dots \\vec v_n\\rangle\\), diem que \\(\\{\\vec v_1, \\dots \\vec v_n\\}\\) són un sistema de generadors de \\(W\\). Observació. Si \\(\\{\\vec v_1, \\dots \\vec v_n\\}\\subset \\K^m\\) és un sistema de generadors i hi afegim un vector \\(\\vec u\\), llavors \\(\\{\\vec u,\\vec v_1, \\dots \\vec v_n\\}\\subset \\K^m\\) és també un sistema de generadors. O sigui, afegir vectors conserva la propietat de ser sistema de generadors. Observació. Si \\(\\{\\vec v_1, \\dots \\vec v_n\\}\\subset \\K^m\\), llavors \\(\\{\\vec v_1, \\dots \\vec v_n\\}\\) són un sistema de generadors de \\(\\langle \\vec v_1, \\dots \\vec v_n\\rangle\\) (que és un subespai vectorial). Vegem algunes propietats dels sistemes de generadors: Lema 2.2 Si \\(\\vec v_1, \\dots , \\vec v_n, \\vec u\\) són vectors de \\(\\K^m\\), llavors: \\(\\vec u\\in \\langle \\vec v_1, \\dots , \\vec v_n \\rangle\\) si i només si \\(\\langle \\vec v_1, \\dots , \\vec v_n \\rangle=\\langle\\vec u, \\vec v_1, \\dots , \\vec v_n \\rangle\\). Si \\(\\vec u = \\lambda_1 \\vec v_1 + \\cdots + \\lambda_n \\vec v_n\\) i \\(i\\) és un índex tal que \\(\\lambda_i\\neq 0\\), llavors: \\[\\langle \\vec v_1,\\dots,\\vec v_{i-1},\\vec v_i , \\vec v_{i+1}, \\dots , \\vec v_n\\rangle=\\langle \\vec v_1, \\dots,\\vec v_{i-1},\\vec u , \\vec v_{i+1},\\dots,\\vec v_n\\rangle.\\] Prova. Comencem demostrant (a): Suposem \\(\\vec u\\in \\langle \\vec v_1, \\dots , \\vec v_n \\rangle\\), per tant existeixen \\(\\lambda_1, \\dots, \\lambda_n\\in\\K\\) tals que \\(\\vec u=\\lambda_1\\vec v_1+\\dots \\lambda_n\\vec v_n\\). Sempre tenim \\(\\langle \\vec v_1, \\dots , \\vec v_n \\rangle\\subset\\langle\\vec u, \\vec v_1, \\dots , \\vec v_n \\rangle\\), per tant cal comprovar l’altre inclusió. Sigui \\(\\vec w \\in \\langle\\vec u, \\vec v_1, \\dots , \\vec v_n \\rangle\\), per tant existeixen \\(\\mu,\\mu_1,\\dots \\mu_n\\in\\K\\) tals que \\(\\vec w=\\mu \\vec u+\\mu_1\\vec v_1+\\dots \\mu_n\\vec v_n\\). Utilitzant que \\(\\vec u=\\lambda_1\\vec v_1+\\dots \\lambda_n\\vec v_n\\), tenim: \\[\\vec w=(\\mu\\lambda_1+\\mu_1)\\vec v_1+\\dots (\\mu\\lambda_n+\\mu_n)\\vec v_n ,\\] per tant \\(w \\in \\langle \\vec v_1, \\dots , \\vec v_n \\rangle\\). En canvi, si \\(\\vec u\\notin \\langle \\vec v_1, \\dots , \\vec v_n \\rangle\\), com que sempre tenim \\(\\vec u\\in \\langle\\vec u, \\vec v_1, \\dots , \\vec v_n \\rangle\\), obtenim que \\(\\langle \\vec v_1, \\dots , \\vec v_n \\rangle\\neq\\langle\\vec u, \\vec v_1, \\dots , \\vec v_n \\rangle\\). Per demostrar (b) comencem observant que, com que \\(\\lambda_i\\neq 0\\), \\[v_i=(1/\\lambda_i) \\vec u -(\\lambda_1/\\lambda_i) \\vec v_1- \\cdots-(\\lambda_{i-1}/\\lambda_i) \\vec v_{i-1} - (\\lambda_{i+1}/\\lambda_i) \\vec v_{i+1} - \\cdots -(\\lambda_n/\\lambda_i) \\vec v_n\\] pel que \\(\\vec v_1 \\in \\langle \\vec v_1, \\dots,\\vec v_{i-1},\\vec u , \\vec v_{i+1},\\dots,\\vec v_m\\rangle\\). Apliquem ara dos cops (a) per obtenir (l’ordre en que s’escriuen els vectors en un sistema de generadors no importa): \\[\\langle \\vec v_1, \\dots , \\vec v_n\\rangle= \\langle \\vec u,\\vec v_1, \\dots, \\vec v_i, \\dots , \\vec v_n\\rangle= \\langle \\vec v_1, \\dots,\\vec v_{i-1},\\vec u , \\vec v_{i+1},\\dots,\\vec v_n\\rangle.\\] Vegem ara quin és el nombre mínim de vectors generadors de \\(\\K^m\\): Lema 2.3 Si \\(\\{\\vec v_1, \\dots, \\vec v_n\\}\\subset \\K^m\\) són un sistema de generadors, llavors \\(n\\geq m\\). Prova. Considerem els vectors estàndard \\(\\vec e_j\\in\\K^m\\) definits a la Subsecció 1. Com que \\(\\langle \\vec v_1, \\dots, \\vec v_n \\rangle=\\K^m\\), tenim que \\(\\vec e_j \\in \\langle \\vec v_1, \\dots, \\vec v_n \\rangle\\), per tant, existeixen \\(x_{1j}, \\dots, x_{nj}\\) tals que \\(x_{1j} \\vec v_1+ \\cdots+x_{nj}\\vec v_n=\\vec e_j\\). Podem escriure aquesta igualtat de forma matricial: \\[A X = \\1_m\\] on \\(A \\in M_{m\\times n}(\\K)\\) és la matriu que té per columnes els vectors \\(\\vec v_j\\), \\(X\\in M_{n\\times m}(\\K)\\) és la matriu que té per coeficients \\(x_{ij}\\) i \\(\\1_m\\) és la matriu que té per columnes els vectors \\(\\vec e_j\\) (que és la identitat). Ara utilitzem el rang: \\(m=\\Rang(AX)\\leq\\Rang(A)\\), per tant \\(A\\) té com a mínim \\(m\\) columnes i \\(n\\geq m\\). Definició 2.8 Diem que un conjunt ordenat de vectors \\(\\calb=[\\vec v_1, \\dots, \\vec v_n]\\) amb \\(\\vec v_j\\in \\K^m\\), és una base de \\(\\K^m\\) si \\(\\{\\vec v_1,\\dots,\\vec v_n\\}\\), són linealment independents i generadors de \\(\\K^m\\). Observació. L’ordre dels vectors d’una base és important: de la definició es dedueix que si \\([\\vec v_1, \\vec v_2, \\vec v_3,\\dots, \\vec v_n]\\) és una base de \\(\\K^m\\), llavors \\([\\vec v_2, \\vec v_1, \\vec v_3,\\dots, \\vec v_n]\\) també ho és, però considerarem que són bases diferents. Teorema 2.1 Sigui \\(\\calb=[\\vec v_1, \\dots, \\vec v_n]\\) una base de \\(\\K^m\\), llavors \\(m=n\\). Diem que \\(\\K^m\\) té dimensió \\(m\\). Prova. Com que han de ser linealment independents, del Corol·lari 2.3, obtenim que \\(n\\leq m\\). Com que han de ser generadors, del Lema 2.3, obtenim que \\(n\\geq m\\), per tant, \\(m=n\\). Exemple 2.6 Els vectors (ho escrivim per files) \\((1,0)\\) i \\((0,1)\\) formen una base de \\(\\R^2\\). Els vectors \\((1,0)\\) i \\((1,1)\\) també formen una base de \\(\\R^2\\). Els vectors \\((1,0)\\), \\((0,1)\\) i \\((1,1)\\) no formen una base de \\(\\R^2\\) (no són linealment independents). Proposició 2.8 Si \\(\\langle \\vec v_1, \\dots , \\vec v_n \\rangle = \\K^m\\), qualsevol subconjunt maximal de vectors linealment independents de \\(\\vec v_1, \\dots, \\vec v_n\\) formaran una base de \\(\\K^m\\). Si \\(\\vec v_1, \\dots, \\vec v_n \\in \\K^m\\) és un conjunt de vectors linealment independents, podem ampliar el conjunt fins a obtenir una base de \\(\\K^m\\). Prova. Demostrem primer (a): en aquest cas, maximal vol dir que si afegim qualsevol vector, el conjunt obtingut deixa de ser linealment independent. Suposem que tenim un conjunt maximal de vectors linealment independents \\(\\vec v_{i_1}, \\dots ,\\vec v_{i_k}\\). Per demostrar que són base cal veure que són generadors i ho farem per contraposició lògica. Si no fossin generadors, tindríem: \\[\\langle \\vec v_{i_1}, \\dots ,\\vec v_{i_k}\\rangle \\subsetneqq \\langle \\vec v_{1}, \\dots ,\\vec v_{n}\\rangle\\] i per tant hi hauria un \\(\\vec v_j\\) tal que \\(\\vec v_j \\not\\in \\langle \\vec v_{i_1}, \\dots ,\\vec v_{i_k}\\rangle\\). Llavors \\(\\vec v_{i_1}, \\dots ,\\vec v_{i_k}, \\vec v_j\\) serien linealment independents (si \\(\\lambda_1\\vec v_{i_1}+\\cdots + \\lambda_k\\vec v_{i_k}+\\lambda \\vec v_j=\\vec 0\\), pot passar que \\(\\lambda=0\\) o \\(\\lambda\\neq0\\); si \\(\\lambda=0\\), com que \\(\\vec v_{i_1}, \\dots ,\\vec v_{i_k}\\) són linealment independents, tots els \\(\\lambda_i=0\\); si \\(\\lambda\\neq 0\\), podríem aïllar \\(\\vec v_j\\) en funció dels altres, contradient que \\(\\vec v_j \\not\\in \\langle \\vec v_{i_1}, \\dots ,\\vec v_{i_k}\\rangle\\)). Per tant, tindríem que \\(\\vec v_{i_1}, \\dots ,\\vec v_{i_k}\\) no seria maximal, tenint una contradicció. Per demostrar (b), considerem el conjunt \\(\\vec v_1,\\dots,\\vec v_n,\\vec e_1, \\dots, \\vec e_m\\) (on \\(\\vec e_i\\) són els vectors estàndard) i en triem un subconjunt maximal que contingui els primers \\(n\\) vectors. El resultat de l’apartat (a) també es pot llegir com que si tenim un sistema de generadors de \\(\\K^n\\), podem anar eliminant vectors fins a quedar-nos amb una base. Proposició 2.9 Si \\(\\vec v_1, \\dots, \\vec v_n\\) és un conjunt de vectors de \\(\\K^n\\), llavors són equivalents: \\(\\vec v_1, \\dots, \\vec v_n\\) formen una base. \\(\\vec v_1, \\dots, \\vec v_n\\) són un sistema de generadors. \\(\\vec v_1, \\dots, \\vec v_n\\) són linealment independents. Prova. Tenim que per definició de base (a)\\(\\Rightarrow\\)(b) i (a)\\(\\Rightarrow\\)(c). Suposem que tenim (b). Si no fossin linealment independents, hi hauria un subconjunt de vectors linealment independents \\(\\vec v_{i_1}, \\dots, \\vec v_{i_k}\\) maximals amb \\(k&lt;n\\), per tant tindríem una base amb \\(k\\) elements en un espai de dimensió \\(n&gt;k\\), per tant contradicció. Per tant els vectors són linealment independents (i generadors), per tant tenim (a). Suposem que tenim (c). Si \\(\\vec v_1, \\dots, \\vec v_n\\) no fossin generadors, podríem afegir vectors fins tenir una base, però aquesta nova base tindria més de \\(n\\) vectors, pel que contradiu que l’espai \\(\\K^n\\) sigui de dimensió \\(n\\). Per tant \\(\\vec v_1, \\dots, \\vec v_n\\) són generadors (i linealment independents), per tant tenim (a). Encara podem donar una altra caracterització del concepte de base. Teorema 2.2 Un conjunt de vectors \\([\\vec v_1, \\dots, \\vec v_n] \\subset \\K^m\\) formen una base si i només si tot per a tot vector \\(\\vec v\\in \\K^m\\) existeixen uns únics \\(\\lambda_1, \\dots, \\lambda_n\\in\\K\\) tals que \\(\\vec v=\\lambda_1\\vec v_1+\\cdots + \\lambda_n\\vec v_n\\). Prova. Suposem \\(\\{\\vec v_1, \\dots, \\vec v_n\\} \\subset \\K^m\\), llavors són generadors de \\(\\K^m\\) si i només si per a tot vector \\(\\vec v\\in \\K^m\\) existeixen \\(\\lambda_1, \\dots, \\lambda_n\\in\\K\\) tals que \\(\\vec v=\\lambda_1\\vec v_1+\\cdots + \\lambda_n\\vec v_n\\), per tant si i només si el sistema té solució. Suposem \\(\\{\\vec v_1, \\dots, \\vec v_n\\} \\subset \\K^m\\), llavors són linealment independents si i només si existeixen \\(\\lambda_1, \\dots, \\lambda_n,\\mu_1,\\dots, \\mu_n\\in\\K\\) tals que \\(\\lambda_1\\vec v_1+\\cdots + \\lambda_n\\vec v_n=\\mu_1\\vec v_1+\\cdots + \\mu_n\\vec v_n\\) implica que \\(\\lambda_j=\\mu_j\\) per a tot \\(j\\), per tant, si i només si la solució és única. Observació. Equivalentment al Teorema 2.2, podem dir que \\(\\{\\vec v_1, \\dots, \\vec v_n\\} \\subset \\K^m\\) formen una base si i només si la matriu \\(A\\in M_{m\\times n}(\\K)\\) formada pels vectors \\(\\vec v_i\\) a les columnes és invertible (en particular, ha de ser quadrada i per tant \\(n=m\\)). Prova. Si són base, necessitem que el sistema \\(Ax=B\\) tingui solució única per a tot \\(B\\in M_{n\\times 1}\\). Anem considerant \\(B_j=\\vec e_j\\) (els vectors estàndard definits a la Secció 1) i resolent cada un dels sistemes amb solucions \\(X_j\\in M_{n\\times 1}\\), tindrem una matriu \\(X\\) que compleix \\(AX=\\1_n\\), per ant \\(A\\) és invertible. Si la matriu \\(A\\) és invertible, \\(x=A^{-1}B\\), per tant el sistema és compatible determinat per a tot \\(B\\). Volem aprofitar les matrius per estudiar espais vectorials que no siguin del tipus \\(\\K^n\\). Com a cas particular, tenim els subespais. Comencem veient que el concepte de base també es pot estendre a subespais de \\(\\K^m\\). Definició 2.9 Considerem \\(V\\subset \\K^m\\) un subespai vectorial (Definició 2.3). Un conjunt ordenat de vectors \\([\\vec v_1, \\dots, \\vec v_n]\\) amb \\(\\vec v_i \\in V\\) són una base de \\(V\\) si són linealment independents i generen \\(V\\). Podem fer les observacions següents: Proposició 2.10 Si \\([\\vec v_1, \\dots, \\vec v_n]\\) és una base d’un subespai \\(V\\subset \\K^m\\), llavors: \\(n\\leq m\\) i \\(n=m\\) si i només si \\(V=\\K^m\\). Prova. és conseqüència del Corol·lari 2.3: si \\(\\{ \\vec v_1, \\dots, \\vec v_n \\}\\) són linealment independents a \\(\\K^m\\), llavors \\(n\\leq m\\). \\(b\\) Suposem que \\(n=m\\), per la Proposició 2.9, si śon linealment independents són base de tot \\(\\K^m\\), per tant \\(V=\\K^m\\). I, si \\(V=\\K^m\\), llavors qualsevol base ha de tenir \\(m\\) vectors, i \\(n=m\\). Observació. Sigui \\(V\\) un subespai vectorial de \\(\\K^m\\), \\(\\{\\vec u_1,\\dots,\\vec u_k\\}\\subset V\\) un conjunt de vectors linealment independents i \\(\\{\\vec v_1,\\dots,\\vec v_n\\}\\subset V\\) un conjunt de vectors generadors de \\(V\\). Llavors \\(k\\leq n\\). Prova. Com que \\(\\vec u_1\\in V=\\langle \\vec v_1,\\dots,\\vec v_n \\rangle\\), existeixen \\(\\lambda_i\\in\\K\\) tals que \\(\\vec u_1=\\lambda_1 \\vec v_1+\\cdots+\\lambda_n\\vec v_n\\). Com que \\(\\vec u_1 \\neq\\vec 0\\) (el \\(\\vec 0\\) no forma part de cap conjunt de vectors linealment independents), existeix \\(\\lambda_i\\neq 0\\). Si cal, reordenem els vectors \\(\\vec v_1,\\dots,\\vec v_n\\) i suposem \\(\\lambda_1\\neq 0\\). Aplicant ara el Lema 2.2, tenim: \\[V=\\langle \\vec v_1,\\dots,\\vec v_n \\rangle=\\langle \\vec u_1,\\vec v_2,\\dots,\\vec v_n \\rangle .\\] Considerem ara \\(\\vec u_2\\): com que \\(\\vec u_2\\in V=\\langle \\vec u_1,\\vec v_2,\\dots,\\vec v_n \\rangle\\), existeixen \\(\\mu_1\\in\\K\\) i \\(\\lambda_i\\in\\K\\) \\((i\\geq 2)\\) tals que \\(\\vec u_1=\\mu_1\\vec u_1+\\lambda_2\\vec v_2+\\cdots+\\lambda_n\\vec v_n\\). Com que \\(\\{\\vec u_1,\\vec u_2\\}\\) són linealment independents, existeix \\(\\lambda_i\\) tal que \\(\\lambda_i\\neq 0\\) amb \\(i\\geq 2\\). Si cal, podem reordenar els vectors \\(\\{\\vec v_2,\\dots,\\vec v_n\\}\\) i suposar que \\(\\lambda_2\\neq 0\\). Tornem a aplicar el Lema 2.2 i obtenim: \\[V=\\langle \\vec u_1,\\vec v_2,\\dots,\\vec v_{n} \\rangle=\\langle \\vec u_1,\\vec u_2,\\vec v_3,\\dots,\\vec v_{n} \\rangle .\\] Iterem ara aquest procediment. Si \\(k&gt;n\\), llavors podem iterar el procediment amb els \\(n\\) primers vectors de \\(\\{\\vec u_1,\\dots,\\vec u_k\\}\\), obtenint: \\[V=\\langle \\vec u_1,\\vec u_2,\\dots,\\vec u_{n} \\rangle\\] i per tant \\(\\vec u_{n+1} \\in V=\\langle \\vec u_1,\\vec u_2,\\dots,\\vec u_{n} \\rangle\\), que contradiu el fet de que \\(\\{\\vec u_1, \\dots, \\vec u_n,\\vec u_{n+1}\\}\\) siguin linealment independents. Teorema 2.3 Si \\(V\\subset \\K^m\\) és un subespai vectorial, totes les bases de \\(V\\) tenen el mateix nombre d’elements i l’anomenem dimensió de \\(V\\). Prova. Suposem que tenim \\(\\calb_1=[\\vec u_1,\\dots,\\vec u_k]\\) i \\(\\calb_2=[\\vec v_1,\\dots,\\vec v_n]\\) dues bases de \\(V\\). Com que els vectors de \\(\\calb_1\\) són linealment independents i els de \\(\\calb_2\\) generadors de \\(V\\), \\(k\\leq n\\). Com que els vectors de \\(\\calb_1\\) són generadors de \\(V\\) i \\(\\calb_2\\) són linealment independents, \\(k\\geq n\\). Per tant \\(k=n\\). Veiem que aquests resultats són molt semblants als que obteníem per \\(\\K^n\\) i els podem resumir a: Teorema 2.4 Si \\(V\\) és un subespai de \\(\\K^m\\) de dimensió \\(n\\), llavors: \\(n\\) és el nombre màxim que pot tenir un conjunt de vectors linealment independents de \\(V\\). \\(n\\) és el nombre mínim que pot tenir un conjunt de vectors generadors de \\(V\\). \\(n\\) vectors de \\(V\\) són linealment independents si i només si són generadors de \\(V\\), si i només si són base de \\(V\\). Prova. Només cal veure (c), i de (c) que \\(\\vec v_1, \\dots , \\vec v_n\\) són linealment independents si i només si generen \\(V\\): si són linealment independents, i no generen \\(V\\), podem trobar un vector \\(\\vec u\\not\\in\\langle \\vec v_1,\\dots, \\vec v_n\\rangle\\), per tant \\(\\{\\vec u, \\vec v_1,\\dots,\\vec v_n\\}\\) és un conjunt de vectors linealment independents que té \\(n+1\\) vectors, contradient (a). Si \\(\\vec v_1, \\dots , \\vec v_n\\) són generadors i no són linealment independents, vol dir que un d’ells es pot escriure com ca combinació lineal dels altres, i per tant tindríem un conjunt de vectors generadors amb \\(n-1\\) vectors, que no pot ser. Aprofitem tots aquests continguts per a veure com es pot trobar una base de la imatge d’una aplicació lineal \\(f\\colon \\K^n\\to \\K^m\\): Lema 2.4 Si \\(f\\colon \\K^n \\to K^m\\) és una aplicació lineal i \\(A\\) la matriu tal que \\(f=f_A\\), llavors \\(\\Ima(f)=\\langle f(\\vec e_1), \\dots, f(\\vec e_n) \\rangle\\) i \\(\\dim(\\Ima(f))=\\Rang(A)\\). Prova. \\(A\\) és la matriu que té per columnes \\(f(\\vec e_1), \\dots, f(\\vec e_n)\\), i a la demostració de la Proposició 2.1 vàrem veure que \\(\\Ima(f)=\\langle f(\\vec e_1), \\dots, f(\\vec e_n) \\rangle\\). Pel Lema 2.1, el rang d’\\(A\\) és el nombre màxim de vectors linealment independents que hi ha entre les seves columnes, per tant, també generen i formen una base, obtenint que és la dimensió de \\(\\Ima(f)\\). Per trobar una base de la imatge, podem utilitzar el resultat següent: Proposició 2.11 Considerem \\(A\\in M_{m\\times n}(\\K)\\) i la matriu reduïda equivalent \\(\\rref(A)\\) (que l’hem obtingut amb operacions elementals per files). Siguin \\(j_1, \\dots , j_k\\) les columnes de \\(\\rref(A)\\) que contenen un pivot. Llavors \\([f_A(\\vec e_{j_1}), \\dots, f_A(\\vec e_{j_k})]\\) és una base d’\\(\\Ima(f_A)\\). Prova. Pel Lema 2.4 i el Teorema 2.4 (c), tant sols cal demostrar que \\(\\{f_A(\\vec e_{j_1}), \\dots, f_A(\\vec e_{j_k})\\}\\) són linealment independents: considerem \\(B\\) la submatriu d’\\(A\\) formada per les columnes \\(j_1\\),…, \\(j_k\\) d’\\(A\\) i hi apliquem les mateixes transformacions que passen d’\\(A\\) a \\(\\rref(A)\\). Obtindrem una matriu reduïda amb les columnes \\(j_i\\), …, \\(j_k\\) de \\(\\rref(A)\\), que són les que tenen un pivot, i que serà equivalent a \\(B\\), per tant, per unicitat, tindrem \\(\\rref(B)\\). Però \\(\\rref(B)\\) té rang \\(k\\), i per tant \\(B\\) també. Però \\(k\\) és el nombre de columnes de \\(B\\), per tant, les columnes de \\(B\\) són linealment independents, que és el que volíem veure. Teorema 2.5 Sigui \\(f\\colon \\K^n\\to \\K^m\\) una aplicació lineal. Llavors: \\[n=\\dim(\\Ker(f))+\\dim(\\Ima(f)) \\,.\\] Prova. Considerem la matriu \\(A\\in M_{m\\times n}(\\K)\\) associada a \\(f=f_A\\). El resultat es dedueix directament d’interpretar \\(A\\) com la matriu associada a un sistema d’equacions lineals homogeni: \\(n\\) és el número d’incògnites, que és igual a les que fan de paràmetre lliure més les que no. \\(\\Ker(f_A)\\) són les solucions, per tant \\(\\dim(\\Ker(f_A))\\) són els graus de llibertat, el nombre de paràmetres lliures. \\(\\dim(\\Ima(f_A))=\\Rang(A)\\) són el nombre de columnes de \\(\\rref(A)\\) que tenen un pivot i, per tant, les que no són paràmetres lliures. Observació. En general, els subespais es descriuen o bé mitjançant un sistema d’equacions lineals homogeni o bé a través de generadors. Vegem com podem passar d’una descripció a l’altra. Primer observem que si \\(V\\leq \\K^n\\) és un subespai de dimensió \\(k\\), necessitem, com a mínim, \\(k\\) vectors linealment independents per generar-lo, i, un sistema d’equacions lineal homogeni amb, com a mínim, \\(n-k\\) equacions per definir-lo. Suposem que \\(V\\subset\\K^n\\) és el subespai vectorial definit com les solucions d’un sistema d’equacions lineal homogeni amb matriu associada \\(A\\) (sense ampliar, ja que tots els termes independents són zero). En aquest cas, si \\(\\dim(V)=k\\), \\(k\\) són els graus de llibertat i \\(\\Rang(A)=n-k\\). Resolem el sistema passant a \\(\\rref(A)\\) i aïllant les variables corresponents a les columnes que no tenen pivot (termes independents) en funció de les que tenen pivot (\\(k\\) variables). Qualsevol solució es pot escriure d’aquesta manera (per tant els \\(k\\) vectors que sortiran generen \\(V\\)). Tenim que, per l’equació del Teorema 2.5, \\(k=\\dim(\\Ker(A))\\), obtenint que les solucions han de ser una base de \\(V\\). Suposem que \\(V=\\langle \\vec v_1, \\dots, \\vec v_n\\rangle \\subset \\K^m\\) un subespai de dimensió \\(k\\) (per tant, esperem \\(m-k\\) equacions). Una manera d’obtenir unes equacions que defineixin \\(V\\) és amb el procediment següent: escrivim els vectors \\(\\vec v_j\\) en columna obtenint una matriu \\(A\\): \\[A=\\left(\\begin{array}{cccc} \\mid &amp; \\mid &amp;\\mid &amp; \\mid \\\\ \\vec v_1 &amp; \\vec v_2 &amp;\\cdots&amp; \\vec v_n \\\\ \\mid &amp; \\mid &amp;\\mid &amp; \\mid \\\\ \\end{array}\\right)\\] i escrivim ara \\(\\overline A\\): una matriu nova formada per la matriu \\(A\\) amb una matriu identitat \\(\\1_m\\) a la dreta: \\[\\overline A= \\left(\\begin{array}{c|c} A &amp; \\1_m\\\\ \\end{array}\\right)\\] Esglaonem \\(\\overline A\\) per files, obtenint, a les columnes formades per les \\(\\vec v_j\\), un total de \\(k\\) files no nul·les (on \\(k=\\Rang(A)\\) i per tant \\(k=\\dim(V)\\)), i \\(m-k\\) files amb tots els valors zero a les \\(n\\) primeres columnes: \\[\\overline A \\sim \\left( \\begin{array}{c|c} T &amp; P_1 \\\\ \\hline 0 &amp; P_2 \\end{array} \\right)\\] Amb \\(P_2\\in M_{(m-k)\\times m}(\\K)\\) complint que \\(P_2 A=0\\), pel que les columnes d’\\(A\\) (els vectors \\(\\vec v_j\\)) són solució del sistema d’equacions homogeni determinat per \\(P_2\\). A més, \\(P_2\\) és una submatriu d’una matriu quadrada de rang màxim, per tant les files són linealment independents i per tant les solucions de \\(P_2\\) tenen dimensió \\(k\\), obtenint que han de ser exactament \\(V\\). Exemple 2.7 Suposem que el subespai \\(V\\) està definit per les equacions següents i en volem calcular una base: \\[\\begin{align*} 3 x_1 -5x_2 +x_3 + x_4 &amp; = 0 \\\\ x_1 - x_2 - x_3 + x_4 &amp; = 0 \\\\ x_1-2x_2+x_3 &amp; =0 \\end{align*}\\] La matriu associada al sistema és: \\[\\begin{pmatrix} 3 &amp; -5 &amp; 1 &amp; 1 \\\\ 1 &amp; -1 &amp; -1 &amp; 1 \\\\ 1 &amp; -2 &amp; 1 &amp; 0 \\end{pmatrix}\\] Que és equivalent a: \\[\\begin{pmatrix} 1 &amp; 0 &amp; -3 &amp; 2 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix}\\] Amb solució \\(x_1=3x_3-2x_4\\), \\(x_2=2x_3-x_4\\) i paràmetres lliures \\(x_3\\) i \\(x_4\\). Per tant: \\[\\begin{pmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4 \\end{pmatrix}= \\begin{pmatrix} 3x_3-2x_4\\\\ 2x_3-x_4\\\\ x_3\\\\ x_4 \\end{pmatrix}= x_3 \\begin{pmatrix} 3\\\\ 2\\\\ 1 \\\\ 0 \\end{pmatrix}+ x_4\\begin{pmatrix} -2 \\\\ -1 \\\\ 0\\\\ 1 \\end{pmatrix}\\] I una base de \\(V\\) és: \\[\\left[\\begin{pmatrix} 3\\\\ 2\\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} -2 \\\\ -1 \\\\ 0\\\\ 1 \\end{pmatrix}\\right]\\] Exemple 2.8 Suposem que \\(V\\) està generat per: \\[V=\\left\\langle \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix}, \\begin{pmatrix}2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix}\\right\\rangle,\\] i volem trobar un sistema d’equacions lineals que descrigui \\(V\\). Considerem la matriu: \\[\\left( \\begin{array}{ccc|cccc} 1 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 2 &amp; 3 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 3 &amp; 4 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ 1 &amp; 4 &amp; 5 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right),\\] i l’esglaonem per files: \\[\\left( \\begin{array}{ccc|rrcc} 1 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 1 &amp; -1 &amp; 1 &amp; 0 &amp; 0\\\\ \\hline 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; -3 &amp; 0 &amp; 1 \\end{array} \\right).\\] Per tant, \\(V\\) també es pot definir com el subespai que correspon a les equacions: \\[\\begin{align*} x_1-2x_2+x_3&amp;=0\\\\ 2x_1-3x_2+x_4&amp;=0. \\end{align*}\\] 2.4 Suma i intersecció de subespais vectorials Volem construir subespais vectorials a partir d’altres subespais: si \\(U,V\\subset \\K^n\\) són subespais vectorials, en particular són conjunts, pel que té sentit considerar la intersecció \\(U\\cap V\\) i la unió \\(V\\cup V\\). Veiem que: \\(\\vec 0\\in U\\) i \\(\\vec 0\\in V\\) (ja que són subespai), per tant, \\(\\vec 0\\in U \\cap V\\), si \\(\\vec v_1, \\vec v_2 \\in U\\cap V\\), llavors \\(\\vec v_1+\\vec v_2 \\in U\\cap V\\): \\(\\vec v_1\\) i \\(\\vec v_2\\) són vectors d’\\(U\\) i \\(V\\). Com que \\(U\\) és subespai, \\(\\vec v_1+\\vec v_2 \\in U\\). Com que \\(V\\) és subespai, \\(\\vec v_1+\\vec v_2 \\in V\\). Obtenim, llavors \\(\\vec v_1+\\vec v_2 \\in U\\cap V\\) i si \\(\\vec v \\in U\\cap V\\) i \\(\\lambda \\in \\K\\), llavors si \\(\\lambda \\vec v \\in U\\cap V\\): com que \\(\\vec v \\in U\\) i \\(U\\) és subespai vectorial, \\(\\lambda \\vec u\\in U\\). El mateix argument implica \\(\\lambda \\vec v \\in V\\), per tant \\(\\lambda \\vec v \\in U\\cap V\\). Obtenint: Proposició 2.12 Si \\(U,V\\subset \\K^n\\) són subespais vectorials, llavors \\(U\\cap V\\) també és un subespai vectorial. En canvi, si considerem la unió, si \\(\\vec v_1, \\vec v_2 \\in U\\cup V\\), podem assegurar que \\(\\vec v_1+\\vec v_2 \\in U \\cup V\\)? Vegem que no és sempre cert: considerem \\(U=\\langle \\big( \\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}\\big) \\rangle \\subset \\R^2\\) i \\(V=\\langle \\big( \\begin{smallmatrix} 0 \\\\ 1 \\end{smallmatrix}\\big) \\rangle\\subset \\R^2\\), llavors \\(U\\cup V\\) són els vectors que pertanyen als dos eixos. Llavors \\(\\big( \\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}\\big), \\big( \\begin{smallmatrix} 0 \\\\ 1 \\end{smallmatrix}\\big)\\in U\\cup V\\), però \\(\\big( \\begin{smallmatrix} 1 \\\\ 1 \\end{smallmatrix}\\big)=\\big( \\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}\\big)+\\big( \\begin{smallmatrix} 0 \\\\ 1 \\end{smallmatrix}\\big) \\not\\in U\\cup V\\). Per això, definim el concepte de suma: Definició 2.10 Si \\(U,V\\subset \\K^m\\) són subespais vectorials, definim la suma \\(U+V\\) com: \\[U+V=\\{\\vec u+\\vec v \\mid \\vec u \\in U \\text{ i } \\vec v \\in V\\} .\\] Lema 2.5 Si \\(U,V\\subset \\K^m\\) són subespais vectorials, llavors \\(U+V\\) és un subespai vectorial. Més concretament, \\(U+V\\) és el subespai generat per \\(U\\) i \\(V\\) (podem escriure \\(U+V=\\langle U,V\\rangle\\)). Prova. Vegem que és \\(U+V\\) un subespai: El vector \\(\\vec 0\\) és d’\\(U+V\\): com que \\(U\\) i \\(V\\) són subespais, \\(\\vec 0\\) pertany a tots dos. A més, es pot escriure com \\(\\vec 0=\\vec 0 +\\vec 0\\), per tant és d’\\(U+V\\). Si \\(\\vec w_1,\\vec w_2 \\in U+V\\), vol dir que existeixen \\(u_1, u_2 \\in U\\), i \\(\\vec v_1, \\vec v_2 \\in V\\) tals que \\(\\vec w_1=\\vec u_1+\\vec v_1\\) i \\(\\vec w_2=\\vec u_2+\\vec v_2\\). Llavors, \\(\\vec w_1+\\vec w_2=(\\vec u_1+\\vec u_2)+(\\vec v_1+\\vec v_2)\\), amb \\(\\vec u_1+\\vec u_2 \\in U\\) i \\(\\vec v_1+\\vec v_2\\in V\\) (ja que tant \\(U\\) com \\(V\\) són subespais vectorials). Si \\(\\vec w \\in U+V\\) i \\(\\lambda \\in K\\), llavors \\(\\lambda \\vec w \\in U+V\\): existeixen \\(\\vec u \\in U\\) i \\(\\vec v\\in V\\) tals que \\(\\vec w=\\vec u +\\vec v\\). Llavors \\(\\lambda \\vec w=\\lambda \\vec u+\\lambda \\vec v\\), amb \\(\\lambda \\vec u\\in U\\) i \\(\\lambda \\vec v\\in V\\) (ja que \\(U\\) i \\(V\\) són subespais). Ara tenim que \\(U\\subset U+V\\) i \\(V\\subset U+V\\), per tant, com que \\(U+V\\) és subespai, \\(\\langle U,V\\rangle\\subset U+V\\). Per definició, els vectors d’\\(U+V\\) estan generats pels vectors d’\\(U\\) i de \\(V\\), per tant \\(U+V\\leq \\langle U,V \\rangle\\). Observació. Podem calcular la intersecció i la suma de subespais vectorials amb la següent observació: Si \\(U\\subset \\K^n\\) és el subespai vectorial definit com les solucions del sistema d’equacions lineals homogeni que té per matriu associada \\(A\\), i \\(V\\subset \\K^n\\) el que té per matriu associada \\(B\\), llavors: \\[U \\cap V \\text{ és solució del sistema que té per matriu associada } \\begin{pmatrix} A \\\\ B \\end{pmatrix}.\\] Dit d’altra manera, si \\(U = \\Ker(f_A)\\) i \\(V=\\Ker(f_B)\\), aleshores \\(U\\cap V = \\Ker(f_{\\left(\\substack{A\\\\B}\\right)})\\). Si \\(U=\\langle \\vec u_1, \\dots, \\vec u_k \\rangle \\subset \\K^n\\) i \\(V=\\langle \\vec v_1, \\dots, \\vec v_l \\rangle \\subset \\K^n\\), llavors: \\[U+V=\\langle \\vec u_1, \\dots, \\vec u_k, \\vec v_1, \\dots, \\vec v_l \\rangle .\\] Dit d’altra manera, si \\(U = \\Ima(f_A)\\) i \\(V=\\Ima(f_B)\\), aleshores \\(U+V = \\Ima(f_{(A|B)})\\). Exemple 2.9 Suposem que se’ns donen els següents dos subespais d’\\(\\R^4\\): \\[U = \\left\\langle \\begin{pmatrix}1\\\\2\\\\1\\\\1\\end{pmatrix},\\begin{pmatrix}1\\\\2\\\\4\\\\-1\\end{pmatrix}\\right\\rangle,\\quad V = \\left\\{\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{pmatrix}~\\colon ~ x_1 + x_2 + 2x_3 + x_4=0\\right\\}.\\] Se’ns demana que donem una base d’\\(U\\cap V\\). Per resoldre aquest problema, observem que per calcular la intersecció ens cal tenir equacions per \\(U\\) i \\(V\\). Per tant, ens cal realitzar els següents passos: Trobar equacions per \\(U\\), Definir equacions per \\(U\\cap V\\), i Trobar una base d’\\(U\\cap V\\). Per trobar equacions per \\(U\\), esglaonem la matriu ampliada amb la identitat: \\[\\left( \\begin{array}{rr|rrrr} 1 &amp; 1 &amp; 1&amp;0&amp;0&amp;0\\\\ 2&amp;2&amp;0&amp;1&amp;0&amp;0\\\\ 1&amp;4&amp;0&amp;0&amp;1&amp;0\\\\ 1&amp;-1&amp;0&amp;0&amp;0&amp;1 \\end{array} \\right)\\sim\\cdots\\sim \\left( \\begin{array}{rr|rrrr} 1&amp;1&amp;1&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;-2&amp;0&amp;1&amp;1\\\\\\hline 0&amp;0&amp;-2&amp;1&amp;0&amp;0\\\\ 0&amp;0&amp;-5&amp;0&amp;2&amp;3 \\end{array} \\right).\\] Així, \\(U\\) ve definit per les equacions \\[\\begin{align*} -2x_1+x_2 &amp;= 0\\\\ -5x_1 +2x_3+3x_4&amp;=0. \\end{align*}\\] Les equacions per \\(U\\cap V\\) són, doncs: \\[\\begin{align*} x_1+x_2+2x_3+x_4&amp;=0\\\\ -2x_1+x_2 &amp;= 0\\\\ -5x_1 +2x_3+3x_4&amp;=0. \\end{align*}\\] Finalment, per trobar una base d’\\(U\\cap V\\) hem de resoldre el sistema anterior. Esglaonant la matriu corresponent, obtenim la matriu \\[\\left(\\begin{array}{rrrr} 1&amp;0&amp;0&amp;\\frac{-1}{4}\\\\ 0&amp;1&amp;0&amp;\\frac{-1}{3}\\\\ 0&amp;0&amp;1&amp;\\frac{7}{8} \\end{array}\\right).\\] Així, una base d’\\(U\\cap V\\) ve donada per \\[\\left[\\begin{pmatrix}2\\\\4\\\\-7\\\\8\\end{pmatrix}\\right].\\] Observació. Considerem \\(U,V\\subset \\K^n\\) subespais vectorials. Suposem que \\(j=\\dim(U\\cap V)\\), \\(k=\\dim(U)\\) i \\(l=\\dim(V)\\). Siguin: \\(\\calb=[\\vec w_1, \\dots , \\vec w_j]\\) una base d’\\(U\\cap V\\), \\(\\calb_U=[\\vec w_1, \\dots , \\vec w_j,\\vec u_{j+1}, \\dots, \\vec u_k]\\) una base d’\\(U\\) i \\(\\calb_V=[\\vec w_1, \\dots , \\vec w_j,\\vec v_{j+1}, \\dots, \\vec v_l]\\) una base de \\(V\\). Observem que hem començat amb una base d’\\(U\\cap V\\) que, per una banda hem ampliat fins a base d’\\(U\\) i per altra, fins a base de \\(V\\). Veiem que: \\(\\{\\vec w_1, \\dots , \\vec w_j,\\vec u_{j+1}, \\dots, \\vec u_k,\\vec v_{j+1}, \\dots, \\vec v_l\\}\\) generen \\(U+V\\): si \\(\\vec w \\in U+V\\), existeixen \\(\\vec u\\in U\\) i \\(\\vec v\\in V\\) tals que \\(\\vec w=\\vec u +\\vec v\\). Com que \\(\\calb_U\\) i \\(\\calb_V\\) són bases d’\\(U\\) i \\(V\\) respectivament, llavors, podem escriure: \\[\\begin{align*} \\vec u &amp;=\\lambda_1 \\vec w_1 + \\cdots + \\lambda _j \\vec w_j + \\lambda_{j+1} \\vec u_{j+1} + \\cdots + \\lambda_k \\vec u_k \\\\ \\vec v &amp;=\\mu_1 \\vec w_1 + \\cdots + \\mu _j \\vec w_j + \\mu_{j+1} \\vec v_{j+1} + \\cdots + \\mu_k \\vec v_k \\end{align*}\\] i per tant: \\[\\vec w=(\\lambda_1+\\mu_1) \\vec w_1 + \\cdots +(\\lambda _j+\\mu_j) \\vec w_j + \\lambda_{j+1} \\vec u_{j+1} + \\cdots \\lambda_k \\vec u_k+\\mu_{j+1}\\vec v_{j+1} + \\cdots + \\mu_k \\vec v_k.\\] \\(\\{\\vec w_1, \\dots , \\vec w_j,\\vec u_{j+1}, \\dots, \\vec u_k,\\vec v_{j+1}, \\dots, \\vec v_l\\}\\) són linealment independents: si \\[\\begin{align*} (\\#eq:UcapV+U+V) \\vec 0=\\lambda_1 \\vec w_1 + \\cdots \\lambda _j \\vec w_j + \\lambda_{j+1} \\vec u_{j+1} + \\cdots + \\lambda_k \\vec u_k+\\mu_{j+1}\\vec v_{j+1} + \\cdots + \\mu_k \\vec v_k, \\end{align*}\\] llavors: \\[\\vec w := \\lambda_1 \\vec w_1 + \\cdots \\lambda _j \\vec w_j + \\lambda_{j+1} \\vec u_{j+1} + \\cdots +\\lambda_k \\vec u_k=-(\\mu_{j+1}\\vec v_{j+1} + \\cdots + \\mu_k \\vec v_k)\\] és un vector d’\\(U\\) i \\(V\\), per tant d’\\(U\\cap V\\). Llavors, es pot escriure de manera única com: \\[\\vec w= \\gamma_1 \\vec w_1 + \\cdots +\\gamma _j \\vec w_j,\\] i per tant \\(\\lambda_i=\\gamma_i\\) per a \\(i\\leq j\\) i \\(\\lambda_{j+1}=\\cdots \\lambda_k=0\\). Llavors, de l’Equació @ref(eq:UcapV+U+V) ens queda la igualtat: \\[\\vec 0=\\lambda_1 \\vec w_1 + \\cdots +\\lambda _j \\vec w_j +\\mu_{j+1}\\vec v_{j+1} + \\cdots + \\mu_k \\vec v_k\\] i tots els coeficients són zero ja que \\(\\calb_V\\) és una base de \\(V\\). Per tant \\([\\vec w_1, \\dots , \\vec w_j,\\vec u_{j+1}, \\dots, \\vec u_k,\\vec v_{j+1}, \\dots, \\vec v_l]\\) és una base d’\\(U+V\\). Si ara comptem quants elements té cadascuna de les bases, tenim: Teorema 2.6 Si \\(U,V \\subset \\K^n\\) són subespais vectorials, llavors: \\[\\dim(U)+\\dim(V)=\\dim(U+V)+\\dim(U\\cap V) .\\] Definició 2.11 Si \\(U,V,W \\subset \\K^n\\) són subespais vectorials, diem que \\(W\\) és la suma directa d’\\(U\\) i \\(V\\), i escrivim \\(W=U\\oplus V\\), si \\(W=U+V\\) i \\(U\\cap V=\\{\\vec 0\\}\\). Proposició 2.13 Si \\(U,V,W\\subset \\K^n\\) són subespais vectorials, llavors \\(W=U\\oplus V\\) si, i només si, tot vector de \\(W\\) es pot escriure de forma única com a suma d’un vector d’\\(U\\) més un vector de \\(V\\). Prova. Si \\(W=U\\oplus V\\) llavors , com que \\(W=U+V\\), per a tot \\(\\vec w\\in W\\), existeixen \\(\\vec u\\in U\\) i \\(\\vec v \\in V\\) tals que \\(\\vec w=\\vec u+\\vec v\\). Falta veure que \\(\\vec u\\) i \\(\\vec v\\) són únics: suposem que també podem escriure \\(\\vec w=\\vec u&#39;+\\vec v&#39;\\), amb \\(\\vec u&#39;\\in U\\) i \\(\\vec v&#39;\\in V\\). Tenim: \\[\\vec 0 = \\vec w - \\vec w = (\\vec u-\\vec u&#39;)+(\\vec v-\\vec v&#39;)\\] i per tant: \\[\\vec u-\\vec u&#39;=-(\\vec v-\\vec v&#39;) \\in U\\cap V\\] però com que \\(U\\) i \\(V\\) estan en suma directa, \\(U\\cap V=\\{\\vec 0\\}\\), obtenint que \\(\\vec u=\\vec u&#39;\\) i \\(\\vec v=\\vec v&#39;\\). Suposem que tot vector \\(\\vec w \\in W\\) es pot escriure de forma única com \\(\\vec w=\\vec u+\\vec v\\), amb \\(\\vec u \\in U\\) i \\(\\vec v \\in V\\). Com que es pot escriure, tenim \\(W=U+V\\). Sigui \\(\\vec w \\in U\\cap V\\). Podem escriure \\(\\vec w\\) com: \\[\\vec w = \\vec w + \\vec 0 = \\vec 0 + \\vec w\\] on a cada part de la igualtat el primer vector és d’\\(U\\) i el segon de \\(V\\). Per unicitat, les dues expressions han de coincidir i per tant \\(\\vec w = \\vec 0\\). Exemple 2.10 A \\(\\R^3\\), considerem el pla \\(U=\\{x=0\\}\\) i el pla \\(V=\\{y = 0 \\}\\). Aleshores \\(\\R^3 = U + V\\), ja que to vector \\(\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\\) es pot escriure (per exemple) com \\[\\begin{pmatrix} x\\\\y\\\\z \\end{pmatrix}= \\begin{pmatrix} 0\\\\y\\\\z \\end{pmatrix}+ \\begin{pmatrix} x\\\\0\\\\0 \\end{pmatrix},\\] on el primer sumand pertany a \\(U\\) i el segon a \\(V\\). La suma, però, no és directa: \\[U\\cap V=\\{x=y=0\\}=\\left\\langle \\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}\\right\\rangle\\]. En canvi, si en comptes de \\(V\\) considerem la recta definida per \\(V&#39;=\\{y=z=0\\}\\), aleshores \\(\\R^3=U\\oplus V&#39;\\). 2.5 Aplicacions injectives, exhaustives i bijectives Recordem unes definicions que tenen sentit per qualssevol aplicacions entre conjunts: si \\(A\\) i \\(B\\) són conjunts i \\(f\\colon A \\to B\\) és una aplicació entre \\(A\\) i \\(B\\) (això vol dir que assignem a cada element \\(a\\in A\\) un element \\(f(a)\\in B\\), i anomenem a \\(f(a)\\) imatge de l’element \\(a\\) per \\(f\\)), diem que: \\(f\\) és injectiva si quan \\(a\\neq a&#39;\\), llavors \\(f(a)\\neq f(a&#39;)\\) (envia elements diferents a elements diferents). \\(f\\) és exhaustiva si per a tot \\(b\\in B\\), existeix algun \\(a\\in A\\) tal que \\(f(a)=b\\) (tot element té antiimatge). \\(f\\) és bijectiva si és injectiva i exhaustiva. Equivalentment, per a tot \\(b\\in B\\) existeix un únic \\(a\\in A\\) tal que \\(f(a)=b\\). Anomenem \\(\\Ima(f)\\subset B\\) a la imatge de l’aplicació \\(f\\). Per tant, \\(f\\) és exhaustiva si i només si \\(\\Ima(f)=B\\). Si tenim \\(f\\colon A \\to B\\) i \\(g\\colon B \\to C\\) aplicacions de conjunts, definim la composició \\(g\\circ f \\colon A \\to C\\) com l’aplicació \\((g\\circ f)(a)=g(f(a))\\). Exemple 2.11 \\(A=\\{1,2,3\\}\\) , \\(B=\\{a,b,c,d\\}\\) i definim \\(f\\colon A \\to B\\) com \\(f(1)=a\\), \\(f(2)=b\\) i \\(f(3)=a\\). Aquesta aplicació no és ni injectiva (\\(f(1)=f(3)\\) quan \\(1\\neq 3\\)), ni exhaustiva (\\(c\\not\\in \\Ima(f)\\)). \\(A=\\{1,2,3\\}\\) , \\(B=\\{a,b,c,d\\}\\) i definim \\(g\\colon A \\to B\\) com \\(g(1)=a\\), \\(g(2)=b\\) i \\(g(3)=d\\). Aquesta aplicació és injectiva, però no exhaustiva (\\(c\\not\\in \\Ima(f)\\)). \\(A=\\{1,2,3\\}\\) , \\(B=\\{a,b,c,d\\}\\) i definim \\(h\\colon B \\to A\\) com \\(h(a)=1\\), \\(h(b)=2\\) \\(h(c)=1\\) i \\(h(d)=3\\). Aquesta aplicació no és injectiva (\\(h(c)=h(a)\\)), però sí que és exhaustiva. Veiem que \\(h\\circ g \\colon A \\to A\\) és l’aplicació que envia cada element a ell mateix (l’anomenem aplicació identitat d’\\(A\\) i la denotem per \\(\\Id_A\\)). Les aplicacions lineals entre espais vectorials són un cas particular d’aplicacions de conjunts, per tant té sentit de parlar d’aplicacions lineals injectives, exhaustives i bijectives. Tenim una caracterització de les aplicacions lineals injectives: Lema 2.6 \\(f \\colon \\K^n \\to \\K^m\\) una aplicació lineal. Llavors \\(f\\) és injectiva si i només si \\(\\ker(f)=\\{\\vec 0\\}\\). Prova. Suposem que \\(f\\) és injectiva. Com que és lineal \\(f(\\vec 0)=\\vec 0\\), per tant \\(\\{\\vec 0\\} \\subset \\Ker(f)\\). Si \\(\\Ker(f)\\) tingués algun altre vector \\(\\vec v \\neq \\vec 0\\), llavors \\(f\\) no seria injectiva (tindria dos vectors amb la mateixa imatge). Suposem que \\(f\\) no és injectiva: llavors existeixen \\(\\vec u \\neq \\vec v\\) tals que \\(f(\\vec u)=f(\\vec v)\\). Però llavors \\(f(\\vec u- \\vec v)=f(\\vec u)-f(\\vec v)=\\vec 0\\), amb \\(\\vec u-\\vec v\\neq \\vec 0\\), i tenim que \\(\\Ker(f)\\neq \\{\\vec 0\\}\\). Exercici 2.3 Considerem \\(A \\in M_{m\\times n}(\\K)\\) i l’aplicació lineal \\(f_A\\colon \\K^n\\to \\K^m\\) definida a l’Exemple 2.1. Demostreu que: \\(f_A\\) és injectiva si i només si \\(\\Rang(A)=n\\). \\(f_A\\) és exhaustiva si i nomes si \\(\\Rang(A)=m\\). \\(f_A\\) és bijectiva si i només si \\(n=m\\) i \\(A\\) és invertible (en aquest cas diem que \\(f_A\\) és un isomorfisme). 2.6 Coordenades de vectors Definició 2.12 Suposem que \\(V\\) és un subespai vectorial de \\(\\K^m\\) i \\(\\calb =[\\vec v_1, \\dots, \\vec v_n]\\) una base de \\(V\\). Si \\(\\vec w \\in V\\), definim les coordenades de \\(\\vec w\\) en la base \\(\\calb\\) com el vector \\((\\lambda_1,\\dots,\\lambda_n)\\in\\K^n\\) tal que \\[\\vec w = \\sum_{i=1}^n \\lambda_i \\vec v_i \\,.\\] Si hi pot haver confusió, escriurem \\([\\vec w]_\\calb=(\\lambda_1,\\dots,\\lambda_n)\\) per parlar de les coordenades d’un vector en la base \\(\\calb\\). Cal veure que aquesta definició és correcta: Teorema 2.7 Suposem \\(V\\) és un subespai vectorial de \\(\\K^m\\) i \\(\\calb =[\\vec v_1, \\dots, \\vec v_n]\\) una base de \\(V\\). Llavors tot vector \\(\\vec w\\in V\\) té coordenades i són úniques. Prova. És el que diu el Teorema 2.2. Observació. Podem veure que si fixem una base \\(\\calb\\) d’un subespai \\(V\\subset \\K^m\\) i \\([\\vec v_1]_\\calb=(\\lambda_1,\\dots,\\lambda_n)\\), \\([\\vec v_2]_\\calb=(\\mu_1,\\dots,\\mu_n)\\) i \\(\\gamma \\in \\K\\), tenim que: \\([\\vec v_1+\\vec v_2]_\\calb=(\\lambda_1+\\mu_1,\\dots,\\lambda_n+\\mu_n)\\) i \\([\\gamma \\vec v_1]_\\calb=(\\gamma \\lambda_1, \\dots, \\gamma \\lambda_n)\\). Aquesta observació ens permet identificar qualsevol subespai vectorial de \\(\\K^m\\) amb una base fixada \\(\\calb\\) amb \\(n\\) vectors amb l’espai vectorial \\(\\K^n\\). Les aplicacions lineals entre \\(\\K^n\\) i \\(\\K^m\\) les hem estudiat a la Proposició 2.1, obtenint que es corresponen amb matrius \\(A \\in M_{m\\times n}(\\K)\\). Això ens permet fer la definició següent: Definició 2.13 Si \\(f\\colon V \\to W\\) és una aplicació lineal entre subespais vectorials de \\(\\K^m\\) i \\(\\K^r\\) respectivament amb bases \\(\\calb=[\\vec v_1, \\dots, \\vec v_n]\\) i \\(\\calc=[\\vec w_1, \\dots, \\vec w_s]\\) respectivament, definim la matriu associada a \\(f\\) en les bases \\(\\calb\\) i \\(\\calc\\) (escriurem \\([f]_\\calb\\) si l’espai de sortida és el mateix d’arribada i utilitzem la mateixa base \\(\\calb\\)) i l’escrivim com \\([f]_{\\calb,\\calc}\\) com la matriu \\(A\\) que té per coeficients \\(a_{ij}\\) complint: \\[f(\\vec v_j)=\\sum_{i=1}^s a_{ij} \\vec w_i \\,.\\] O sigui, \\[[f]_{\\calb,\\calc}=\\begin{pmatrix} \\mid &amp; \\mid &amp;\\mid &amp; \\mid\\\\ [f(\\vec v_1)]_\\calc &amp; [f(\\vec v_2)]_\\calc &amp;\\cdots&amp;[f(\\vec v_n)]_\\calc\\\\ \\mid &amp; \\mid &amp;\\mid &amp; \\mid \\end{pmatrix}.\\] Del càlcul fet a la Proposició 2.1 es dedueix que aquesta matriu compleix que si \\([\\vec v]_\\calb=(\\lambda_1,\\dots,\\lambda_n)\\), llavors \\([f(v)]_{\\calc}=(\\mu_1,\\dots,\\mu_s)\\), on: \\[\\begin{align*} (\\#eq:matriu-apl-lineal) \\begin{pmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_s \\end{pmatrix}= [f]_{\\calb,\\calc} \\begin{pmatrix} \\lambda_1 \\\\ \\vdots \\\\ \\lambda_n \\end{pmatrix}. \\end{align*}\\] Vegem un cas particular d’aquest argument, quan \\(V=\\K^n\\) i \\(W=\\K^m\\). De la definició de matriu associada i que la composició d’aplicacions es correspon amb la multiplicació de matrius, es dedueixen els resultats següents: Proposició 2.14 Fixem les notacions següents: \\(\\cals_n=[\\vec e_1, \\dots, \\vec e_n]\\) és la base de \\(\\K^n\\) formada pels vectors estàndard, \\(\\calb=[\\vec v_1, \\dots,\\vec v_n]\\) és una base (qualsevol) de \\(\\K^n\\), \\(\\calb&#39;\\) de \\(\\K^m\\) i \\(\\calb&#39;&#39;\\) de \\(\\K^r\\), \\(A\\in M_{m\\times n}(\\K)\\) i \\(f_A\\colon \\K^m\\to \\K^n\\) l’aplicació lineal induïda per \\(A\\) tal i com hem definit a l’Exemple 2.1 i \\(\\Id_n\\colon\\K^n \\to \\K^n\\) és l’aplicació lineal definida per \\(\\Id(\\vec v)=\\vec v\\) (que també es pot pensar com \\(f_{\\1_n}\\)) Llavors: \\([\\Id_n]_{\\calb}=\\1_n\\) (per a qualsevol base \\(\\calb\\) de \\(\\K^n\\)), \\[[\\Id_n]_{\\calb,\\cals_n}= \\begin{pmatrix} \\mid &amp; \\mid &amp;\\mid &amp; \\mid\\\\ \\vec v_1 &amp; \\vec v_2 &amp;\\cdots&amp;\\vec v_n\\\\ \\mid &amp; \\mid &amp;\\mid &amp; \\mid \\end{pmatrix},\\] la matriu que té per columnes els vectors de \\(\\calb\\). \\([\\Id_n]_{\\cals_n,\\calb}=[\\Id_n]_{\\calb,\\cals_n}^{-1}\\). \\([g\\circ f]_{\\calb,\\calb&#39;&#39;}=[g]_{\\calb&#39;,\\calb&#39;&#39;}\\cdot[f]_{\\calb,\\calb&#39;}\\) per a totes \\(f\\colon \\K^n\\to \\K^m\\) i \\(g\\colon \\K^m\\to\\K^r\\) aplicacions lineals. \\(A=[f_A]_{\\cals_n,\\cals_m}\\). \\([f_A]_{\\calb,\\calb&#39;}= [\\Id_m]_{\\calb&#39;,\\cals_m}^{-1} \\cdot A \\cdot [\\Id_n]_{\\calb,\\cals_n}\\). Prova. (a), (b) i (e) es dedueixen de la definició. Si demostrem (d), es deduiran la resta de resultats, ja que utilitzen la relació entre composició d’aplicacions lineals i el producte de matrius en les bases corresponents. Per demostrar (d) utilitzem directament la Definició 2.13 amb l’expressió de l’Equació (??): si \\(\\vec v\\in \\K^n\\) i \\([\\vec v]_\\calb=\\begin{pmatrix}\\lambda_1\\\\\\vdots\\\\\\lambda_n\\end{pmatrix}\\), llavors, les coordenades en la base \\(\\calb&#39;\\) d’\\(f(\\vec v)\\) són=\\([f(\\vec v)]_\\calb = \\begin{pmatrix}\\mu_1\\\\\\vdots\\\\\\mu_m\\end{pmatrix}\\), on: \\[\\begin{pmatrix}\\mu_1\\\\ \\vdots \\\\ \\mu_m \\end{pmatrix}=[f]_{\\calb,\\calb&#39;}\\begin{pmatrix}\\lambda_1 \\\\ \\vdots \\\\ \\lambda_n \\end{pmatrix}\\] I les coordenades de \\(g(f(v))\\) en la base \\(\\calb&#39;&#39;\\) són \\([g(f(v))]_{\\calb&#39;&#39;}=(\\gamma_1,\\dots,\\gamma_r)\\), on: \\[\\begin{pmatrix}\\gamma_1\\\\ \\vdots \\\\ \\gamma_r \\end{pmatrix}=[g]_{\\calb&#39;,\\calb&#39;&#39;}\\begin{pmatrix}\\mu_1 \\\\ \\vdots \\\\ \\mu_m \\end{pmatrix}= [g]_{\\calb&#39;,\\calb&#39;&#39;} [f]_{\\calb,\\calb&#39;}\\begin{pmatrix}\\lambda_1 \\\\ \\vdots \\\\ \\lambda_n \\end{pmatrix}\\] I per tant tenim (d): aquesta propietat caracteritza la matriu \\([g\\circ f]_{\\calb,\\calb&#39;&#39;}\\). Una manera de trobar aquestes igualtats és escriure les aplicacions en forma de diagrama, composant-les (posem les bases com a subíndex, amb \\(\\cals\\) sense especificar la dimensió, que es dedueix de l’espai): \\[\\xymatrix{ (\\K^n)_\\cals \\ar[r]^{f_A} \\ar@{&lt;-&gt;}[d]_\\Id &amp; (\\K^m)_\\cals \\ar@{&lt;-&gt;}[d]^\\Id \\\\ (\\K^n)_\\calb \\ar[r]^{f_A} &amp; (\\K^m)_{\\calb&#39;}}\\] Veiem que per anar de \\((\\K^n)_\\calb\\) a \\((\\K^m)_{\\calb&#39;}\\) podem seguir dos camins, amb el mateix resultat (diem que el diagrama és commutatiu). Si ara ho escrivim en matrius (de les fletxes verticals tant sols hem deixat la que dóna la matriu): \\[\\xymatrixcolsep{6em} \\xymatrix{ (\\K^n)_\\cals \\ar[r]^{A} &amp; (\\K^m)_\\cals \\\\ (\\K^n)_\\calb \\ar[r]^{[f_A]_{\\calb,\\calb&#39;}} \\ar[u]^B &amp; (\\K^m)_{\\calb&#39;} \\ar[u]_{B&#39;}}\\] on \\(B=[Id_n]_{\\calb,\\cals_n}\\) i \\(B&#39;=[\\Id_m]_{\\calb&#39;,\\cals_m}\\) (les matrius que tenen per columnes les bases \\(\\calb\\) i \\(\\calb&#39;\\) respectivament). Si ara tenim en compte que les un vector en les coordenades de \\((\\K^n)_\\calb\\) pot seguir els dos camins per arribar a \\((\\K^m)_{\\cals}\\), obtenim una igualtat equivalent a l’apartat (f) de la Proposició 2.14. Considerem ara el cas particular \\(f\\colon \\K^n \\to K^n\\) (\\(n=m\\) al que hem fet fins ara). Llavors podem identificar el \\(\\K_n\\) de l’espai de sortida amb el d’arribada, i, si considerem \\(\\calb\\) una base de \\(\\K^n\\), la podem considerar com a base de l’espai de sortida i el d’arribada. De l’apartat (f) de la Proposició 2.14, obtenim la igualtat de matrius: \\[[f]_{\\calb,\\calb}=[f]_\\calb=[\\Id_n]_{\\calb,\\cals_n}^{-1} [f]_{\\cals_n,\\cals_n} [\\Id_n]_{\\calb,\\cals_n}\\] Per tant si anomenem \\(B=[\\Id_n]_{\\calb,\\cals_n}\\), \\(A=[f]_{\\cals_n,\\cals_n}\\) i \\(A&#39;=[f]_{\\calb,\\calb}\\) tenim que: \\[A&#39;=B^{-1} A B \\,.\\] Això ens porta a la definició següent: Definició 2.14 Diem que \\(A\\) i \\(A&#39;\\), dues matrius quadrades \\(n\\times n\\), són similars si existeix una matriu invertible \\(B\\in M_{n\\times n}(\\K)\\) tal que \\[A&#39;=B^{-1}AB \\,.\\] O sigui, si corresponen a una mateixa aplicació lineal, però amb bases diferents. Proposició 2.15 La relació “ser similar a” és una relació d’equivalència, o sigui, compleix: \\(A\\) és similar a \\(A\\) (reflexiva), si \\(A\\) és similar a \\(A&#39;\\), llavors \\(A&#39;\\) és similar a \\(A\\) (simètrica), si \\(A\\) és similar a \\(A&#39;\\) i \\(A&#39;\\) és similar a \\(A&#39;&#39;\\), llavors \\(A\\) és similar a \\(A&#39;&#39;\\) (transitiva). Prova. Per demostrar la propietat reflexiva, observem que \\(A=\\1_n^{-1} A \\1_n\\). Per demostrar la propietat simètrica, observem que si \\(A&#39;=B^{-1}AB\\), llavors \\(A=BA&#39;B^{-1}\\), per tant considerem \\(B^{-1}\\) a la definició. Per demostrar la propietat transitiva: l’enunciat ens diu que existeixen matrius invertibles \\(B\\) i \\(B&#39;\\) tals que: \\[A&#39;=B^{-1}AB \\text{ i } A&#39;&#39;=(B&#39;)^{-1}A&#39;B&#39;\\] llavors tenim: \\[A&#39;&#39;=(B&#39;)^{-1}A&#39;B&#39;=(B&#39;)^{-1}(B^{-1}AB)B&#39;=(BB&#39;)^{-1}A(BB&#39;)\\] per tant, \\(A&#39;&#39;\\) i \\(A&#39;\\) són similars. Exemple 2.12 Són les matrius \\(A=\\big(\\begin{smallmatrix} -16 &amp; -45 \\\\ 6 &amp; 17\\end{smallmatrix}\\big)\\) i \\(A&#39;=\\big(\\begin{smallmatrix} -1 &amp; 0 \\\\ 0 &amp; 2 \\end{smallmatrix}\\big)\\) similars? Per respondre la pregunta necessitem calcular la \\(B\\) tal que \\(A&#39;=B^{-1}AB\\), o bé demostrar que no existeix aquesta \\(B\\). Intentem calcular-la: \\[B=\\begin{pmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\end{pmatrix}\\] i ha de complir que \\(BA&#39;=AB\\) (així ens estalviem d’escriure la inversa), obtenint que: \\[\\begin{pmatrix} -b_{11} &amp; 2b_{12} \\\\ -b_{21} &amp; 2b_{22} \\end{pmatrix} = \\begin{pmatrix} -16b_{11}-45b_{21} &amp; -16b_{12}-45b_{22}\\\\ 6b_{11}+17b_{21} &amp; 6b_{12}+17b_{22} \\end{pmatrix}\\] Igualant coeficient a coeficient arribem a un sistema d’equacions lineal amb 4 incògnites i 4 equacions, que si el resolem té 2 paràmetres lliures i una possible solució particular és \\[B=\\begin{pmatrix} 3 &amp; -5 \\\\ -1 &amp; 2 \\end{pmatrix}\\] i per tant \\(A\\) i \\(A&#39;\\) són similars. Exemple 2.13 Són les matrius \\(A=\\big(\\begin{smallmatrix} -16 &amp; -45 \\\\ 6 &amp; 17\\end{smallmatrix}\\big)\\) i \\(A&#39;=\\big(\\begin{smallmatrix} 1 &amp; 0 \\\\ 0 &amp; 2 \\end{smallmatrix}\\big)\\) similars? (així aprofitem els càlculs d’abans). Intentem calcular \\(B\\) definida com: \\[B=\\begin{pmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\end{pmatrix}\\] i ha de complir que és invertible i \\(BA&#39;=AB\\), obtenint que: \\[\\begin{pmatrix} b_{11} &amp; 2b_{12} \\\\ b_{21} &amp; 2b_{22} \\end{pmatrix} = \\begin{pmatrix} -16b_{11}-45b_{21} &amp; -16b_{12}-45b_{22}\\\\ 6b_{11}+17b_{21} &amp; 6b_{12}+17b_{22} \\end{pmatrix}\\] Igualant coeficient a coeficient arribem a un sistema d’equacions lineal amb 4 incògnites i 4 equacions, que si el resolem té 1 paràmetre lliure i les solucions són \\[B=\\begin{pmatrix} 0 &amp; -5\\lambda \\\\ 0 &amp; 2\\lambda \\end{pmatrix}\\] Observem que aquesta matriu mai pot ser invertible (té una columna tot zeros) i per tant \\(A\\) i \\(A&#39;\\) no són similars. Veiem que hem hagut de fer molts càlculs per comprovar si són similars o no. Més endavant, veurem resultats que ens permetrà determinar si són similars amb menys càlculs. Exercici 2.4 Determineu per a quins valors de \\(\\lambda_1, \\lambda_2, \\mu_1\\) i \\(\\mu_2\\) les matrius \\[A=\\begin{pmatrix} \\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2 \\end{pmatrix} \\text{ i } A&#39;=\\begin{pmatrix} \\mu_1 &amp; 0 \\\\ 0 &amp; \\mu_2 \\end{pmatrix}\\] són similars. 2.7 Espais vectorials Fins ara hem estat treballant amb \\(\\K^n\\), però tan sols estàvem utilitzant algunes de les propietats que té \\(\\K^n\\). Quan un té uns resultats, és habitual intentar veure si es poden generalitzar a altres objectes. Exemple 2.14 Considerem els polinomis en una variable amb coeficients en \\(\\K\\) de grau més petit o igual que 2. Per tant, un element serà \\(p(x)=a_0+a_1x+a_2x^2\\) amb \\(a_i\\in \\K\\). Anomenem \\(\\K[x]_{\\leq2}\\) a aquest conjunt. Observem que a \\(\\K[x]_{\\leq2}\\) tenim: Una operació suma: donats dos polinomis de grau més petit o igual a 2, els podem sumar, i tenim un polinomi de grau més petit o igual a 2. Aquesta suma és commutativa, associativa, té un element neutre (el polinomi 0) i per a qualsevol \\(p(x)\\in\\K[x]_{\\leq2}\\), existeix un altre \\(q(x)\\in\\K[x]_{\\leq2}\\) tal que \\(p(x)+q(x)=0\\). Una operació producte per escalar: donat un \\(\\lambda \\in \\K\\) i un polinomi \\(p(x)\\in\\K[x]_{\\leq2}\\), podem fer la multiplicació \\(\\lambda p(x)\\) i, com que no ha crescut el grau, \\(\\lambda p(x)\\in\\K[x]_{\\leq2}\\). Aquesta operació compleix que multiplicar per \\(1\\) és no fer res, és distributiva respecte la suma d’escalars i polinomis, i que \\(\\lambda(\\mu p(x))=(\\lambda\\mu)p(x)\\). Aquestes propietats impliquen que podem fer tot el que fèiem a \\(\\K^n\\), canviant les coordenades pels coeficients a cada grau. Més concretament, si considerem l’aplicació: \\[\\begin{array}{rcl} f\\colon \\K[x]_{\\leq2} &amp; \\longrightarrow &amp; \\K^3 \\\\ a_0+a_1x+a_2x^2 &amp; \\mapsto &amp; (a_0,a_1,a_2) \\end{array}\\] compleix que: És lineal: \\(f(p(x)+q(x))=f(p(x))+f(q(x))\\) i \\(f(\\lambda p(x))=\\lambda f(p(x))\\) per a tots \\(p(x),q(x)\\in \\K[x]_{\\leq2}\\) i \\(\\lambda \\in \\K\\). És bijectiva (i per tant un isomorfisme). I això voldrà dir que tot el que hem fet a \\(\\K^3\\) i té a veure amb aquesta estructura (suma de vectors i multiplicació per escalar) també es podrà fer a \\(\\K[x]_{\\leq2}\\). Definició 2.15 Un \\(\\K\\)-espai vectorial \\(E\\) és un conjunt \\(E\\) amb dues operacions, suma i producte per escalar: \\[\\begin{array}{rrcl} +\\colon &amp; E\\times E &amp; \\longrightarrow &amp; E \\\\ &amp; (u,v) &amp; \\mapsto &amp; u+v \\end{array} \\text{ i } \\begin{array}{rrcl} \\cdot\\colon &amp; \\K\\times E &amp; \\longrightarrow &amp; E \\\\ &amp; (\\lambda,v) &amp; \\mapsto &amp; \\lambda \\cdot v \\end{array}\\] tals que: La suma és commutativa: \\(u+v=v+u\\) per a tots \\(u,v\\in E\\), associativa: \\((u+v)+w=u+(v+w)\\) per a tots \\(u,v,w \\in E\\), té element neutre: existeix \\(0=0_E\\in E\\) tal que \\(0+u=u\\) per a tot \\(u\\in E\\) i tot \\(u\\in E\\) té un element invers \\(v\\) complint que \\(u+v=0\\) (escrivim \\(v=-u\\)). El producte per escalar compleix que \\(1\\cdot u=u\\) per a tot \\(u\\in E\\), \\((\\lambda \\mu)\\cdot u=\\lambda \\cdot(\\mu\\cdot u)\\) per a tot \\(\\lambda, \\mu \\in \\K\\) i \\(u\\in E\\), \\((\\lambda + \\mu)\\cdot u=(\\lambda \\cdot u) + (\\mu\\cdot u)\\) per a tot \\(\\lambda, \\mu \\in \\K\\) i \\(u\\in E\\) i \\(\\lambda\\cdot (u+v)=(\\lambda \\cdot u) + (\\lambda\\cdot v)\\) per a tot \\(\\lambda \\in \\K\\) i \\(u,v\\in E\\). Exemple 2.15 Els conjunts \\(\\K^n\\) amb les operacions que s’estan utilitzant a aquest curs són un \\(\\K\\)-espai vectorial. Les matrius \\(M_{m\\times n}(\\K)\\), amb la suma coeficient a coeficient i la multiplicació per escalar (que afecta a tots els coeficients) també és un \\(\\K\\)-espai vectorial. Els polinomis de \\(\\K[x]\\) (no limitem el grau!). Si \\(A\\) és un conjunt i \\(\\K\\) un cos, considerem \\(\\Map(A,\\K)\\) el conjunt de les aplicacions de conjunts \\(f\\colon A \\to \\K\\). Si \\(f,g\\in\\Map(A,\\K)\\), definim \\((f+g)(x)=f(x)+g(x)\\) i \\((\\lambda f)(x)=\\lambda f(x)\\) (per a tot \\(\\lambda \\in K\\)), llavors \\(\\Map(A,\\K)\\) és un \\(\\K\\)-espai vectorial (veiem que poder sumar funcions i multiplicar-les per escalar depèn de que ho puguem fer al cos \\(\\K\\)). El vector zero és la funció constant \\(f(a)=0\\) per a tot \\(a\\in A\\). Si \\(A=\\N\\), podem identificar \\(f\\in \\Map(\\N,\\K)\\) com la successió d’escalars \\((x_0,x_1,\\dots)\\) amb \\(x_i=f(i)\\in \\K\\). Si considerem les funcions contínues \\(f\\colon \\R \\to \\R\\) amb l’estructura de l’apartat anterior, són un espai vectorial (ja que la suma de contínues és contínua i la multiplicació d’un escalar per una funció contínua és una funció contínua). L’espai d’equacions lineals amb \\(n\\) incògnites: \\[\\{a_1x_1+a_2x_2+\\cdots+a_nx_n=b ~\\colon~ a_i, b \\in\\K\\},\\] amb la suma d’equacions variable a variable (i terme independent més terme independent), i producte per escalar que multiplica tota l’equació. El vector zero seria l’equació \\(0=0\\). La resta d’aquesta secció la dedicarem a generalitzar conceptes que ja hem vist per \\(\\K^n\\), a espais vectorials abstractes. Definició 2.16 Un subconjunt \\(W\\subseteq V\\) d’un \\(\\K\\)-espai vectorial \\(V\\) és un subespai de \\(V\\) si: El vector \\(0_V\\) pertany a \\(W\\). Si \\(w_1\\) i \\(w_2\\) són elements de \\(W\\), aleshores la seva suma \\(w_1+w_2\\) també és a \\(W\\) (direm que \\(W\\) és tancat per la suma). Si \\(w\\) és de \\(W\\) i \\(\\lambda \\in\\K\\), aleshores \\(\\lambda w\\) també pertany a \\(W\\) (direm que \\(W\\) és tancat pel producte per escalars). Observació. Notem que un subespai \\(W\\) d’un \\(\\K\\)-espai vectorial \\(V\\) és un \\(\\K\\)-espai vectorial per si sol (amb la suma i producte per escalars heredat de \\(V\\)). Definició 2.17 Considerem un subconjunt \\(F\\) d’un \\(\\K\\)-espai vectorial \\(V\\). Diem que \\(F\\) genera \\(V\\) si tot \\(v\\in V\\) es pot escriure com a combinació lineal d’alguns elements \\(v_1,\\ldots, v_n\\) de \\(F\\). Diem que \\(F\\) és un conjunt linealment dependent si algun \\(v\\in F\\) es pot escriure com a combinació lineal d’altres. Diem que \\(F\\) és linealment independent si no és linealment dependent. De manera equivalent, el conjunt \\(F\\) és linealment independent si l’única manera d’obtenir \\(\\vec 0\\) com a combinació lineal \\(\\lambda_1v_1+\\cdots+ \\lambda_nv_n\\) amb \\(v_1,\\ldots v_n\\in F\\) és amb \\(\\lambda_1=\\cdots=\\lambda_n=0\\). El conjunt \\(F\\) és una base si genera \\(V\\) i és linealment independent. Observació. Hi ha espais vectorials que tenen bases amb infinits elements. Per exemple, l’espai vectorial \\(\\K[x]\\) format pels polinomis amb coeficients a \\(\\K\\) té base \\[[1,x,x^2,x^3,x^4,\\ldots].\\] Definició 2.18 Direm que un \\(\\K\\)-espai vectorial \\(V\\) té dimensió finita si té una base \\(F=[v_1,\\ldots,v_n]\\) formada per un nombre finit de vectors. En cas contrari, diem que \\(V\\) té dimensió infinita. De la mateixa manera que hem demostrat el Teorema 2.3, si \\(V\\) té dimensió finita aleshores totes les bases tenen el mateix nombre d’elements. En aquest cas, direm que la dimensió de \\(V\\) és el nombre d’elements d’una base qualsevol. Exemple 2.16 Considerem l’espai vectorial \\(\\calc^\\infty(\\R)\\) format per les funcions diferenciables. Considerem el subconjunt: \\[V=\\{ f \\in \\calc^\\infty(\\R) \\mid f&#39;&#39;+f=0\\}.\\] Demostrem primer que \\(V\\subset\\calc^\\infty(\\R)\\) és un subespai vectorial: La funció \\(f=0\\) compleix l’equació, ja que \\(f=f&#39;=f&#39;&#39;=0\\) i per tant \\(f&#39;&#39;+f=0\\). Si \\(f\\in V\\) i \\(g\\in V\\), tenim \\(f&#39;&#39;+f=0\\) i \\(g&#39;&#39;+g=0\\). Tenim que \\((f+g)&#39;&#39;=f&#39;&#39;+g&#39;&#39;\\), i per tant: \\[(f+g)&#39;&#39;+(f+g)=f&#39;&#39;+g&#39;&#39;+f+g=f&#39;&#39;+f+g+g&#39;&#39;=0+0=0\\,,\\] per tant \\(f+g\\in V\\). Si \\(f\\in V\\) i \\(\\lambda\\in\\R\\), tenim \\((\\lambda f)&#39;&#39;=\\lambda f&#39;&#39;\\), per tant: \\[(\\lambda f)&#39;&#39;+\\lambda f=\\lambda f&#39;&#39;+\\lambda f=\\lambda(f&#39;&#39;+f)=\\lambda 0=0 \\, ,\\] i per tant \\(\\lambda f \\in V\\). Intentem calcular la dimensió de \\(V\\): del curs de càlcul sabem que les funcions \\(\\sin\\) i \\(\\cos\\) compleixen aquesta equació, i com que no són múltiple una de l’altra, \\(\\dim(V)\\geq 2\\). Suposem que tenim tres funcions \\(f,g,h\\in V\\) i considerem la matriu (on els coeficients són funcions!): \\[\\begin{pmatrix} f &amp; g &amp; h\\\\ f&#39; &amp; g&#39; &amp; h&#39; \\\\ f&#39;&#39; &amp; g&#39;&#39; &amp; h&#39;&#39; \\end{pmatrix}\\] I, si apliquem la condició \\(f&#39;&#39;=-f\\), \\(g&#39;&#39;=-g\\), \\(h&#39;&#39;=-h\\) i en calculem el rang tenim: \\[\\Rang\\begin{pmatrix} f &amp; g &amp; h\\\\ f&#39; &amp; g&#39; &amp; h&#39; \\\\ f&#39;&#39; &amp; g&#39;&#39; &amp; h&#39;&#39; \\end{pmatrix} = \\Rang\\begin{pmatrix} f &amp; g &amp; h\\\\ f&#39; &amp; g&#39; &amp; h&#39; \\\\ -f &amp; -g &amp; -h \\end{pmatrix}= \\Rang\\begin{pmatrix} f &amp; g &amp; h\\\\ f&#39; &amp; g&#39; &amp; h&#39; \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}\\leq 2,\\] on a la segona igualtat hem sumat la primera fila a la tercera. Per tant hi ha una combinació lineal entre els vectors \\(\\left(\\begin{smallmatrix}f\\\\f&#39;\\\\f&#39;&#39;\\end{smallmatrix}\\right)\\), \\(\\left(\\begin{smallmatrix}g\\\\g&#39;\\\\g&#39;&#39;\\end{smallmatrix}\\right)\\) i \\(\\left(\\begin{smallmatrix}h\\\\h&#39;\\\\h&#39;&#39;\\end{smallmatrix}\\right)\\), en particular entre \\(f\\), \\(g\\) i \\(h\\), pel que no poden ser linealment independents i \\(\\dim(V)&lt;3\\). Per tant, hem demostrat que \\(\\dim(V)=2\\) i una possible base és \\([\\sin,\\cos]\\). Una demostració més explícita del fet que \\(\\sin\\) i \\(\\cos\\) generen \\(V\\) és la següent: demostrem primer que, si \\(g\\in V\\), aleshores \\[C_g = g^2 + (g&#39;)^2\\] és una funció constant. Això es veu calculant la derivada, i veient que és zero: \\[\\frac{d}{dx} (g^2+(g&#39;)^2) = 2gg&#39; + 2g&#39;g&#39;&#39; = 2gg&#39;-2g&#39;g = 0.\\] Per tant, \\(C_g = g(0)^2 + g&#39;(0)^2\\), per exemple. Donada \\(f\\in V\\), apliquem aquest resultat a la funció \\[x\\mapsto g(x) = f(x)-f(0)\\cos(x) - f&#39;(0)\\sin(x),\\] que és de \\(V\\) si assumim que \\(f\\) ho és. Aleshores \\(C_g = g(0)^2 + g&#39;(0)^2 = 0 + 0 = 0\\), i per tant \\((g(x))^2+(g&#39;(x))^2=0\\) per tot \\(x\\), i com que és la suma de dos quadrats a \\(\\R\\), obtenim que tant \\(g(x)=0\\) com \\(g&#39;(x)=0\\) per tot \\(x\\in\\R\\). D’aquí es dedueix que \\(f(x) = f(0)\\cos(x) + f&#39;(0)\\sin(x)\\). Definició 2.19 Sigui \\(V\\) un \\(\\K\\)-espai vectorial de dimensió finita \\(n\\), i sigui \\(\\calb = [v_1,\\ldots ,v_n]\\) una base ordenada. Aleshores tot element \\(v\\in V\\) es pot escriure de manera única com \\[v= \\lambda_1v_1+\\cdots +\\lambda_n v_n.\\] Definim les coordenades de \\(v\\) en la base \\(\\calb\\) com \\[[v]_{\\calb} = \\begin{pmatrix}\\lambda_1\\\\\\vdots\\\\\\lambda_n\\end{pmatrix}.\\] A continuació introduirem el concepte d’aplicació lineal entre espais vectorials sobre el mateix cos \\(\\K\\). De manera informal, una aplicació és lineal si respecta les operacions bàsiques dels espais vectorials: la suma i el producte per escalars. Definició 2.20 Una aplicació \\(f\\colon V\\to W\\) entre dos \\(\\K\\)-espais vectorials \\(V\\) i \\(W\\) és una aplicació lineal si: \\(f(v_1+v_2) = f(v_1) + f(v_2)\\) per a tot \\(v_1,v_2\\in V\\), i \\(f(\\lambda v) = \\lambda f(v)\\) per a tot \\(v\\in V\\) i tot \\(\\lambda\\in\\K\\). Exemple 2.17 Si \\(V\\) és un \\(\\K\\)-espai vectorial de dimensió finita i \\(\\calb\\) és una base, l’aplicació \\([\\cdot]_\\calb\\colon V\\to \\K^n\\) és una aplicació lineal. Definició 2.21 Si \\(f\\colon V\\to W\\) és una aplicació lineal entre \\(\\K\\)-espais vectorials \\(V\\) i \\(W\\), definim el nucli i la imatge com els subespais \\[\\ker(f) = \\{v\\in V~\\colon~ f(v)=\\vec 0\\}\\subset V,\\] \\[\\Ima(f) = \\{f(v)~\\colon~ v\\in V\\}\\subset W.\\] Exemple 2.18 Considerem l’espai vectorial \\(\\calc^\\infty(\\R)\\) format per les funcions diferenciables. L’aplicació \\(D\\colon \\calc^\\infty(\\R)\\to \\calc^\\infty(\\R)\\), definida com \\(D(f) = f&#39;\\) (la derivada) és una aplicació lineal. El nucli de \\(D\\) és \\[\\ker(D)=\\{f~\\colon~ f&#39;=0\\} = \\{\\text{funcions constants}\\},\\] que és un subespai de dimensió \\(1\\). Donada una funció \\(g\\in\\calc^\\infty(\\R)\\), el teorema fonamental del càlcul ens diu que podem trobar una funció \\(G\\) (una primitiva de \\(g\\)) tal que \\(G&#39;=g\\). Per tant \\(\\Ima(D)=\\calc^\\infty(\\R)\\). Si \\(f\\colon V\\to W\\) és bijectiva (equivalentment, \\(\\ker f = \\{\\vec 0\\}\\) i \\(\\Ima(f)=W\\)), direm que \\(f\\) és invertible, o que és un isomorfisme. També direm que els espais \\(V\\) i \\(W\\) són isomòrfics, i escriurem \\(V\\cong W\\). Exemple 2.19 Sigui \\(V\\) un \\(\\K\\)-espai vectorial de dimensió \\(n\\). Si triem una base \\(\\calb\\) de \\(V\\), aleshores l’aplicació coordenada \\([\\cdot]_\\calb\\colon V\\to\\K^n\\) és un isomorfisme. Per tant, tot \\(\\K\\)-espai vectorial \\(V\\) de dimensió \\(n&lt;\\infty\\) és isomòrfic a \\(\\K^n\\), però no tenim un isomorfisme concret de \\(V\\) amb \\(\\K^n\\) fins que triem una base. Proposició 2.16 Siguin \\(V\\) i \\(W\\) dos \\(\\K\\)-espais vectorials de dimensió finita. Aleshores: Els espais \\(V\\) i \\(W\\) són isomorfs si i només si \\(\\dim(V)=\\dim(W)\\). Si \\(f\\colon V\\to W\\) és una aplicació lineal amb \\(\\ker f=\\{0\\}\\), i \\(\\dim(V)=\\dim(W)\\), aleshores \\(f\\) és un isomorfisme. Si \\(f\\colon V\\to W\\) és una aplicació lineal amb \\(\\Ima(f)=W\\), i \\(\\dim(V)=\\dim(W)\\), aleshores \\(f\\) és un isomorfisme. Observació. La hipòtesi de dimensió finita és necessària: a l’Exemple 2.18 hem vist com la derivada no és un isomorfisme (perquè té nucli no zero), però en canvi és exhaustiu. Finalment, veurem com podem associar matrius a aplicacions lineals entre espais vectorials. Sigui \\(f\\colon V\\to W\\) una aplicació lineal entre els \\(\\K\\)-espais vectorials \\(V\\) i \\(W\\), que suposem de dimensió finita, posem \\(\\dim(V)=m\\) i \\(\\dim(W)=n\\). Escollim bases \\(\\calb=[v_1,\\ldots v_m]\\) de \\(V\\) i \\(\\calc=[w_1,\\ldots,w_n]\\) de \\(W\\). Aleshores tenim el diagrama \\[\\begin{align*} \\tag{2.1} \\xymatrix{ V\\ar[r]^f\\ar[d]^{\\cong}_{[\\cdot]_{\\calb}} &amp; W\\ar[d]_{\\cong}^{[\\cdot]_{\\calc}}\\\\ \\K^m\\ar@{--&gt;}[r]&amp;\\K^n. } \\end{align*}\\] Fixem-nos que l’aplicació resultant de resseguir el diagrama d’esquerra a dreta és una aplicació lineal de \\(\\K^m\\) a \\(\\K^n\\) i, per tant, té una matriu associada. La columna \\(i\\)-èssima d’aquesta matriu és \\([f(v_i)]_\\calc\\). Definició 2.22 La matriu de l’aplicació lineal \\(f\\) en les bases \\(\\calb\\) i \\(\\calc\\) és la matriu \\[[f]_{\\calb,\\calc}=\\begin{pmatrix} \\mid&amp;\\cdots&amp;\\mid\\\\ [f(v_1)]_\\calc&amp;\\cdots&amp;[f(v_m)]_\\calc\\\\ \\mid &amp; \\cdots &amp;\\mid \\end{pmatrix}\\] Si l’espai de sortida és el mateix que el d’arribada, i escrivim la mateixa base tant per la sortida com l’arribada, escrivim \\([f]_\\calb\\) enlloc de \\([f]_{\\calb,\\calb}\\). Un cas particular interessant es dona quan considerem l’aplicació identitat: Definició 2.23 Siguin \\(\\calb\\) i \\(\\calc\\) dues bases d’un \\(\\K\\)-espai vectorial \\(V\\). Aleshores la matriu de canvi de base de \\(\\calb\\) a \\(\\calc\\) és la matriu \\[S_{\\calb,\\calc} = [\\Id]_{\\calb,\\calc}.\\] El cas més general que podem considerar és el següent. Siguin \\(V\\) i \\(W\\) com fins ara, i prenem bases \\(\\calb\\) i \\(\\calb&#39;\\) de \\(V\\), i bases \\(\\calc\\) i \\(\\calc&#39;\\) de \\(W\\). Aleshores enganxant quatre còpies del Diagrama (2.1) obtenim el diagrama següent: \\[\\xymatrix@C50pt@R50pt{ \\K^m\\ar[rrr]^{[f]_{\\calb,\\calc}}\\ar[dd]_{S_{\\calb,\\calb&#39;}}&amp;&amp;&amp; \\K^n\\ar[dd]^{S_{\\calc,\\calc&#39;}}\\\\ &amp;V\\ar[r]^f\\ar[ul]_{[\\cdot]_\\calb}\\ar[dl]^{[\\cdot]_{\\calb&#39;}}&amp;W\\ar[ur]^{[\\cdot]_{\\calc}}\\ar[dr]_{[\\cdot]_{\\calc&#39;}}\\\\ \\K^m\\ar[rrr]^{[f]_{\\calb&#39;,\\calc&#39;}}&amp;&amp;&amp;\\K^n. }\\] Observem que les quatre fletxes exteriors tenen matrius associades, però que les cinc fletxes interiors no en tenen pas. En tot cas, les fletxes exteriors ens donen la següent fórmula, que ens relaciona la matriu de \\(f\\) en les bases \\(\\calb,\\calc\\) i la matriu de \\(f\\) en les bases \\(\\calb&#39;,\\calc&#39;\\). \\[[f]_{\\calb&#39;,\\calc&#39;} S_{\\calb,\\calb&#39;} = S_{\\calc,\\calc&#39;} [f]_{\\calb,\\calc}.\\] Exemple 2.20 Considerem \\(\\R[x]_{\\leq2}\\), els polinomis de grau més petit o igual que \\(2\\) com a \\(\\R\\)-espai vectorial amb la base \\(\\calb=[1,x,x^2]\\). Per \\(a\\in \\R\\), considerem l’aplicació lineal: \\[\\begin{array}{cccc} f_a\\colon &amp; \\R[x]_{\\leq2} &amp; \\longrightarrow &amp; \\R^3 \\\\ &amp; p(x) &amp; \\mapsto &amp; \\begin{pmatrix} p(a) \\\\ p&#39;(a) \\\\ p&#39;&#39;(a) \\end{pmatrix} \\end{array}\\] Considerem les imatges dels elements de \\(\\calb\\): \\[f_a(1)=\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\text{, } f_a(x)=\\begin{pmatrix} a \\\\ 1 \\\\ 0 \\end{pmatrix} \\text{ i } f_a(x^2)=\\begin{pmatrix} a^2 \\\\ 2a \\\\ 2 \\end{pmatrix}.\\] Per tant, si \\(\\calc=[\\vec e_1, \\vec e_2, \\vec e_3]\\) és la base d’\\(\\R^3\\) formada pels vectors estàndard, tenim: \\[[f_a]_{\\calb,\\calc}=\\begin{pmatrix} 1 &amp; a &amp; a^2 \\\\ 0 &amp; 1 &amp; 2a \\\\ 0 &amp; 0 &amp; 2\\end{pmatrix}.\\] En particular, deduïm que \\(f_a\\) és un isomorfisme (la matriu \\([f_a]_{\\calb,\\calc}\\) és invertible). Considerem ara la base de \\(\\R[x]_{\\leq2}\\) formada pels vectors \\(\\calb&#39;=[1,(x-a),(x-a)^2]\\). En aquest cas: \\[f_a(1)=\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\text{, } f_a(x-a)=\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\text{ i } f_a((x-a)^2)=\\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix}.\\] I per tant: \\[[f_a]_{\\calb&#39;,\\calc}=\\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 2\\end{pmatrix}.\\] Llavors, el diagrama que tenim és: \\[\\xymatrix@C50pt@R50pt{ \\R^3 \\ar[rrd]^{[f_a]_{\\calb,\\calc}} &amp; &amp; \\\\ &amp; \\R[x]_{\\leq2} \\ar[ul]^{[\\cdot]_\\calb} \\ar[r]^{f_a} \\ar[dl]_{[\\cdot]_{\\calb&#39;}} &amp; \\R^3 \\\\ \\R^3 \\ar[rru]_{[f_a]_{\\calb&#39;,\\calc}} \\ar[uu]^{S_{\\calb&#39;,\\calb}} &amp; &amp; }\\] On \\[S_{\\calb&#39;,\\calb}=\\begin{pmatrix} 1 &amp; -a &amp; a^2 \\\\ 0 &amp; 1 &amp; -2a \\\\ 0 &amp; 0 &amp; 1\\end{pmatrix}.\\] Podem comprovar que es compleix: \\[[f_a]_{\\calb&#39;,\\calc}=[f_a]_{\\calb,\\calc} S_{\\calb&#39;,\\calb} \\,,\\] és a dir, \\[\\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 2\\end{pmatrix} = \\begin{pmatrix} 1 &amp; a &amp; a^2 \\\\ 0 &amp; 1 &amp; 2a \\\\ 0 &amp; 0 &amp; 2\\end{pmatrix} \\begin{pmatrix} 1 &amp; -a &amp; a^2 \\\\ 0 &amp; 1 &amp; -2a \\\\ 0 &amp; 0 &amp; 1\\end{pmatrix}.\\] 2.8 Exercicis recomanats Els exercicis que segueixen són útils per practicar el material presentat. La numeració és la de [1]. Secció 2.1: 6, 8, 24, 26, 28, 30. Secció 2.2: 2, 6, 10, 20. Secció 2.3: 4, 12, 20. Secció 2.4: 4, 20, 28. Secció 3.1: 10, 16, 22. Secció 3.2: 6, 18, 24, 28. Secció 3.3: 16, 22, 32. Secció 3.4: 8, 22, 38. Secció 4.1: 1-5, 20, 26. Secció 4.2: 6, 10, 58, 66, 68, 74. Secció 4.3: 4, 14, 52, 54, 60. References "],["diagonalització.html", "Capítol 3. Diagonalització 3.1 Motivació 3.2 Determinants 3.3 Polinomi característic. Valors i vectors propis 3.4 Vectors propis associats a un valor propi 3.5 Interludi: els nombres complexos 3.6 Matrius sobre \\(\\R\\) 3.7 Exercicis recomanats", " Capítol 3. Diagonalització El contingut d’aquesta secció és pot trobar a [1 Temes 6, 7] i a [2 Tema 4]. 3.1 Motivació Considerem les matrius següents: \\[A=\\begin{pmatrix} -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} \\text{ i } B=\\begin{pmatrix} 5 &amp; -15 &amp; -21 \\\\ -3 &amp; 9 &amp; 13 \\\\ 3 &amp; -9 &amp; -13 \\end{pmatrix}\\] i suposem que volem calcular \\(A^5\\), \\(\\Rang(A)\\), \\(\\Ker(f_A)\\) o una base de \\(\\Ima(f_A)\\); i exactament els mateixos càlculs per \\(B\\). Per \\(A\\), aquests càlculs són quasi immediats: \\[A^5=\\begin{pmatrix} (-1)^5 &amp; 0 &amp; 0 \\\\ 0 &amp; 2^5 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} \\,\\] \\(\\Rang(A)=2\\), \\(\\Ker(f_A)=\\langle \\left( \\begin{smallmatrix} 0 \\\\ 0 \\\\ 1 \\end{smallmatrix} \\right) \\rangle\\) i una base de \\(\\Ima(f_A)\\) pot ser \\(\\calb=( \\left( \\begin{smallmatrix} -1 \\\\ 0 \\\\ 0 \\end{smallmatrix} \\right) , \\left( \\begin{smallmatrix} 0 \\\\ 2 \\\\ 0 \\end{smallmatrix} \\right))\\). Per la matriu \\(B\\), podem fer \\(B^5\\), però ens porta més càlculs, igual que calcular-ne el rang, el nucli de \\(f_B\\) o una base de la imatge de \\(f_B\\). Observem ara que hi ha una relació entre \\(A\\) i \\(B\\): \\[B=S A S^{-1}\\] on \\[S=\\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ -1 &amp; -1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\end{pmatrix}\\] i això permet aprofitar els càlculs que hem fet per \\(A\\) per deduir els de \\(B\\): \\[B^5=S A S^{-1} S A S^{-1} \\cdots S A S^{-1}=S A^5 S^{-1} \\, ,\\] tenim la igualtat \\(\\Rang(B)=\\Rang(A)=2\\), \\(\\Ker(f_B)=S\\Ker(f_A)\\) i una base de \\(\\Ima(f_B)\\) s’aconsegueix amb la base de \\(\\Ima(f_A)\\) multiplicada (per l’esquerra) per la matriu \\(S^{-1}\\). A aquest exemple, el que hem vist és que encara que les matrius \\(A\\) i \\(B\\) corresponen a una mateixa transformació lineal \\(f\\) expressada en dues bases diferents, els càlculs han quedat molt més fàcils amb la matriu \\(A\\) pel simple fet de ser una matriu diagonal. A aquest capítol, l’objectiu principal és: donada una aplicació lineal \\(f\\colon E \\to E\\) (amb \\(E\\) un espai vectorial de dimensió finita), trobar una base \\(\\calb\\) d’\\(E\\) tal que \\([f]_\\calb\\) sigui una matriu diagonal. També veurem que, a vegades, no existeix cap base amb aquesta propietat. 3.2 Determinants Probablement ja coneixem el determinant de matrius \\(2\\times 2\\) i \\(3\\times 3\\) definits directament com: \\[\\begin{align} \\tag{3.1} \\begin{vmatrix} a &amp; b \\\\ c &amp; d \\end{vmatrix}= ad -bc \\end{align}\\] i \\[\\begin{align} \\tag{3.2} \\begin{vmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; a_{23}\\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{vmatrix} = a_{11}a_{22}a_{33}-a_{11}a_{23}a_{32}-a_{12}a_{21}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}. \\end{align}\\] Una de les propietats principals és la següent: el determinant val zero si i només si els vectors columna de la matriu són linealment dependents. Si pensem les columnes de les matrius com a vectors de \\(\\K^n\\), podem considerar el determinant com una aplicació: \\[\\begin{array}{rcl} \\det \\colon M_n(\\K)=(\\K^n)^n &amp; \\longrightarrow &amp; \\K \\\\ (\\vec v_1, \\dots , \\vec v_n) &amp; \\mapsto &amp; \\det(\\vec v_1, \\dots , \\vec v_n) \\end{array}\\] i habitualment escriurem: \\[\\det(\\vec v_1, \\dots, \\vec v_n)=\\begin{vmatrix} \\mid &amp; \\mid &amp; &amp; \\mid \\\\ \\vec v_1 &amp; \\vec v_2 &amp; \\cdots &amp; \\vec v_n \\\\ \\mid &amp; \\mid &amp; &amp; \\mid \\end{vmatrix}\\] Demanarem que el determinant compleixi les propietats següents: \\(\\det(\\vec v_1, \\dots, \\lambda \\vec v_j, \\dots,\\vec v_n)=\\lambda \\det(\\vec v_1, \\dots, \\vec v_j, \\dots,\\vec v_n)\\) per a tots \\(\\vec v_j\\in \\K^n\\) i \\(\\lambda \\in \\K\\). \\(\\det(\\vec v_1, \\dots, \\vec v_{j-1}, \\vec v_j+\\vec w_j, \\vec v_{j+1} \\dots,\\vec v_n)= \\det(\\vec v_1, \\dots, \\vec v_{j-1},\\vec v_j,\\vec v_{j+1}, \\dots,\\vec v_n) +\\) \\(\\det(\\vec v_1, \\dots, \\vec v_{j-1},\\vec w_j,\\vec v_{j+1}, \\dots,\\vec v_n)\\) per a tots \\(\\vec v_j\\) i \\(\\vec w_j\\in \\K^n\\). \\(\\det(\\vec v_1,\\dots , \\vec v_j, \\dots, \\vec v_k, \\dots ,\\vec v_n)=0\\) si \\(\\vec v_j=\\vec v_k\\) amb \\(j\\neq k\\). \\(\\det(\\1_n)=\\det(\\vec e_1, \\dots, \\vec e_n)=1\\), on \\(\\vec e_j\\) són els vectors estàndard. Observació. Les propietats D1 i D2 es poden resumir dient que el determinant és lineal a cada columna. Es diu que el determinat és una aplicació multilineal. Exercici 3.1 Demostreu que els determinants de matrius \\(2\\times 2\\) i \\(3\\times 3\\) definits a les Equacions (3.1) i (3.2) respectivament compleixen aquestes propietats i que són les úniques aplicacions de \\(M_2(\\K) \\to \\K\\) i \\(M_3(\\K)\\to\\K\\) respectivament que les compleixen. Fixem-nos que la propietat D1 implica que si la matriu té una columna tota zero aleshores el determinant és zero: Lema 3.1 Tenim \\(\\det(\\vec v_1, \\dots, \\vec 0, \\dots,\\vec v_n)=0\\). Prova. En efecte, com que \\(\\vec 0=0 \\cdot \\vec 0\\), aleshores \\[\\begin{align*} \\det(\\vec v_1, \\dots, \\vec 0, \\dots,\\vec v_n) &amp; = \\det(\\vec v_1, \\dots, 0\\cdot\\vec 0, \\dots,\\vec v_n) = \\\\ &amp; = 0 \\det(\\vec v_1, \\dots, \\vec 0, \\dots,\\vec v_n)=0. \\end{align*}\\] Lema 3.2 Si considerem les transformacions elementals per columnes corresponents a les transformacions T1, T2 i T3 definides a la Subsecció 1.3, el determinant es modifica com: T1: Si \\(B\\) és la matriu resultant de multiplicar una columna per \\(\\lambda\\) a la matriu \\(A\\): \\[\\det(B)=\\lambda \\det(A).\\]. T2: Si \\(B\\) és la matriu resultant de sumar a una columna d’\\(A\\) \\(\\mu\\) vegades una altra columna d’\\(A\\): \\[\\det(B)=\\det(A).\\] T3: Si \\(B\\) és la matriu resultant d’intercanviar dues columnes diferents d’\\(A\\), llavors: \\[\\det(B)=-\\det(A).\\] Prova. D1 ens diu com es transforma \\(\\det\\) per la transformació elemental T1, obtenint el resultat de l’enunciat. Estudiem ara el canvi T2: sumem a una columna \\(\\mu\\) vegades una altra: \\[\\begin{align*} \\det(\\dots, \\vec v_j, \\dots, \\vec v_k+\\mu\\vec v_j,\\dots) &amp; =\\det(\\dots, \\vec v_j, \\dots, \\vec v_k,\\dots) + \\det(\\dots, \\vec v_j, \\dots,\\mu\\vec v_j,\\dots)= \\\\ &amp; = \\det(\\dots, \\vec v_j, \\dots, \\vec v_k,\\dots) + \\mu\\det(\\dots, \\vec v_j, \\dots,\\vec v_j,\\dots)= \\\\ &amp; = \\det(\\dots, \\vec v_j, \\dots, \\vec v_k,\\dots) + 0 \\end{align*}\\] on primer hem aplicat D2 (separar la suma de vectors), llavors D1 (treure el \\(\\mu\\) fora del determinant) i finalment D3 per veure que un dels determinants és zero. Mirem com es modifica el determinant per la transformació T3: vegem que si intercanviem dues columnes \\(j\\neq k\\), llavors \\[\\det(\\vec v_1,\\dots , \\vec v_j, \\dots, \\vec v_k, \\dots ,\\vec v_n)=-\\det(\\vec v_1,\\dots , \\vec v_k, \\dots, \\vec v_j, \\dots ,\\vec v_n) .\\] Per tal de simplificar la notació escrivim \\(\\det(\\dots , \\vec v_j, \\dots, \\vec v_k, \\dots)\\): \\[\\begin{align*} 0 &amp; = \\det(\\dots , \\vec v_j+\\vec v_k, \\dots, \\vec v_j+\\vec v_k, \\dots) = \\\\ &amp; = \\det(\\dots , \\vec v_j+\\vec v_k, \\dots, \\vec v_j, \\dots) + \\det(\\dots , \\vec v_j+\\vec v_k, \\dots, \\vec v_k, \\dots) = \\\\ &amp; = \\det(\\dots , \\vec v_j, \\dots, \\vec v_j, \\dots) + \\det(\\dots , \\vec v_k, \\dots, \\vec v_j, \\dots) + \\\\ &amp; \\quad + \\det(\\dots , \\vec v_j, \\dots, \\vec v_k, \\dots) + \\det(\\dots , \\vec v_k, \\dots, \\vec v_k, \\dots) = \\\\ &amp; = 0 + \\det(\\dots , \\vec v_k, \\dots, \\vec v_j , \\dots) + \\det(\\dots , \\vec v_j, \\dots, \\vec v_k , \\dots) + 0 \\end{align*}\\] Teorema 3.1 Si tenim dues aplicacions \\(\\det\\colon (\\K^n)^n \\to \\K\\) i \\(\\det&#39;\\colon (\\K^n)^n \\to \\K\\) complint D1, D2, D3 i D4, llavors \\(\\det=\\det&#39;\\). Prova. Volem veure que si \\(\\vec v_1, \\dots , \\vec v_n \\in \\K^n\\), llavors \\(\\det(\\vec v_1, \\dots , \\vec v_n)=\\det&#39;(\\vec v_1, \\dots , \\vec v_n)\\): si considerem \\(A\\) la matriu formada pels vectors \\(\\vec v_1, \\dots, \\vec v_n\\) per columna, podem aplicar transformacions elementals per columnes fins a tenir, o bé una matriu identitat, o bé una matriu amb l’última columna tot zeros. Pel Lema 3.2, veiem com es modifica qualsevol aplicació (tant \\(\\det\\), com \\(\\det&#39;\\)) que compleixi els axiomes D1, D2, D3 i D4, obtenint que \\(\\det(A)=\\lambda\\det(\\rcef(A))\\) (reduced column echelon form) i \\(\\det&#39;(A)=\\lambda\\det&#39;(\\rcef(A))\\) (amb el mateix \\(\\lambda\\)), però o bé \\(\\rcef(A)=\\1_n\\) (i tenim \\(\\det(\\rcef(A))=1=\\det&#39;(\\rcef(A))\\) per D4), o bé \\(\\rcef(A)\\) té una columna tot zeros i per la Observació 3.1, \\(\\det(A)=0=\\det&#39;(A)\\). Per tant, com que \\(\\det\\) i \\(\\det&#39;\\) estan determinats pels canvis elementals per columnes i pel seu valor a la identitat o a una matriu amb una columna tot zeros, han de valer el mateix. Exemple 3.1 Calculem el determinant d’\\(A\\), on:\\[A=\\begin{pmatrix} 1 &amp; 2 &amp; 6 \\\\ 0 &amp; -1 &amp; -8 \\\\ 5 &amp; 6 &amp; 0 \\end{pmatrix}\\] fent transformacions elementals: \\[\\begin{align*} \\begin{vmatrix} 1 &amp; 2 &amp; 6 \\\\ 0 &amp; -1 &amp; -8 \\\\ 5 &amp; 6 &amp; 0 \\end{vmatrix} &amp; = \\begin{vmatrix} 1 &amp; 2 &amp; 6 \\\\ 0 &amp; -1 &amp; -8 \\\\ 0 &amp; -4 &amp; -30 \\end{vmatrix}= -\\begin{vmatrix} 1 &amp; 2 &amp; 6 \\\\ 0 &amp; 1 &amp; 8 \\\\ 0 &amp; -4 &amp; -30 \\end{vmatrix}= \\\\ &amp; = -\\begin{vmatrix} 1 &amp; 0 &amp; 6 \\\\ 0 &amp; 1 &amp; 8 \\\\ 0 &amp; 0 &amp; 2 \\end{vmatrix}= -2\\begin{vmatrix} 1 &amp; 0 &amp; 6 \\\\ 0 &amp; 1 &amp; 8 \\\\ 0 &amp; 0 &amp; 1 \\end{vmatrix}= -2 \\det(\\1_3)=-2\\,. \\end{align*}\\] Amb tot això, el que no hem demostrat és que existeixi una aplicació \\(\\det\\) que tingui les propietats D1, D2, D3 i D4. Per demostrar la existència, el que farem és construir-la explícitament. Considerem \\(A\\in M_n(\\K)\\) i el producte de \\(n\\) coeficients de la matriu \\(a_{i_1j_1} \\cdots a_{i_nj_n}\\) tals que no hi hagi dos coeficients d’una mateixa columna, ni d’una mateixa fila. Dit d’una altra manera, com que hi ha \\(n\\) files i \\(n\\) columnes,considerem \\(n\\) parelles \\(\\{(i_1,j_1),\\dots,(i_n,j_n)\\}\\) que compleixin la igualtat de conjunts: \\[\\begin{align*} \\tag{3.3} \\{i_1, \\dots, i_n\\}=\\{1, \\dots, n\\} = \\{j_1, \\dots, j_n\\} \\,. \\end{align*}\\] Definició 3.1 Anomenem un patró d’\\(n\\) elements a un conjunt \\(\\calp=\\{(i_1,j_1),\\dots,(i_n,j_n)\\}\\subset \\{1,\\dots,n\\}^2\\) que compleixi que \\(i_k\\neq i_l\\) i \\(j_k\\neq j_l\\) si \\(k\\neq l\\), o, equivalentment, que compleixi l’Equació (3.3). Donat un patró d’\\(n\\) elements \\(\\calp\\) i una matriu \\(A\\in M_n(\\K)\\), definim l’element \\(a_\\calp\\) com el producte: \\[a_\\calp=\\prod_{(i,j)\\in\\calp} a_{ij} .\\] Exemple 3.2 En el cas de les matrius \\(3\\times 3\\), tenim \\(6\\) patrons possibles i ens donen els productes: \\[\\begin{array}{ccccc} \\begin{pmatrix} \\boxed{a_{11}} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; \\boxed{a_{22}} &amp; a_{23}\\\\ a_{31} &amp; a_{32} &amp; \\boxed{a_{33}} \\end{pmatrix} &amp; \\Longleftrightarrow &amp; \\calp=\\{(1,1),(2,2),(3,3)\\} &amp; \\Longleftrightarrow &amp; a_\\calp=a_{11}a_{22}a_{33} \\\\ \\begin{pmatrix} \\boxed{a_{11}} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; \\boxed{a_{23}}\\\\ a_{31} &amp; \\boxed{a_{32}} &amp; a_{33} \\end{pmatrix} &amp; \\Longleftrightarrow &amp; \\calp=\\{(1,1),(2,3),(3,2)\\} &amp; \\Longleftrightarrow &amp; a_\\calp=a_{11}a_{23}a_{32} \\\\ \\begin{pmatrix} a_{11} &amp; \\boxed{a_{12}} &amp; a_{13}\\\\ \\boxed{a_{21}} &amp; a_{22} &amp; a_{23}\\\\ a_{31} &amp; a_{32} &amp; \\boxed{a_{33}} \\end{pmatrix} &amp; \\Longleftrightarrow &amp; \\calp=\\{(1,2),(2,1),(3,3)\\} &amp; \\Longleftrightarrow &amp; a_\\calp=a_{12}a_{21}a_{33} \\\\ \\begin{pmatrix} a_{11} &amp; \\boxed{a_{12}} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; \\boxed{a_{23}}\\\\ \\boxed{a_{31}} &amp; a_{32} &amp; a_{33} \\end{pmatrix} &amp; \\Longleftrightarrow &amp; \\calp=\\{(1,2),(2,3),(3,1)\\} &amp; \\Longleftrightarrow &amp; a_\\calp=a_{12}a_{23}a_{31} \\\\ \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\boxed{a_{13}}\\\\ \\boxed{a_{21}} &amp; a_{22} &amp; a_{23}\\\\ a_{31} &amp; \\boxed{a_{32}} &amp; a_{33} \\end{pmatrix} &amp; \\Longleftrightarrow &amp; \\calp=\\{(1,3),(2,1),(3,2)\\} &amp; \\Longleftrightarrow &amp; a_\\calp=a_{13}a_{21}a_{32} \\\\ \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\boxed{a_{13}}\\\\ a_{21} &amp; \\boxed{a_{22}} &amp; a_{23}\\\\ \\boxed{a_{31}} &amp; a_{32} &amp; a_{33} \\end{pmatrix} &amp; \\Longleftrightarrow &amp; \\calp=\\{(1,3),(2,2),(3,1)\\} &amp; \\Longleftrightarrow &amp; a_\\calp=a_{13}a_{22}a_{31} \\end{array}\\] I podeu veure que coincideixen amb els sumands de l’Equació (3.2). Ara tant sols cal decidir si cada element suma o resta, i per això necessitem el signe d’un patró: Definició 3.2 Fixat un enter positiu \\(n\\) i un patró \\(\\calp=\\{(i_1,j_1),\\dots,(i_n,j_n)\\}\\subset \\{1,\\dots,n\\}^2\\), definim \\(\\sign(\\calp)\\), el signe de \\(\\calp\\), com \\((-1)^\\epsilon\\), on \\(\\epsilon\\) és el nombre de parelles \\([(i,j),(i&#39;,j&#39;)]\\in\\calp\\) tals que \\(i&lt;i&#39;\\) i \\(j&gt;j&#39;\\). Exemple 3.3 En el cas \\(3\\times 3\\), els patrons tenen el signe següent: \\[\\begin{array}{|c|c|c|c|} \\hline \\text{Patró} &amp; \\text{Parelles} &amp; \\epsilon &amp; \\text{Signe} \\\\ \\hline \\{(1,1),(2,2),(3,3)\\} &amp; &amp; 0 &amp; (-1)^0=1 \\\\ \\{(1,1),(2,3),(3,2)\\} &amp; \\{[(2,3),(3,2)]\\} &amp; 1 &amp; (-1)^1=-1 \\\\ \\{(1,2),(2,1),(3,3)\\} &amp; \\{[(1,2),(2,1)]\\} &amp; 1 &amp; (-1)^1=-1 \\\\ \\{(1,2),(2,3),(3,1)\\} &amp; \\{[(1,2),(3,1)],[(2,3),(3,1)]\\} &amp; 2 &amp; (-1)^2=1 \\\\ \\{(1,3),(2,1),(3,2)\\} &amp; \\{[(1,3),(2,1)],[(1,3),(3,2)\\} &amp; 2 &amp; (-1)^2=1 \\\\ \\{(1,3),(2,2),(3,1)\\} &amp; \\{[(1,3),(2,2)],[(1,3),(3,1)],[(2,2),(3,1)]\\} &amp; 3 &amp; (-1)^3=-1 \\\\ \\hline \\end{array}\\] Els patrons tenen les propietats següents: Lema 3.3 Considerem \\(A\\in M_n(\\K)\\), amb \\(n\\) fixada. Cada patró \\(\\calp\\) es correspon amb una aplicació bijectiva \\(\\sigma\\colon \\{1,2,\\dots,n\\} \\to \\{1,2,\\dots,n\\}\\) (s’anomena permutació). Hi ha \\(1\\cdot 2 \\dots n = n!\\) (factorial d’\\(n\\)) patrons diferents a \\(A\\). Definim el signe d’una permutació \\(\\sigma\\colon \\{1,\\dots,n\\}\\to\\{1,\\dots,n\\}\\) com \\[\\sign(\\sigma)=\\prod_{i&lt;j} \\frac{\\sigma(j)-\\sigma(i)}{j-i} .\\] Si \\(\\sigma\\) és la permutació corresponent a \\(\\calp\\), llavors \\(\\sign(\\sigma)=\\sign(\\calp)\\). Si \\(\\Id\\colon \\{1,\\dots,n\\}\\to\\{1,\\dots,n\\}\\) és l’aplicació \\(\\Id(i)=i\\) (identitat), llavors \\(\\sign(\\Id)=1\\). Si \\(\\sigma\\) i \\(\\tau\\) són dues permutacions de \\(\\{1,\\dots, n\\}\\), llavors \\[\\sign(\\sigma\\circ\\tau)=\\sign(\\sigma)\\sign(\\tau)\\]. \\(\\sign(\\sigma)=\\sign(\\sigma^{-1})\\). Si \\(\\tau_{k\\ell}\\) correspon a la permutació: \\[\\tau_{k\\ell}(m)=\\left\\{\\begin{array}{ll} m &amp; \\text{si $m\\not\\in\\{k,\\ell\\}$}\\\\ \\ell &amp; \\text{si $m=k$} \\\\ k &amp; \\text{si $m=\\ell$} \\end{array} \\right.\\] amb \\(k\\neq \\ell\\), llavors \\(\\sign(\\tau_{k\\ell})=-1\\). Prova. Els patró \\(\\calp\\) té \\(n\\) parelles \\((i,j)\\) on, si mirem tant sols la primera coordenada, hi ha exactament un \\(1\\), un \\(2\\), …i un \\(n\\). Definim \\(\\sigma(i)=j\\) si \\((i,j)\\in\\calp\\). Aquesta aplicació és injectiva i exhaustiva perquè a la segona coordenada de les parelles de \\(\\calp\\) també hi ha un únic \\(1\\), un únic \\(2\\), …i un únic \\(n\\). Si tenim una aplicació bijectiva \\(\\sigma\\colon \\{1,2,\\dots,n\\} \\to \\{1,2,\\dots,n\\}\\), definim els patró corresponent com \\(\\calp=\\{(1,\\sigma(1)), \\ldots, (n,\\sigma(n))\\}\\). Per demostrar (b) comptem quantes aplicacions bijectives \\(\\sigma\\) hi ha de \\(\\{1,2,\\dots, n\\}\\) en ell mateix: \\(\\sigma(1)\\) pot ser qualsevol element de \\(\\{1,2,\\dots, n\\}\\), per tant en podem escollir \\(n\\); \\(\\sigma(2)\\) pot ser qualsevol element de \\(\\{1,2,\\dots, n\\}\\) excepte \\(\\sigma(1)\\), per tant en podem escollir \\(n-1\\); iterant aquest procediment, tindrem \\(n(n-1)(n-2) \\cdots 2\\cdot 1\\) aplicacions bijectives, i aquesta és la definició de factorial d’\\(n\\). A l’expressió de (c) veiem que tant al denominador hi ha el producte \\((2-1)(3-1)\\cdots (n-1)(3-2) \\cdots\\), mentre que al numerador hi ha els mateixos factors, on es canvien de signe els que compleixen \\(\\sigma(i)&gt;\\sigma(j)\\) amb \\(i&lt;j\\), pel que el quocient serà \\(\\pm 1\\), i el signe ve determinat pel nombre de vegades que passa \\(\\sigma(i)&gt;\\sigma(j)\\) amb \\(i&lt;j\\), que és la definició de signe d’un patró. Calculem el signe de la identitat per demostrar (d): \\[\\sign(\\Id)=\\prod_{i&lt;j} \\frac{\\Id(j)-\\Id(i)}{j-i}=\\prod_{i&lt;j} \\frac{j-i}{j-i}=1 .\\] La demostració d’(e) ve de considerar: \\[\\begin{align*} \\sign(\\sigma\\circ\\tau) &amp; =\\prod_{i&lt;j} \\frac{\\sigma(\\tau(j))-\\sigma(\\tau(i))}{j-i} = \\prod_{i&lt;j} \\frac{\\sigma(\\tau(j))-\\sigma(\\tau(i))}{\\tau(j)-\\tau(i)}\\frac{\\tau(j)-\\tau(i)}{j-i}=\\\\ &amp; =\\prod_{i&lt;j} \\frac{\\sigma(\\tau(j))-\\sigma(\\tau(i))}{\\tau(j)-\\tau(i)}\\prod_{i&lt;j}\\frac{\\tau(j)-\\tau(i)}{j-i}= \\sign(\\sigma)\\sign(\\tau), \\end{align*}\\] on observem que: \\[\\prod_{i&lt;j}\\frac{\\sigma(j)-\\sigma(i)}{j-i}=\\prod_{i&lt;j} \\frac{\\sigma(\\tau(j))-\\sigma(\\tau(i))}{\\tau(j)-\\tau(i)}\\] ja que hi ha els mateixos factors al numerador i denominador, i si un factor del numerador ha canviat de signe, el corresponent factor del denominador també. La demostració d’(f) es dedueix de que \\(1=\\sign(\\Id)=\\sign(\\sigma\\circ \\sigma^{-1})=\\sign(\\sigma)\\sign(\\sigma^{-1})\\), i que el signe de qualsevol permutació és \\(\\pm1\\). Finalment, la demostració de (g) és molt semblant a la de (d): primer considerem \\(k&lt;\\ell\\) (com que \\(\\tau_{k\\ell}=\\tau_{\\ell k}\\), si cal, les intercanviem). Llavors el signe de \\(\\tau_{k\\ell}\\) serà com el de \\(\\Id\\), però hi haurà un factor diferent a quan \\((i,j)=(k,\\ell)\\), que sortirà \\(\\frac{k-\\ell}{\\ell-k}=-1\\), per tant \\(\\sign(\\tau_{k\\ell})=-1\\). Ara ja podem definir el determinant d’una matriu: Definició 3.3 Sigui \\(A\\in M_n(\\K)\\) una matriu quadrada. Considerem \\(P_n\\) el conjunt de tots els patrons d’\\(n\\) elements (\\(P_n\\) té \\(n!\\) elements) i per cada \\(\\calp\\in P_n\\), considerem \\(a_\\calp\\) com a la Definició 3.1. Definim el determinant d’\\(A\\) com: \\[\\begin{align*} (\\#eq:def-det) \\det(A)=\\sum_{\\calp \\in P_n} \\sign(\\calp) a_\\calp \\,. \\end{align*}\\] Teorema 3.2 El determinant de la Definició 3.3 compleix les propietats D1, D2, D3 i D4. Prova. Fixem \\(A\\in M_n(\\K)\\) i \\(a_{ij}\\) els seus coeficients. D1: si fixem una columna \\(j\\), cada sumand de l’Equació (??) té exactament un coeficient \\(a_{ij}\\). Si el substituïm per \\(\\lambda a_{ij}\\), tots els sumands de l’Equació (??) queden multiplicats per \\(\\lambda\\), pel que es compleix D1. D2: si la columna \\(j\\) es pot escriure com la suma de dues columnes: \\(a_{ij}=a&#39;_{ij}+a&#39;&#39;_{ij}\\), com que a l’Equació (??) cada sumand té exactament un d’aquests coeficients, podem separar la suma, obtenint D2. D3: Si tenim la columna \\(j\\) i la columna \\(k\\) d’\\(A\\) que són iguals (amb \\(j\\neq k\\)), tenim que cada sumand de l’Equació (??) apareix dues vegades, una pel patró \\(\\calp_1\\) i \\(\\calp_2\\). Hem de veure que apareix amb signe diferent, i llavors la suma serà zero. Si \\(\\sigma_1\\) és la permutació que correspon al patró \\(\\calp_1\\) i \\(\\sigma_2\\) la que correspon al patró \\(\\calp_2\\), llavors \\(\\sigma_1=\\tau_{jk}\\circ\\sigma_2\\) i, pel Lema 3.3, tenen signe diferent. D4: l’únic patró \\(\\calp\\) tal que \\((\\1_n)_\\calp\\neq 0\\) és el patró \\(\\{(1,1),(2,2),\\dots, (n,n)\\}\\), i és un producte d’uns amb signe positiu. Vegem ara més propietats dels determinants: Proposició 3.1 Si \\(A, B\\in M_{n\\times n}(\\K)\\), llavors: \\(A\\) és invertible si i només si \\(\\det(A)\\neq 0\\). \\(\\det(A)\\neq 0\\) si i només si les files (i les columnes) d’\\(A\\) són linealment independents. \\(\\det(A)=\\det(A^T)\\), on \\(A^T\\) és la transposada d’\\(A\\). \\(\\det(AB)=\\det(A)\\det(B)\\). Prova. Per demostrar (a), recordem que una matriu és invertible si i només si podem aconseguir la identitat mitjançant transformacions elementals. En aquest cas, cada transformació modifica el determinant canviant-li el signe o multiplicant per un \\(\\lambda\\neq 0\\). Llavors: Si \\(A\\) és invertible, \\(\\det(A)=\\lambda \\det(\\1_n)=\\lambda \\neq 0\\). Si \\(A\\) no és invertible, \\(\\det(A)=\\lambda \\det(B)\\), on \\(B\\) és una matriu amb l’última fila tot zeros, per tant \\(\\det(B)=0\\) (cada sumand del determinant a l’Equació (??) té un coeficient de l’última fila), d’on es dedueix que \\(\\det(A)=0\\). Per demostrar (b), el raonament de files (o columnes) linealment independents és el mateix, tenint en compte que les files (o columnes) són linealment independents si i només si la matriu \\(A\\) és equivalent a la identitat. Per demostrar (c), observem que si \\(B=A^T\\), per l’Equació (??), \\(\\det(A)\\) i \\(\\det(B)\\) tenen els mateixos sumands, pel que cal veure que els signes dels patrons \\(\\calp\\) i \\(\\calp&#39;\\) corresponents a coeficients iguals \\(a_\\calp\\) i \\(b_{\\calp&#39;}\\), són els mateixos: observem que si \\(a_\\calp\\) és el coeficient corresponent a \\(\\calp\\), que és un patró que correspon a una permutació \\(\\sigma\\) (veure Lema 3.3), llavors, si \\(\\calp&#39;\\) és el patró corresponent a \\(\\sigma^{-1}\\), tenim que \\(a_\\calp=b_{\\calp&#39;}\\), i amb el mateix signe, ja que pel Lema 3.3, \\(\\sign(\\calp)=\\sign(\\sigma)=\\sign(\\sigma^{-1})=\\sign(\\calp&#39;)\\). Falta demostrar la fórmula del producte de determinants \\(\\det(AB)=\\det(A)\\det(B)\\): comencem amb la matriu \\(A\\) i li fem les transformacions elementals necessàries per obtenir \\(\\rref(A)\\). Aquestes transformacions elementals aniran modificant el determinant, i obtindrem \\(\\det(A)=\\lambda\\det(\\rref(A))\\), on: Aquestes transformacions elementals es corresponen amb multiplicar per una matriu invertible \\(P\\): \\(\\rref(A)=PA\\). \\(\\lambda\\) només depèn de les transformacions elementals que hem fet, per tant, si agafem \\(C\\in M_n(\\K)\\) i hi fem les mateixes transformacions que hem fet a \\(A\\) i obtenim \\(C&#39;=PC\\), tindrem \\(\\det(C)=\\lambda \\det(C&#39;)\\). Si \\(A\\) és invertible, llavors \\(\\rref(A)= PA=\\1_n\\) i \\(\\det(A)=\\lambda\\det(\\1_n)=\\lambda\\). Considerem primer el cas en que \\(A\\) no és invertible: llavors \\(\\det(A)=0\\) i les \\(n\\) columnes d’\\(A\\) no són linealment independents. Com que les columnes d’\\(AB\\) són combinació lineal de les de \\(A\\) i també en té \\(n\\), tenim que les columnes de \\(AB\\) tampoc són linealment independents i per tant \\(\\det(AB)=0\\) i en particular \\(\\det(AB)=\\det(A)\\det(B)\\). Suposem ara que \\(A\\) és invertible, per tant \\(\\rref(A)=PA=\\1_n\\). Considerem ara \\(C=AB\\) i fem les mateixes transformacions elementals que hem fet a \\(A\\) per obtenir \\(\\rref(A)\\). Amb els raonaments que hem vist, tenim: \\[\\det(AB)=\\det(C)=\\lambda \\det(C&#39;)=\\lambda \\det(PAB)=\\lambda \\det(\\1_n B)=\\det(A)\\det(B).\\] Ara veurem una manera inductiva de calcular el determinant. També es podia haver definit com veurem ara, i s’hauria de demostrar que compleix les propietats D1, D2, D3 i D4. Per això, necessitem una notació que utilitzarem a aquesta secció (es pot confondre amb una de les notacions utilitzades a les seccions anteriors, fixeu-vos amb la \\(c\\) del superíndex). Si \\(A\\in M_{n}(\\K)\\), notem per \\(A^c_{ij}\\in M_{(n-1)}(\\K)\\) la matriu que resulta d’eliminar la fila \\(i\\) i la columna \\(j\\) d’\\(A\\). Proposició 3.2 Si \\(A\\in M_{n\\times n}(\\K)\\), amb \\(n\\geq2\\), podem desenvolupar el determinant per qualsevol fila o columna segons les fórmules següents: \\[\\begin{array}{ll} \\det(A)=\\sum_{j=1}^n (-1)^{i+j} a_{ij} \\det(A^c_{ij}) &amp; \\text{(si desenvolupem per la fila $i$)},\\\\[3mm] \\det(A)=\\sum_{i=1}^n (-1)^{i+j} a_{ij} \\det(A^c_{ij}) &amp; \\text{(si desenvolupem per la columna $j$)}. \\end{array}\\] Prova. Per a demostrar això, mirem com són tots els sumands i els comparem amb els de l’Equació (??): l’hem calculat de manera recursiva, i cada cop que fem una iteració anem esborrant la fila i columna corresponent a aquell coeficient. Per tant, hi haurà un coeficient de la primera fila, un altre de la segona, …, de tal manera que cada cop agafem una columna diferent, i per tant tindrem el sumand: \\[\\begin{align*} (\\#eq:sumanddet) (-1)^\\epsilon a_{1j_1} a_{2j_2} \\cdots a_{nj_n} \\end{align*}\\] amb \\(j_k\\neq j_l\\) si \\(k\\neq l\\), i el signe ve determinat per la paritat de \\(\\epsilon\\), que es pot veure que és la mateixa que la de la permutació, obtenint el mateix que a l’Equació (??) Aquesta expressió no depèn de per quina fila o columna desenvolupem, pel que el resultat final serà el mateix. Exemple 3.4 Calculem el determinant d’\\(A\\), on:\\[A=\\begin{pmatrix} 1 &amp; 2 &amp; 6 \\\\ 0 &amp; -1 &amp; -8 \\\\ 5 &amp; 6 &amp; 0 \\end{pmatrix}\\] desenvolupant per la primera fila: \\[\\begin{align*} \\begin{vmatrix} 1 &amp; 2 &amp; 6 \\\\ 0 &amp; -1 &amp; -8 \\\\ 5 &amp; 6 &amp; 0 \\end{vmatrix} &amp; = 1 \\begin{vmatrix} -1 &amp; -8 \\\\ 6 &amp; 0 \\end{vmatrix} -2 \\begin{vmatrix} 0 &amp; -8 \\\\ 5 &amp; 0 \\end{vmatrix} + 6 \\begin{vmatrix} 0 &amp; -1 \\\\ 5 &amp; 6 \\end{vmatrix} = \\\\ &amp; = (0-(-48))-2(40)+6(5)=48-80+30=-2. \\end{align*}\\] Exercici 3.2 Si \\(\\calp\\) és un patró amb \\(n\\) elements i \\(A_\\calp\\in M_n(\\K)\\) és una matriu que té tots els coeficients zero, excepte els elements \\(a_{ij}=1\\), per a \\((i,j)\\in\\calp\\), llavors \\(\\det(A_\\calp)=\\sign(\\calp)\\). Exercici 3.3 Si \\(A\\in M_n(\\K)\\) és una matriu triangular superior (o inferior), llavors \\(\\det(A)\\) és el producte d’elements de la diagonal. 3.3 Polinomi característic. Valors i vectors propis A la motivació d’aquest capítol (§1) hem vist una aplicació lineal \\(f\\colon \\R^3\\to \\R^3\\) que escrita en una base \\(\\calb\\) o en una altra \\(\\calc\\) tenia les expressions següents: \\[[f]_\\calb=\\begin{pmatrix} -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} \\text{ i } [f]_\\calc=\\begin{pmatrix} 5 &amp; -15 &amp; -21 \\\\ -3 &amp; 9 &amp; 13 \\\\ 3 &amp; -9 &amp; -13 \\end{pmatrix}\\] Veiem que la primera és diagonal, mentre que la segona no. Això vol dir que l’aplicació lineal envia les rectes generades per cada vector de la base \\(\\calb\\) a elles mateixes, mentre que les rectes generades pels vectors de la base \\(\\calc\\) es converteixen en altres rectes. Si fixem uns eixos de coordenades amb la base \\(\\calb\\), podrem veure com \\(f\\) modifica els vectors d’\\(\\R^3\\) molt més fàcilment que si fixem els eixos a la base \\(\\calc\\). Per tant, estem dient que per la mateixa aplicació lineal \\(f\\colon E \\to E\\), aprofitant que els coeficients de \\([f]_\\calb\\) depenen de la base \\(\\calb\\), buscarem una base \\(\\calb\\) tal que la matriu \\([f]_\\calb\\) sigui el més “senzilla” possible. Definició 3.4 Sigui \\(A \\in M_n(\\K)\\) i \\(f_A\\colon \\K^n\\to\\K^n\\) l’aplicació lineal induïda. Diem que \\(A\\) és diagonalitzable si es compleix una de les condicions equivalents següents: \\(A\\) és similar a una matriu diagonal. Existeix una base \\(\\calb\\) de \\(\\K^n\\) tal que \\([f_A]_\\calb\\) és diagonal. Existeix una matriu invertible \\(S\\in M_n(\\K)\\) tal que el producte \\(S^{-1}AS\\) és diagonal. Exemple 3.5 La matriu \\(B=\\left(\\begin{smallmatrix} 5 &amp; -15 &amp; -21 \\\\ -3 &amp; 9 &amp; 13 \\\\ 3 &amp; -9 &amp; -13 \\end{smallmatrix}\\right)\\) és diagonalitzable (veure apartat 1). Observació. Si una matriu \\(A\\) és diagonalitzable i \\(\\calb=[\\vec v_1, \\dots, \\vec v_n]\\) és una base en que \\([f_A]_\\calb\\) és diagonal amb coeficients a la diagonal \\(\\lambda_1, \\dots, \\lambda_n\\), llavors es compleix que \\(A \\vec v_j=\\lambda_j \\vec v_j\\). Per tant, els vectors \\(\\vec v_j\\) de la base \\(\\calb\\) compleixen que \\(A\\vec v_j\\) és un múltiple de \\(\\vec v_j\\). Utilitzem aquest fet per veure que no totes les matrius són diagonalitzables: Exemple 3.6 La matriu \\(A=\\left(\\begin{smallmatrix} 0 &amp; -1 \\\\ 1 &amp; 0\\end{smallmatrix}\\right)\\in M_2(\\R)\\), que correspon a una rotació d’angle \\(\\pi/2\\), no és diagonalitzable: suposem que sí, i que \\(\\vec v\\) és un vector d’una base \\(\\calb\\) tal que \\([f_A]_\\calb\\) és diagonal, llavors es complirà que \\(A\\vec v=\\lambda \\vec v\\) per a cert \\(\\lambda\\in \\R\\), però \\(\\vec v\\) i \\(A\\vec v\\) són perpendiculars per a tot \\(\\vec v \\in \\R^2\\), pel que no pot ser. Una altra manera de veure-ho és que si \\(\\vec v=\\left(\\begin{smallmatrix} x \\\\ y \\end{smallmatrix}\\right)\\) s’hauria de complir: \\[\\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} \\lambda x \\\\ \\lambda y \\end{pmatrix}\\] I queda el sistema homogeni: \\[\\begin{align*} -\\lambda x - y =0 \\\\ x - \\lambda y=0 \\end{align*}\\] Com que estem treballant a \\(\\R\\), aquest sistema té rang \\(2\\) per qualsevol \\(\\lambda\\in\\R\\), i per tant l’única solució és \\(\\left(\\begin{smallmatrix} x \\\\ y \\end{smallmatrix}\\right)=\\left(\\begin{smallmatrix} 0 \\\\ 0 \\end{smallmatrix}\\right)\\), que no pot formar part de cap base. Veiem, doncs, que per saber si una matriu \\(A\\in M_n(\\K)\\) és diagonalitzable, hem de veure si existeix una base \\(\\calb=[\\vec v_1, \\dots, \\vec v_n]\\) i escalars \\(\\lambda_j\\in\\K\\) tals que \\(A\\vec v_j=\\lambda_j \\vec v_j\\). Posem nom a aquests escalars i vectors: Definició 3.5 Donada una matriu \\(A\\in M_n(\\K)\\), diem que un vector no nul \\(\\vec v\\in \\K^n\\) és un vector propi de valor propi \\(\\lambda \\in \\K\\) si \\(A\\vec v=\\lambda \\vec v\\). Els elements del conjunt format pels \\(\\lambda\\in \\K\\) tals que existeix un vector \\(\\vec v\\in\\K^n\\) no nul tal que \\(A\\vec v=\\lambda \\vec v\\) s’anomenen valors propis d’\\(A\\). Exemple 3.7 Analitzem què és un vector propi de valor propi \\(0\\) d’una matriu \\(A\\in M_n(\\K)\\): serà \\(\\vec v\\neq \\vec 0\\) tal que \\(A\\vec v=\\vec 0\\), per tant, serà un vector de \\(\\Ker(f_A)\\) (les solucions del sistema homogeni amb matriu associada \\(A\\)). Deduïm que \\(0\\) és un valor propi d’\\(A\\) si i només si \\(\\Ker(f_A)\\neq \\{\\vec 0\\}\\), si i només si \\(\\det(A)=0\\). Exemple 3.8 Quins són els valors propis i els vectors propis de \\(\\1_n\\)? Per a tot \\(\\vec v\\in\\K^n\\) es compleix que \\(\\1_n\\vec v=\\vec v\\), per tant, tot vector no nul és vector propi de valor propi \\(1\\). Exemple 3.9 Considerem la matriu de la reflexió a \\(\\R^2\\) respecte la recta \\(r\\) que passa per l’origen amb vector director \\(\\left(\\begin{smallmatrix}1\\\\1\\end{smallmatrix}\\right)\\). Segons la Proposició 2.4, correspon a la matriu: \\[\\refl{}=\\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix}\\] Per definició de la reflexió, els vectors \\(\\vec v\\) de la recta \\(r\\) compliran \\(A\\vec v=\\vec v\\), per tant són valors propis de vector propi \\(1\\): \\(A\\left(\\begin{smallmatrix}1\\\\1\\end{smallmatrix}\\right)=\\left(\\begin{smallmatrix}1\\\\1\\end{smallmatrix}\\right)\\). En canvi, els vectors \\(\\vec w\\) perpendiculars a \\(r\\) (els generats per \\(\\left(\\begin{smallmatrix}1\\\\-1\\end{smallmatrix}\\right)\\)), compleixen que \\(A\\vec w=-\\vec w\\), per tant, són vectors propis de valor propi \\(-1\\). Això vol dir que, per a \\(\\calb=[\\left(\\begin{smallmatrix}1\\\\1\\end{smallmatrix}\\right),\\left(\\begin{smallmatrix}1\\\\-1\\end{smallmatrix}\\right)]\\), \\[[\\refl]_\\calb=\\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix}.\\] Mirant l’Exemple 3.8, podem deduir quan un valor \\(\\lambda\\in\\K\\) és un valor propi d’una matriu: Teorema 3.3 Un escalar \\(\\lambda\\in \\K\\) és un valor propi d’una matriu \\(A\\in M_n(\\K)\\) si, i només si, \\(\\det(A-\\lambda\\1_n)=0\\). Prova. L’escalar \\(\\lambda\\) és un valor propi d’\\(A\\) si i només si existeix \\(\\vec v\\neq \\vec 0\\) tal que \\(A\\vec v=\\lambda \\vec v\\), i aquesta igualtat és equivalent a que \\((A-\\lambda \\1_n)\\vec v=\\vec 0\\). Per tant, és equivalent a que el sistema homogeni donat per la matriu \\(A-\\lambda\\1_n\\) tingui solució diferent de \\(\\vec 0\\), i això és equivalent a que \\(\\det(A-\\lambda\\1_n)=0\\). Això ens porta a la definició següent: Definició 3.6 Si \\(A\\in M_n(\\K)\\), el polinomi característic d’\\(A\\) és \\(p_A(x)=\\det(A-x \\1_n)\\), un polinomi de grau \\(n\\). El que hem vist al Teorema 3.3 és que \\(\\lambda\\) és un valor propi d’\\(A\\) si i només si \\(p_A(\\lambda)=0\\), on \\(p_A(x)\\) és el polinomi característic d’\\(A\\) en la variable \\(x\\). Exemple 3.10 Calculem els valors propis de la matriu \\(B=\\left(\\begin{smallmatrix} 5 &amp; -15 &amp; -21 \\\\ -3 &amp; 9 &amp; 13 \\\\ 3 &amp; -9 &amp; -13 \\end{smallmatrix}\\right)\\): hem de fer el determinant: \\[0= \\begin{vmatrix} 5-x &amp; -15 &amp; -21 \\\\ -3 &amp; 9-x &amp; 13 \\\\ 3 &amp; -9 &amp; -13-x \\end{vmatrix} = -x^3 +x^2+2x=-x(x+1)(x-2)\\] i per tant els valors propis són \\(\\{0,-1,2\\}\\) (tal i com hem vist a l’apartat 1). Observació. El Teorema 3.3 redueix el problema de trobar els valors propis d’una matriu a un de trobar les arrels d’un polinomi de grau \\(n\\). Per a \\(n\\leq 4\\) existeixen fórmules algebraiques explícites per trobar expressions d’aquestes arrels, mentre que per a \\(n\\geq 5\\) es pot demostrar que no n’hi ha (de fòrmula algebraica explícita general). Exercici 3.4 Calculeu el polinomi característic d’una matriu \\(2\\times 2\\) general \\(\\left(\\begin{smallmatrix}a&amp;b\\\\c&amp;d\\end{smallmatrix} \\right)\\). Hi ha casos, però, en que és fàcil calcular les arrels del polinomi característic: Lema 3.4 Si \\(A\\in M_n(\\K)\\) és una matriu triangular superior (o inferior), els valors propis d’\\(A\\) són els valors de la diagonal d’\\(A\\) (els elements \\(a_{ii}\\)). Prova. Podem calcular l’expressió \\(\\det(A-x\\1_n)\\) desenvolupant per l’última fila i anar tirant enrera, i tindrem: \\[\\det(A-x \\1_n)=(a_{11}-x)(a_{22}-x)\\cdots (a_{nn}-x),\\] i observem que aquesta expressió s’anul·la pels valors \\(x=a_{ii}\\). Exemple 3.11 Estudiem la diagonalització de la matriu \\[A=\\begin{pmatrix} 2 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix}.\\] Pel Lema 3.4, com que \\(A\\) és triangular superior, els únics possibles valors propis són els elements \\(2\\) i \\(1\\). Un vector propi de valor propi \\(2\\) és un vector \\(\\vec v=\\left(\\begin{smallmatrix} x \\\\ y \\end{smallmatrix}\\right)\\) tal que: \\[\\begin{pmatrix} 2 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 2x \\\\ 2y \\end{pmatrix}\\] I per tant queda només l’equació \\(y = 0\\). Per tant la solució és \\(\\vec v= x\\left(\\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}\\right)\\). Un vector propi de valor propi \\(1\\) és un vector \\(\\vec v=\\left(\\begin{smallmatrix} x \\\\ y \\end{smallmatrix}\\right)\\) tal que: \\[\\begin{pmatrix} 2 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}\\] i queda l’equació \\(x+y=0\\), per tant la solució és \\(\\vec v=x\\left(\\begin{smallmatrix} 1 \\\\ -1 \\end{smallmatrix}\\right)\\). Per tant, a la base \\(\\calb=[\\left(\\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}\\right),\\left(\\begin{smallmatrix} 1 \\\\ -1 \\end{smallmatrix}\\right)]\\), tenim: \\[[f_A]_\\calb=\\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}.\\] Exemple 3.12 Estudiem la diagonalització de la matriu \\[A=\\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix}.\\] Pel Lema 3.4, com que \\(A\\) és triangular superior, l’únic valor propi possible és l’\\(1\\). Si fos diagonalitzable, \\(A\\) hauria de ser similar a \\(\\1_2\\), la matriu identitat \\(2\\times 2\\), per tant, hauria d’existir \\(S\\) una matriu invertible tal que \\(A=S\\cdot\\1_2\\cdot S^{-1}\\), però \\(S\\cdot \\1_2 \\cdot S^{-1}=\\1_2\\), pel que \\(A\\) no és diagonalitzable. El fet de que els valors propis d’\\(A\\) siguin les arrels del polinomi característic d’\\(A\\) dóna una limitació del nombre de valors propis diferents que pot tenir \\(A\\): Lema 3.5 Si \\(A\\in M_n(\\K)\\), el nombre de valors propis d’\\(A\\) és com a molt \\(n\\). Prova. Els valors propis d’\\(A\\) són els zeros de \\(p_A(x)\\), que és un polinomi de grau \\(n\\). Utilitzem ara que un polinomi de grau \\(n\\) sobre un cos \\(\\K\\) pot tenir com a molt \\(n\\) arrels diferents. També es poden calcular alguns coeficients del polinomi característic: Lema 3.6 Si \\(A\\in M_n(\\K)\\), \\[p_A(x)=(-1)^{n} x^n + (-1)^{n-1}\\Tr(A) x^{n-1} + \\cdots + \\det(A) ,\\] on \\(\\Tr(A)\\) s’anomena la traça d’\\(A\\) i és la suma dels elements de la diagonal; i els termes que no estan escrits corresponen a \\(x\\), \\(x^2\\), …i \\(x^{n-2}\\). Prova. Quan calculem \\(p_A(x)=\\det(A-x\\1_n)\\) a partir de l’Equació (??), l’únic sumand que conté \\(n\\) o \\(n-1\\) factors que contenen una \\(x\\) és el corresponent al patró \\((1,1),(2,2),\\dots, (n,n)\\) (si fem fem el producte d’un patró a \\(A-x\\1_n\\) que té un element de fora de la diagonal, com a mínim en té dos fora de la diagonal, pel que el grau en \\(x\\) és menor o igual a \\(n-2\\)), i per tant, tenim que: \\[p_A(x)=(a_{11}-x)(a_{22}-x)\\cdots(a_{nn}-x)+\\text{polinomi de grau $n-2$ en $x$} ,\\] i d’aquí veiem que el coeficient de \\(x^n\\) és \\((-1)^n\\) i el de \\(x_{n-1}\\) és \\((-1)^{n-1}(a_{11}+a_{22}+\\cdots + a_{nn})\\). Per veure que el terme independent de \\(p_A(x)\\) és el determinant d’\\(A\\), cal utilitzar que el terme independent de \\(p_A(x)\\) és \\(p_A(0)\\), i per tant és \\(\\det(A-0\\cdot\\1_n)=\\det(A)\\). Demostrem ara que el polinomi característic és el mateix per matrius similars: Proposició 3.3 Si \\(A\\) i \\(B\\in M_n(\\K)\\) són matrius similars, llavors \\(p_A(x)=p_B(x)\\). Prova. Si \\(A\\) i \\(B\\) són matrius similars, existeix \\(S\\in M_n(\\K)\\) una matriu invertible tal que \\(S^{-1}AS=B\\). Llavors: \\[\\begin{align*} p_B(x) &amp; =\\det(B-x\\1_n)=\\det(S^{-1}AS-x\\1_n)=\\det(S^{-1}AS-S^{-1}(x\\1)_nS)=\\\\ &amp; = \\det(S^{-1}(A-x\\1_n)S)= \\det(S^{-1})\\det(A-x\\1_n)\\det(S)=\\\\ &amp; = \\det(S)^{-1}\\det(S)\\det(A-x\\1_n) = p_A(x) \\, . \\end{align*}\\] On a l’última igualtat hem utilitzat que \\(\\det(S^{-1})=\\det(S)^{-1}\\). Exercici 3.5 Demostreu que el recíproc no és cert: trobeu dues matrius \\(A\\) i \\(B\\) amb \\(p_A(x)=p_B(x)\\) però tals que \\(A\\) i \\(B\\) no siguin similars. 3.4 Vectors propis associats a un valor propi L’objectiu d’aquesta secció és estudiar els vectors propis d’un valor propi donat. Aquestes tenen un nom: Definició 3.7 Fixat \\(\\lambda\\in\\K\\), un valor propi d’una matriu \\(A\\in M_n(\\K)\\), el subespai de vectors propis de valor propi \\(\\lambda\\) (més el vector \\(\\vec 0\\)) s’anomena subespai propi d’\\(A\\) associat a \\(\\lambda\\) i el denotem per \\(E_\\lambda\\). En altres paraules: \\[E_\\lambda=\\Ker(f_{A-\\lambda\\1_n})=\\Ker(A-\\lambda I_n) ,\\] quedant demostrat que és un subespai vectorial. Vegem que els subespais propis només s’intersequen al vector \\(\\vec 0\\): Lema 3.7 Fixem \\(A\\in M_n(\\K)\\) i \\(\\lambda_1\\neq\\lambda_2\\) dos valors propis diferents d’\\(A\\). Llavors \\[E_{\\lambda_1}\\cap E_{\\lambda_2}=\\{\\vec 0\\}.\\] Més en general, vectors propis de valor propi diferent són linealment independents. Prova. Sigui \\(\\vec v\\in E_{\\lambda_1}\\cap E_{\\lambda_2}\\), llavors \\(A\\vec v=\\lambda_1\\vec v\\) i \\(A\\vec v=\\lambda_2\\vec v\\), amb \\(\\lambda_1\\neq\\lambda_2\\), i això només pot passar si \\(\\vec v=\\vec 0\\). Vegem ara el cas general: siguin \\(\\{\\vec v_1, \\dots, \\vec v_k\\}\\) vectors propis de valors propis \\(\\{\\lambda_1, \\dots, \\lambda_k\\}\\) respectivament, amb tots els \\(\\lambda_i\\) diferents. Considerem un subconjunt de vectors de \\(\\{\\vec v_1, \\dots, \\vec v_k\\}\\) que siguin linealment independents i que sigui maximal. Aquest subconjunt tindrà \\(\\ell\\) vectors i podem considerar que són els primers (si és necessari, els reordenem). Per tant \\(\\{\\vec v_1, \\dots, \\vec v_\\ell\\}\\subset\\{\\vec v_1, \\dots, \\vec v_k\\}\\) amb els \\(\\ell\\) primers linealment independents. Volem veure \\(\\ell=k\\). Suposem que no, llavors \\(\\ell&lt;k\\) i existeixen uns únics \\(\\mu_1,\\dots, \\mu_\\ell \\in \\K\\) tals que: \\[\\begin{align*} (\\#eq:v-ellp1) \\vec v_{\\ell+1}=\\mu_1\\vec v_1+ \\cdots+ \\mu_\\ell\\vec v_\\ell. \\end{align*}\\] També podem considerar que hi ha dues \\(\\mu_i\\neq0\\): com que \\(\\vec v_{\\ell+1}\\neq\\vec 0\\), com a mínim n’hi ha una. Si només n’hi hagués una, llavors \\(E_{\\lambda_i}\\cap E_{\\lambda_{\\ell+1}}\\neq\\{\\vec 0\\}\\), contradient el primer apartat d’aquest lema. Suposem doncs (si cal, reordenem), \\(\\mu_1\\neq0\\neq\\mu_2\\). Resumint, la situació és la següent: si \\(\\ell&lt;k\\), podem considerar \\(\\vec v_{\\ell+1}\\) és un vector propi de valor propi \\(\\lambda_{\\ell+1}\\) que es pot posar com \\(\\vec v_{\\ell+1}=\\mu_1\\vec v_1+ \\cdots+ \\mu_\\ell\\vec v_\\ell\\) amb \\(\\mu_1\\neq0\\neq\\mu_2\\) i \\(\\vec v_1\\) i \\(\\vec v_2\\) són vectors propis de valor propi \\(\\lambda_1\\) i \\(\\lambda_2\\) respectivament, amb \\(\\lambda_1\\neq\\lambda_2\\). Apliquem \\(A\\) a l’Equació (??): \\[\\begin{align*} \\lambda_{\\ell+1}\\vec v_{\\ell+1} &amp; =A\\vec v_{\\ell+1}=A(\\mu_1\\vec v_1+ \\cdots+ \\mu_\\ell\\vec v_\\ell)= \\\\ &amp; = \\lambda_1\\mu_1\\vec v_1+\\lambda_2\\mu_2\\vec v_2+\\dots+\\lambda_\\ell\\mu_\\ell\\vec v_\\ell \\end{align*}\\] I també: \\[\\begin{align*} \\lambda_{\\ell+1}\\vec v_{\\ell+1} &amp; = \\lambda_{\\ell+1}\\mu_1\\vec v_1+\\lambda_{\\ell+1}\\mu_2\\vec v_2+\\dots+\\lambda_{\\ell+1}\\mu_\\ell\\vec v_\\ell \\end{align*}\\] Per tant, com que \\(\\{\\vec v_1, \\dots, \\vec v_\\ell\\}\\) són linealment independents, tenim: \\[\\lambda_{\\ell+1} \\mu_1=\\lambda_1\\mu_1 \\text{ i } \\lambda_{\\ell+1}\\mu_2=\\lambda_2\\mu_2\\] amb \\(\\mu_1\\neq0\\neq\\mu_2\\), per tant \\(\\lambda_1=\\lambda_{\\ell+1}=\\lambda_2\\), contradient que \\(\\lambda_1\\neq\\lambda_2\\). Per tant, la contradicció ve de suposar \\(\\ell&lt;k\\), el que implica \\(\\ell=k\\) i els \\(k\\) vectors són linealment independents. Exemple 3.13 Si prenem la matriu \\[A=\\begin{pmatrix}2 &amp; 1 \\\\ 0 &amp; 1\\end{pmatrix},\\] als càlculs de l’Exemple 3.11 hem vist que els subespais propis són: \\[E_2=\\langle\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\rangle \\text{ i } E_1=\\langle\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\rangle.\\] A més, com que tenim dos vectors propis linealment independents en un espai de dimensió \\(2\\), tenim una base de vectors propis i la matriu \\(A\\) diagonalitza. Exemple 3.14 Si prenem ara la matriu \\[A=\\begin{pmatrix}1 &amp; 1 \\\\ 0 &amp; 1\\end{pmatrix},\\] als càlculs de l’Exemple 3.12 hem vist que l’únic subespai propi és: \\[E_1=\\langle\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\rangle.\\] En aquest cas, el subespai format pels vectors propis té dimensió \\(1\\), pel que no tenim una base de vectors propis i la matriu \\(A\\) no diagonalitza. Més en general, per saber si una matriu diagonalitza, cal estudiar el polinomi característic i els subespais propis. Per això, ens convé la definició següent: Definició 3.8 Considerem \\(A\\in M_n(\\K)\\) i \\(\\lambda\\) un valor propi d’\\(A\\). Definim la multiplicitat algebraica de \\(\\lambda\\) com a valor propi d’\\(A\\) com el valor \\(m\\geq 1\\) tal que \\[p_A(x)=(x-\\lambda)^m q(x)\\] amb \\(q(x)\\) un polinomi de grau \\(n-m\\) tal que \\(q(\\lambda)\\neq 0\\). Escriurem \\(\\multalg_A(\\lambda)\\). Definim la multiplicitat geomètrica de \\(\\lambda\\) com a valor propi d’\\(A\\) com la dimensió de \\(E_\\lambda\\). Escriurem \\(\\multgeom_A(\\lambda)\\). Observació. A la definició de \\(\\multalg_A(\\lambda)\\) utilitzem que si \\(\\lambda\\) és un valor propi, llavors, \\(p(\\lambda)=0\\). Si ara fem la divisió de polinomis \\(p(x)\\) dividit per \\((x-\\lambda)\\), existeixen polinomis \\(q_1(x)\\) (quocient) i \\(r_1(x)\\) (la resta, que com que ha de ser de grau menor a \\(1\\), ha de ser una constant, i per tant escrivim \\(r_1\\)) tals que: \\[p(x)=(x-\\lambda)q_1(x)+r.\\] Si avaluem a \\(\\lambda\\) ens queda: \\[0=p(\\lambda)=(\\lambda-\\lambda)q_1(\\lambda)+r_1=r_1 ,\\] i per tant \\(r_1=0\\), obtenint que: \\[p(x)=(x-\\lambda)q_1(x).\\] Aquest procediment es pot fer un altre cop si \\(q_1(\\lambda)=0\\), i tindríem que \\(p(x)=(x-\\lambda)^2q_2(x)\\). Iterem el procediment fins que \\(q_m(\\lambda)\\neq0\\), no podem continuar el procediment, i definim la multiplicitat algebraica d’aquesta manera. Mirem ara un cas particular: Proposició 3.4 Si \\(A \\in M_n(\\K)\\) té \\(n\\) valor propis diferents, llavors: Per a cada \\(\\lambda\\) valor propi d’\\(A\\), \\(\\multalg_A(\\lambda)=\\multgeom_A(\\lambda)=1\\). La matriu \\(A\\) diagonalitza. Prova. Com que tenim \\(n\\) valors propis diferents, com a mínim tenim \\(n\\) vectors propis de valor propi diferents, i pel Lema 3.7, seran linealment independents a \\(\\K^n\\), per tant una base. D’aquí obtenim que \\(A\\) diagonalitza. Les \\(n\\) multiplicitats \\(\\multalg_A(\\lambda)\\) i \\(\\multgeom_A(\\lambda)\\) són com a mínim \\(1\\) i, sumades, com a molt \\(n\\) (la suma de les \\(\\multalg\\) és, com a molt, el grau de \\(p_A(x)\\); la suma de les \\(\\multgeom\\) és, com a molt, el nombre màxim d’una família de vectors linealment independents a \\(\\K^n\\)), per tant, han de ser \\(1\\). En general, la situació no serà tant bona i el que tenim és: Lema 3.8 Si \\(A\\in M_n(\\K)\\) i \\(\\lambda\\) és un valor propi d’\\(A\\), llavors: Si \\(\\lambda\\) és un valor propi d’\\(A\\), llavors \\(\\multgeom(\\lambda)\\leq \\multalg(\\lambda)\\). \\(A\\) diagonalitza si i només si \\(p_A(x)\\) es pot escriure com a producte de polinomis de grau \\(1\\) \\[\\begin{align*} \\tag{3.4} p_A(x)=(-1)^n (x-\\lambda_1)^{\\multalg_A(\\lambda_1)}\\cdots(x-\\lambda_k)^{\\multalg_A(\\lambda_k)} \\end{align*}\\] amb \\(\\lambda_i\\) valors propis diferents i \\(\\multgeom_A(\\lambda_i)=\\multalg_A(\\lambda_i)\\) per a tot \\(i=1,\\dots, k\\). Prova. Vegem primer (a): sigui \\(\\lambda\\) un valor propi d’\\(A\\) i considerem \\(m=\\multgeom(\\lambda)\\), i per tant \\(\\vec v_1, \\dots, \\vec v_m\\) vectors linealment independents de \\(\\Ker(A-\\lambda\\1_n)\\). Ampliem la família \\(\\vec v_1, \\dots, \\vec v_m\\) fins a tenir una base \\(\\calb\\) de \\(\\K^n\\). En aquesta base, la matriu \\([f_A]_\\calb\\) tindrà a les primeres \\(m\\) columnes tots els coeficients zero, excepte la diagonal, que valdrà \\(\\lambda\\), per tant, \\(p_A(x)=p_{[f_A]_\\calb}=(-1)^n(x-\\lambda)^m q(x)\\) i per tant \\(m=\\multgeom(A)\\leq\\multalg(A)\\). Vegem ara (b): suposem primer que \\(A\\) diagonalitza, llavors, la suma de les multiplicitats geomètriques d’\\(A\\) serà \\(n\\): \\[\\multgeom_A(\\lambda_1)+\\cdots+\\multgeom_A(\\lambda_k)=n\\] amb \\(\\lambda_i\\) valors propis diferents. Però com que \\(\\multgeom(\\lambda)\\leq\\multalg(\\lambda)\\), \\[n\\leq \\multalg_A(\\lambda_1)+\\cdots+\\multalg_A(\\lambda_n) \\leq n\\] on la segona desigualtat és perquè \\(p_A(x)\\) té grau \\(n\\), per tant, les multiplicitats geomètriques i algebraiques són iguals i el polinomi \\(p_A(x)\\) es pot escriure com a producte de polinomis de grau \\(1\\). El recíproc es fa desfent els arguments previs: si \\(p_A(x)\\) factoritza com a producte de factors de grau \\(1\\) com a l’Equació (3.4), per a que diagonalitzi, s’ha de complir que trobem una base de vectors propis. Com que per cada \\(\\lambda\\) valor propi, per hipòtesis, \\(\\multgeom(\\lambda)_A=\\multalg_A(\\lambda)\\), tindrem \\(n\\) vector propis linealment independents, i per tant una base en que \\(A\\) diagonalitza. Exemple 3.15 Volem estudiar si la matriu \\(A\\) següent diagonalitza i, si ho fa, en quina base: \\[A=\\left(\\begin{array}{rrrr} 55 &amp; 91 &amp; -29 &amp; 50 \\\\ -27 &amp; -47 &amp; 15 &amp; -24 \\\\ -33 &amp; -67 &amp; 21 &amp; -26 \\\\ -33 &amp; -57 &amp; 18 &amp; -29 \\end{array}\\right).\\] Calculem primer el polinomi característic: \\[p_A(x)=\\left|\\begin{array}{cccc} 55-x &amp; 91 &amp; -29 &amp; 50 \\\\ -27 &amp; -47-x &amp; 15 &amp; -24 \\\\ -33 &amp; -67 &amp; 21-x &amp; -26 \\\\ -33 &amp; -57 &amp; 18 &amp; -29-x \\end{array}\\right|=x^4-3x^2+2x=x(x-1)^2(x+2).\\] Llavors hem de calcular \\(E_0=\\Ker(f_A)\\), \\(E_1=\\Ker(f_{A-\\1_4})\\) i \\(E_{-2}=\\Ker(f_{A+2\\cdot\\1_4})\\). Sense calcular-los explícitament, per comprovar si diagonalitza només cal calcular \\(\\dim(E_1)\\). Si \\(\\dim(E_1)=2\\), llavors diagonalitza, però si \\(\\dim(E_1)=1\\), no diagonalitza. En aquest cas surt \\(\\dim(E_1)=2\\) i per tant diagonalitza. Fem els càlculs: \\[E_0=\\langle\\begin{pmatrix}4\\\\-3\\\\-7\\\\-3\\end{pmatrix}\\rangle \\text{, } E_1=\\langle\\begin{pmatrix}4\\\\2\\\\-4\\\\-3\\end{pmatrix},\\begin{pmatrix}11\\\\-5\\\\-9\\\\-8\\end{pmatrix}\\rangle \\text{ i } E_{-2}=\\langle\\begin{pmatrix}15\\\\-9\\\\-16\\\\-10\\end{pmatrix}\\rangle\\] Pel que podem confirmar que diagonalitza (hi ha 4 vectors propis linealment independents). Llavors es pot comprovar que: \\[D=S^{-1}AS\\] on \\[D=\\left(\\begin{array}{rrrr} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -2 \\end{array}\\right) \\text{ i } S=\\left(\\begin{array}{rrrr} 4 &amp; 4 &amp; 11 &amp; 15 \\\\ -3 &amp; 2 &amp; -5 &amp; -9 \\\\ -7 &amp; -4 &amp; -9 &amp; -16 \\\\ -3 &amp; -3 &amp; -8 &amp; -10 \\end{array}\\right).\\] Exemple 3.16 Considerem ara la matriu: \\[A=\\left(\\begin{array}{rrrr} 107 &amp; 151 &amp; -49 &amp; 106 \\\\ -53 &amp; -77 &amp; 25 &amp; -52 \\\\ -85 &amp; -127 &amp; 41 &amp; -82 \\\\ -72 &amp; -102 &amp; 33 &amp; -71 \\end{array}\\right)\\] Podem fer els càlculs d’abans, obtenint el mateix polinomi característic: \\[p_A(x)=\\left|\\begin{array}{cccc} 107-x &amp; 151 &amp; -49 &amp; 106 \\\\ -53 &amp; -77-x &amp; 25 &amp; -52 \\\\ -85 &amp; -127 &amp; 41-x &amp; -82 \\\\ -72 &amp; -102 &amp; 33 &amp; -71-x \\end{array}\\right|=x^4-3x^2+2x=x(x-1)^2(x+2).\\] En aquest cas, però, tenim que (després de fer els càlculs) \\(\\dim(E_1)=1&lt;2=\\multalg_A(1)\\), per tant no diagonalitza. Exemple 3.17 Es defineix la successió de Fibonacci com: \\[a_1=a_2=1 \\text{ i } a_{n}=a_{n-1}+a_{n-2} \\text{ per a $n\\geq 2$}.\\] Podem calcular-ne els seus primers termes: \\[1, 1, 2, 3, 5, 8, 13, 21, 34, \\dots\\] aquesta definició fa que per calcula el terme \\(n\\), haguem de calcular tots els anteriors. L’objectiu és trobar una fórmula \\(f(n)\\) tal que \\(a_n=f(n)\\). Considerem els vectors \\(\\smat{a_{n}\\\\a_{n-1}}\\), i veiem que es compleix la relació següent: \\[\\begin{pmatrix} a_{n}\\\\a_{n-1} \\end{pmatrix}= \\begin{pmatrix} 1 &amp; 1\\\\1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} a_{n-1}\\\\a_{n-2} \\end{pmatrix}= \\begin{pmatrix} 1 &amp; 1\\\\1 &amp; 0 \\end{pmatrix}^2 \\begin{pmatrix} a_{n-2}\\\\a_{n-3} \\end{pmatrix}= \\cdots = \\begin{pmatrix} 1 &amp; 1\\\\1 &amp; 0 \\end{pmatrix}^{n-2} \\begin{pmatrix} a_{2}\\\\a_{1} \\end{pmatrix}= \\begin{pmatrix} 1 &amp; 1\\\\1 &amp; 0 \\end{pmatrix}^{n-2} \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}\\] Per tant, ens convé calcular: \\[A^{n-2}= \\begin{pmatrix} 1 &amp; 1\\\\1 &amp; 0 \\end{pmatrix}^{n-2}\\] Si aconseguim escriure \\(A=SDS^{-1}\\) (\\(D\\) matriu diagonal, per tant, la diagonal de \\(D\\) està formada pels valors propis de \\(A\\)), tenim que \\(A^{n-2}=SD^{n-2}S^{-1}\\), i, si \\(D=\\smat{\\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2}\\), llavors: \\[D^{n-2}=\\begin{pmatrix} \\lambda_1^{n-2} &amp; 0 \\\\ 0 &amp; \\lambda_2^{n-2} \\end{pmatrix}\\] Calculem els valor propis d’\\(A\\). Calculem el polinomi característic: \\[p(x)=\\det(A-x\\1_2)=\\begin{vmatrix} 1-x &amp; 1 \\\\ 1 &amp; -x \\end{vmatrix}= x^2-x-1\\] I els valors propis són les solucions de l’equació \\(p(x)=0\\), per tant: \\[\\lambda_i=\\frac{1\\pm \\sqrt{1+4}}{2}=\\frac{1\\pm\\sqrt{5}}{2}\\] Per tant, com que tenim dos valors propis diferents, ja sabem que diagonalitza i tenim: \\[D= \\begin{pmatrix} \\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2 \\end{pmatrix}= \\begin{pmatrix} \\frac{1+\\sqrt{5}}{2} &amp; 0 \\\\ 0 &amp; \\frac{1-\\sqrt{5}}{2} \\end{pmatrix}\\] Per calcular els vectors propis corresponents a cada valor propi, hem de resoldre els sistemes homogenis: \\[\\left. \\begin{array}{rrr} (1-\\lambda_i) x + &amp; y &amp; =0 \\\\ x - &amp; \\lambda_i y &amp;= 0 \\end{array} \\right\\} \\Longleftrightarrow \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\in \\langle \\begin{pmatrix} \\lambda_i \\\\ 1 \\end{pmatrix} \\rangle\\] Llavors, tenim que: \\[S= \\begin{pmatrix} \\lambda_1 &amp; \\lambda_2 \\\\ 1 &amp; 1 \\end{pmatrix} %\\begin{pmatrix} %\\frac{1+\\sqrt{5}}{2} &amp; \\frac{1-\\sqrt{5}}{2} \\\\ 1 &amp; 1 %\\end{pmatrix} \\text{ i } S^{-1}= \\frac{1}{\\lambda_1-\\lambda_2}\\begin{pmatrix} 1 &amp; -\\lambda_2 \\\\ -1 &amp; \\lambda_1 \\end{pmatrix} %\\frac{1}{\\sqrt{5}}\\begin{pmatrix} %1 &amp; \\frac{-1+\\sqrt{5}}{2} \\\\ %-1 &amp; \\frac{1+\\sqrt{5}}{2} %\\end{pmatrix}\\] I per tant: \\[A^{n-2}=SD^{n-2}S^{-1}= \\begin{pmatrix} \\lambda_1 &amp; \\lambda_2 \\\\ 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1^{n-2} &amp; 0 \\\\ 0 &amp; \\lambda_2^{n-2} \\end{pmatrix} \\frac{1}{\\lambda_1-\\lambda_2}\\begin{pmatrix} 1 &amp; -\\lambda_2 \\\\ -1 &amp; \\lambda_1 \\end{pmatrix} %\\begin{pmatrix} %\\frac{1+\\sqrt{5}}{2} &amp; \\frac{1-\\sqrt{5}}{2} \\\\ 1 &amp; 1 %\\end{pmatrix} %\\begin{pmatrix} %\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{n-2} &amp; 0 \\\\ %0 &amp; \\left(\\frac{1-\\sqrt{5}}{2}\\right)^{n-2} %\\end{pmatrix} %\\frac{1}{\\sqrt{5}}\\begin{pmatrix} %1 &amp; \\frac{-1+\\sqrt{5}}{2} \\\\ %-1 &amp; \\frac{1+\\sqrt{5}}{2} %\\end{pmatrix}\\] I si fem els càlculs: \\[\\begin{pmatrix} a_n\\\\a_{n-1} \\end{pmatrix}= A^{n-2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}= \\frac{1}{\\lambda_1-\\lambda_2} \\begin{pmatrix} \\lambda_1^{n-1}-\\lambda_2^{n-1} &amp; \\lambda_1^{n-2}-\\lambda_2^{n-2} \\\\ \\lambda_1^{n-2}-\\lambda_2^{n-2} &amp; \\lambda_1^{n-3}-\\lambda_2^{n-3} \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} %\\frac{1}{\\sqrt{5}} \\begin{pmatrix} %\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{n-1}-\\left(\\frac{1-\\sqrt{5}}{2}\\right)^{n-1} &amp; %\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{n-2}-\\left(\\frac{1-\\sqrt{5}}{2}\\right)^{n-2} \\\\ %\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{n-2}-\\left(\\frac{1-\\sqrt{5}}{2}\\right)^{n-2} &amp; %\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{n-3}-\\left(\\frac{1-\\sqrt{5}}{2}\\right)^{n-3} %\\end{pmatrix}\\] Llavors, després de simplificar: \\[a_n= \\frac{1}{\\lambda_1-\\lambda_2}(\\lambda_1^n-\\lambda_2^n)=\\frac{1}{\\sqrt{5}}\\left( \\left(\\frac{1+\\sqrt{5}}{2}\\right)^n-\\left(\\frac{1-\\sqrt{5}}{2}\\right)^n \\right). %=\\frac{1}{\\sqrt{5}} \\left(\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{n-1}-\\left(\\frac{1-\\sqrt{5}}{2}\\right)^{n-1} + %\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{n-2}-\\left(\\frac{1-\\sqrt{5}}{2}\\right)^{n-2}\\right)\\] 3.5 Interludi: els nombres complexos Introduïm aquí el cos dels nombres complexos, que ens seran útils a la secció següent, quan estudiem matrius sobre els nombres reals. Podem definir els nombres complexos com \\[\\C =\\{a+bi ~|~ a,b\\in\\R\\}.\\] Com a espai vectorial sobre \\(\\R\\), podem identificar \\(\\C\\) amb \\(\\R^2\\), i per tant tenim definida una suma i una producte per reals: \\[(a+bi) + (c+di) = (a+c) + (b+d)i, \\quad \\lambda(a+bi)=\\lambda a + \\lambda b i.\\] Tenim però una operació addicional, el producte de nombres complexos: \\[(a+bi)(c+di) = (ac-bd) + (ad+bc)i,\\] on la fórmula es recorda fàcilment si tenim en compte que \\(i^2=-1\\), i fent servir la propietat distributiva. Definim la part real i la part imaginària d’un nombre complex com: \\[\\Re(a+bi) = a,\\quad \\Im(a+bi) = b.\\] També el conjugat d’un nombre complex es defineix com: \\[\\overline{(a+bi)} = a-bi,\\] i per tant tenim les fórmules \\[\\Re(z) = \\frac{z+\\bar z}{2},\\quad \\Im(z)=\\frac{z-\\bar z}{2i},\\quad \\forall z\\in\\C.\\] La norma d’un element \\(z=a+bi\\) és \\[N(z) = z\\bar z = (a+bi)(a-bi) = a^2+b^2\\geq 0.\\] Observem que \\(z\\bar z \\geq 0\\), i que \\(z\\bar z = 0\\) si i només si \\(z=0\\). Això ens permet calcular fàcilment una fórmula per l’invers d’\\(a+bi\\): \\[(a+bi)^{-1} = \\frac{1}{a+bi} = \\frac{a-bi}{(a+bi)(a-bi)} = \\frac{a}{a^2+b^2}+\\frac{-b}{a^2+b^2}i.\\] Pensat com un element de \\(\\R^2\\), \\(N(z)=|z|^2\\), on \\(|z|=\\sqrt{a^2+b^2}\\) és el mòdul de \\(z\\) pensat com un element de \\(\\R^2\\). Les coordenades polars del punt \\((a,b)\\in\\R^2\\) es calculen fent servir que: \\[a = r\\cos(\\theta),\\quad b = r\\sin(\\theta),\\] i per tant: \\[r = \\sqrt{z\\bar z}=\\sqrt{a^2+b^2},\\quad \\theta = \\arg(z) = \\arctan(b/a) (+\\pi),\\] on haurem de sumar \\(\\pi\\) a \\(\\arctan(b/a)\\in[-\\pi/2,\\pi/2]\\) si \\(a&lt;0\\). Podem escriure (aquí ho podem pensar com una notació, encara que té una justificació algebraica) que \\[a+bi = re^{i\\theta},\\] i així podem recordar les fórmules \\[|zw|=|z||w|,\\quad \\arg(zw)=\\arg(z)+\\arg(w)\\pmod{2\\pi}.\\] El següent resultat és el motiu pel què ens interessa treballar a \\(\\C\\): Teorema 3.4 Tot polinomi \\(f(x)\\in \\C[x]\\) de grau \\(n\\geq 0\\) es pot escriure com \\[f(x) = c (x-\\lambda_1)\\cdots (x-\\lambda_n),\\] on \\(c\\) i \\(\\lambda_1,\\ldots,\\lambda_n\\) són nombres complexos (possiblement repetits). 3.6 Matrius sobre \\(\\R\\) En aquest apartat suposarem donada una matriu \\(A\\in M_n(\\R)\\). Una conseqüència del teorema fonamental de l’àlgebra és que \\(p_A(x)\\) descomposa en producte de factors de grau \\(1\\) i \\(2\\). Si hi ha algun factor de grau \\(2\\), aleshores \\(A\\) no diagonalitza sobre \\(\\R\\), tal i com hem vist. Exemple 3.18 Considerem la matriu \\(R_{a,b}=\\begin{pmatrix}a&amp;-b\\\\b&amp;a\\end{pmatrix}\\), on \\(a, b\\in\\R\\) i \\(b\\neq 0\\). El seu polinomi característic és \\[p_A(x) = x^2-2ax +a^2+b^2 = (x-(a+ib))(x-(a-ib)).\\] Escrivim \\(\\lambda^+= a+bi\\), i \\(\\lambda^- = a-bi\\). Podem calcular \\[E_{\\lambda^+} = \\langle \\begin{pmatrix}i\\\\1\\end{pmatrix}\\rangle, \\quad E_{\\lambda^-} = \\langle \\begin{pmatrix}-i\\\\1\\end{pmatrix}\\rangle.\\] Per tant, obtenim \\[\\begin{pmatrix} i &amp; -i\\\\1 &amp; 1 \\end{pmatrix}^{-1} \\begin{pmatrix} a&amp;-b\\\\b&amp;a \\end{pmatrix} \\begin{pmatrix} i &amp; -i\\\\1 &amp; 1 \\end{pmatrix} = \\begin{pmatrix} a+bi&amp;0\\\\0&amp;a-bi \\end{pmatrix}.\\] Exemple 3.19 Suposem que \\(A\\in M_2(\\R)\\) té valors propis \\(a\\pm bi\\), amb \\(b\\neq 0\\). Com que els valors propis són diferents, la matriu \\(A\\) és diagonalitzable i, per tant, és similar a \\[D = \\begin{pmatrix} a+bi&amp;0\\\\0&amp;a-bi \\end{pmatrix}.\\] Com que la matriu \\(D\\) és també similar a \\[R_{a,b} = \\begin{pmatrix} a&amp;-b\\\\b&amp;a \\end{pmatrix},\\] la matriu \\(A\\) és similar a \\(R_{a,b}\\). De fet, tenim: \\[D = S^{-1} A S,\\] on \\(S\\) té per columnes \\(\\vec u\\) i \\(\\bar \\vec u\\), on \\(\\vec u = \\vec v + i \\vec w\\), amb \\(\\vec v,\\vec w\\in \\R^2\\). Substituint \\(D = T^{-1} R_{a,b} T\\) amb \\(T = \\begin{pmatrix} i &amp; -i\\\\1 &amp; 1 \\end{pmatrix}\\), tenim \\[T^{-1} R_{a,b} T = S^{-1} A S\\Longrightarrow R_{a,b} = TS^{-1} A ST^{-1}.\\] Calculem directament que \\[ST^{-1} = \\begin{pmatrix} \\vert&amp;\\vert\\\\ \\vec w&amp;\\vec v\\\\ \\vert&amp;\\vert \\end{pmatrix}.\\] 3.7 Exercicis recomanats Els exercicis que segueixen són útils per practicar el material presentat. La numeració és la de [1]. Secció 6.2: 4, 18. Secció 6.3: 2, 6. Secció 7.1: 10, 18, 34, 36. Secció 7.2: 2, 10, 20. Secció 7.3: 14, 22, 24. Secció 7.5: 14, 24, 26. References "],["ortogonalitat.html", "Capítol 4. Ortogonalitat 4.1 Ortogonalitat a \\(\\R^n\\) 4.2 El teorema de Pitàgores 4.3 Mètode de Gram-Schmidt 4.4 Aplicacions i matrius ortogonals 4.5 Matriu d’una projecció ortogonal en una base ortonormal 4.6 Mínims quadrats 4.7 Formes bilineals i productes escalars 4.8 Tota matriu simètrica sobre \\(\\mathbb{R}\\) diagonalitza 4.9 Descomposició en valors singulars 4.10 Classificació de formes bilineals simètriques sobre \\(\\mathbb{R}^n\\) 4.11 Exercicis recomanats", " Capítol 4. Ortogonalitat Podeu trobar el contingut de la part d’ortogonalitat a [1 Tema 5], i de la part de formes quadràtiques a ([1],[2]). 4.1 Ortogonalitat a \\(\\R^n\\) Considerem l’espai vectorial \\(\\R^n\\) i recordem el producte escalar definit com: si \\(\\vec u=\\smat{u_1\\\\u_2\\\\ \\vdots\\\\u_n}\\) i \\(\\vec v=\\smat{v_1\\\\v_2\\\\ \\vdots \\\\ v_n}\\) són vectors d’\\(\\R^n\\), llavors \\(\\vec u \\cdot \\vec v = \\vec u^T \\vec v=\\sum_{i=1}^n u_iv_i\\). Observació. Observem que a les igualtats anteriors hi ha cert abús de notació: estem definint el producte escalar (que denotem amb “\\(\\cdot\\)”), que resulta en un nombre real. El terme entre les dues igualtats representa una matriu \\(1\\times 1\\), que obtenim del producte de la matriu fila formada amb les entrades del vector \\(\\vec u\\) amb la matriu columna formada amb les entrades del vector \\(\\vec v\\). Per tant, estem identificant vectors amb matrius columna, i escalars amb matrius \\(1\\times 1\\), i ho seguirem fent sense massa risc de confusió. Definició 4.1 Diem que dos vectors \\(\\vec u\\) i \\(\\vec v\\) són ortogonals si \\(\\vec u\\cdot\\vec v=0\\). Més generalment, diem que els vectors \\(\\vec u_1, \\dots, \\vec u_k\\) de \\(\\R^n\\) són ortogonals si són ortogonals dos a dos, o sigui, si \\(\\vec u_i\\cdot\\vec u_j=0\\) per a tot \\(i\\neq j\\). Definim la longitud d’un vector com la quantitat \\(\\|\\vec u\\|=\\sqrt{\\vec u\\cdot \\vec u}=\\sqrt{\\displaystyle\\sum_{i=1}^n u_i^2}.\\) Diem que un vector \\(\\vec u\\in\\R^n\\) és unitari si \\(\\|\\vec u\\|=1\\). Diem que els vectors \\(\\vec u_1, \\dots, \\vec u_k\\) de \\(\\R^n\\) són ortonormals si són unitaris i ortogonals dos a dos, o sigui, si \\[\\vec u_i\\cdot\\vec u_j= \\begin{cases} 1 &amp; \\text{si $i=j$} \\\\ 0 &amp; \\text{si $i\\neq j.$} \\end{cases}\\] Observació. Podem passar de vectors no nuls \\(\\vec u_1, \\dots, \\vec u_k\\) de \\(\\R^n\\) ortogonals a ortonormals dividint cadascun per la seva norma: si \\(\\vec u_1, \\dots, \\vec u_k\\) de \\(\\R^n\\) són no nuls, llavors \\(\\frac{\\vec u_1}{\\|\\vec u_1\\|}, \\dots, \\frac{\\vec u_k}{\\|\\vec u_k\\|}\\) són unitaris, i si \\(\\vec u_i\\cdot \\vec u_j=0\\), llavors, \\(\\frac{\\vec u_i}{\\|\\vec u_i\\|}\\cdot \\frac{\\vec u_j}{\\|\\vec u_j\\|}=\\frac{1}{\\|\\vec u_i\\| \\|\\vec u_j\\|}(\\vec u_i\\cdot \\vec u_j)=0\\). Exemple 4.1 La base estàndard d’\\(\\R^n\\) formada pels vectors \\(\\vec e_i\\) amb un \\(1\\) a la posició \\(i\\) i \\(0\\) a la resta de posicions és una base ortonormal. Lema 4.1 Si els vectors no nuls \\(\\vec u_1, \\dots, \\vec u_k\\) de \\(\\R^n\\) són ortogonals, llavors són linealment independents. En particular, si tenim \\(n\\) vectors \\(\\vec u_1, \\dots, \\vec u_n\\) de \\(\\R^n\\) ortonormals, formen una base. Prova. Suposem que tenim una combinació lineal dels vectors \\(\\vec u_1, \\dots, \\vec u_k\\) que dóna zero: \\[0 = \\lambda_1 \\vec u_1 + \\cdots + \\lambda_n\\vec u_n \\,.\\] Si veiem que tots els \\(\\lambda_i\\) són zero ja estarem. Triem una \\(i\\) i fent el producte escalar per \\(\\vec u_i\\) tenim: \\[\\begin{align*} 0 &amp; = (\\lambda_1 \\vec u_1 + \\cdots + \\lambda_n\\vec u_n)\\cdot \\vec u_i= \\\\ &amp; = \\lambda_1 (\\vec u_1 \\cdot \\vec u_i) + \\cdots + \\lambda_n(\\vec u_n\\cdot \\vec u_i)=\\\\ &amp; = \\lambda_i \\|\\vec u_i\\|^2\\quad \\text{(tots els altres productes escalars són zero)} \\end{align*}\\] Per tant, com que \\(\\vec u_i\\neq\\vec 0\\), \\(\\lambda_i=0\\). Com que això és cert per cada \\(i=1,\\ldots,n\\), els vectors són linealment independents. En el cas de tenir \\(n\\) vectors ortonormals, són \\(n\\) vectors ortogonals no nuls, per tant linealment independents i com que la dimensió de \\(\\R^n\\) és \\(n\\), són base (Teorema 2.4). Veurem més endavant com trobar una base ortonormal d’un subespai \\(V\\) de \\(\\R^n\\). Suposem però que tenim \\([\\vec u_1,\\ldots,\\vec u_k]\\) una base ortonormal de \\(V\\), i considerem la projecció ortogonal \\(\\proj_V\\colon \\R^n\\rightarrow \\R^n\\), que envia un vector \\(\\vec v\\in \\R^n\\) al vector \\(\\proj_V(\\vec v) = \\vec v^\\parallel\\), on \\[\\vec v^\\parallel = (\\vec v\\cdot \\vec u_1)\\vec u_1 + \\cdots + (\\vec v\\cdot \\vec u_k)\\vec u_k \\in V.\\] Observem que \\(\\proj_V\\colon \\R^n\\rightarrow \\R^n\\) és una aplicació lineal (per les propietats de linealitat del producte escalar). Com que la seva imatge està continguda a \\(V\\) i \\(\\proj_V(\\vec u_i) = \\vec u_i\\), en deduïm que \\(\\Ima(\\proj_V)=V\\). A més, \\[\\begin{align*} \\Ker(\\proj_V) &amp;= \\{\\vec w\\in\\R^n ~|~ \\vec w^\\parallel = 0\\}\\\\ &amp;= \\{\\vec w\\in\\R^n ~|~ \\vec w\\cdot \\vec u_i = 0, \\quad i=1,\\ldots k\\}\\\\ &amp;= \\{\\vec w\\in\\R^n ~|~ \\vec w\\cdot \\vec v = 0\\quad\\text{ per a tot }\\vec w \\in V\\}. \\end{align*}\\] La següent definició dona nom al nucli de \\(\\proj_V\\). Definició 4.2 Donat un subespai \\(V\\subseteq \\R^n\\), el complement ortogonal de \\(V\\), que escriurem \\(V^\\perp\\), és el conjunt \\[V^\\perp = \\{\\vec w\\in \\R^n ~|~ \\vec w\\cdot \\vec v = 0\\text{ per a tot } \\vec v\\in V\\}.\\] El complement ortogonal satisfà les següents propietats bàsiques. Proposició 4.1 Sigui \\(V\\subseteq \\R^n\\) un subespai d’\\(\\R^n\\). Aleshores: \\(V^\\perp\\) també és un subespai d’\\(\\R^n\\), \\(V \\cap V^\\perp = \\{\\vec 0\\}\\), \\(\\dim(V)+\\dim(V^\\perp) = n\\), \\((V^\\perp)^\\perp = V\\). Prova. La primera afirmació es comprova de manera directa, fent servir les propietats del producte escalar. Per veure la segona, sigui \\(\\vec w\\in V\\cap V^\\perp\\). Per tant, \\(\\vec w\\cdot\\vec w =\\| \\vec w\\|= 0\\). Però l’únic vector de longitud \\(0\\) és el vector nul. Seguidament, observem que \\(V=\\Ima(\\proj_V)\\) i que \\(V^\\perp = \\Ker(\\proj_V)\\). Per tant, per la fórmula del nucli–imatge (Teorema 2.5) tenim \\[\\dim(\\Ker\\proj_V) + \\dim(\\Ima\\proj_V) = \\dim(\\R^n)=n,\\] com volíem veure. Finalment, per veure que \\((V^\\perp)^\\perp = V\\), denotem \\(W=V^\\perp\\). Aleshores, si \\(\\vec v\\in V\\), i \\(\\vec w\\in W\\), es té que \\(\\vec v\\cdot \\vec w = 0\\), i per tant \\(\\vec v\\in W^\\perp\\). Concloem doncs que \\(V\\subseteq (V^\\perp)^\\perp\\). Però ara observem que \\(\\dim((V^\\perp)^\\perp) = n - \\dim(V^\\perp) = n- (n-\\dim(V)) = \\dim(V)\\). Veiem que \\((V^\\perp)^\\perp\\) és un subespai que conté \\(V\\) i que té la mateixa dimensió que \\(V\\) i, per tant, ha de coincidir amb \\(V\\). Corol·lary 4.1 Si \\(V\\subseteq\\R^n\\) és un subespai d’\\(\\R^n\\), aleshores \\(\\R^n=V\\oplus V^\\perp\\). Prova. Per l’apartat (b) de la proposició anterior, la intersecció és buida. Per l’apartat (c), la suma de de les dimensions és \\(n\\). Per tant, si prenem una base \\(\\calb\\) de \\(V\\) i una base \\(\\calc\\) de \\(V^\\perp\\), el conjunt \\(\\calb\\cup\\calc\\) serà linealment independent i tindrà \\(n\\) elements. Per tant, serà base de \\(\\R^n\\), que és el que volíem veure. D’aquest corol·lari es dedueix que, donat \\(V\\) un subespai d’\\(\\R^n\\), tot vector \\(\\vec w\\in\\R^n\\) es pot escriure de forma única com: \\[\\vec w = \\vec w^\\parallel + \\vec w^\\perp\\] amb \\(\\vec w^\\parallel \\in V\\) i \\(\\vec w^\\perp \\in V^\\perp\\). A més, tenim que \\(\\vec w^\\perp=\\proj_{V^\\perp}(\\vec w)\\). 4.2 El teorema de Pitàgores El conegut teorema de Pitàgores es pot formular a \\(\\R^n\\). Fixem-nos que el teorema té dues implicacions. Teorema 4.1 Si \\(\\vec v\\) i \\(\\vec w\\) són dos vectors d’\\(\\R^n\\), aleshores: \\[\\| \\vec v + \\vec w \\|^2 = \\|\\vec v\\|^2 + \\|\\vec w\\|^2 \\iff \\vec v \\perp \\vec w.\\] Prova. Calculem: \\[\\| \\vec v + \\vec w\\|^2 = (\\vec v+\\vec w)\\cdot (\\vec v+\\vec w) = \\vec v\\cdot \\vec v + \\vec v\\cdot \\vec w + \\vec w\\cdot \\vec v + \\vec w\\cdot \\vec w=\\|\\vec v\\|^2 + \\|\\vec w\\|^2 +2(\\vec v\\cdot \\vec w).\\] Per tant, la igualtat es compleix si i només si \\(\\vec v\\cdot \\vec w = 0\\). Corol·lary 4.2 Sigui \\(V\\subseteq \\R^n\\) un subespai. Aleshores \\[\\|\\proj_V(\\vec w)\\| \\leq \\| \\vec w\\| \\text{ per a tot $\\vec w\\in\\R^n$,}\\] i la desigualtat és una igualtat si i només si \\(\\vec w\\in V\\). Prova. Apliquem el teorema anterior a \\(\\vec w^\\parallel\\) i \\(\\vec w^\\perp\\) (notem \\(\\vec w = \\vec w^\\parallel + \\vec w^\\perp\\)). \\[\\|\\vec w\\|^2 = \\|\\vec w^\\parallel\\|^2 + \\|\\vec w^\\perp\\|^2,\\] i per tant \\[\\|\\proj_V(\\vec w)\\|^2 \\leq \\|\\vec w\\|^2,\\] amb igualtat si i només si \\(\\|\\vec w^\\perp\\|^2 = 0\\), és a dir si i només si \\(\\vec w^\\perp = 0\\), que és equivalent a \\(\\vec w\\in V\\). Una aplicació d’aquests resultats és una desigualtat molt famosa, coneguda com la desigualtat de Cauchy–Schwarz: Teorema 4.2 Si \\(\\vec v\\) i \\(\\vec w\\) són vectors de \\(\\R^n\\), aleshores \\[|\\vec v\\cdot \\vec w|\\leq \\|\\vec v\\|\\|\\vec w\\|,\\] amb igualtat si i només si \\(\\vec v\\) i \\(\\vec w\\) són paral·lels. Prova. Si \\(\\vec v=\\vec 0\\), aleshores la desigualtat es compleix trivialment. En cas contrari, considerem el subespai \\(V=\\langle \\vec v\\rangle\\), i apliquem el resultat anterior. Ja havíem vist la fórmula \\[\\proj_V(\\vec w) = \\frac{\\vec w\\cdot \\vec v}{\\|\\vec v\\|^2} \\vec v,\\] i per tant \\[\\|\\proj_V(\\vec w)\\| = \\frac{|\\vec w\\cdot \\vec v|}{\\|\\vec v\\|^2} \\|\\vec v\\|,\\] i obtenim \\[\\frac{|\\vec w\\cdot \\vec v|}{\\|\\vec v\\|} \\leq \\|\\vec w\\|,\\] que és la desigualtat que busquem un cop passem \\(\\|\\vec v\\|\\) a l’altra banda (i utilitzem que \\(\\vec w\\cdot \\vec v=\\vec v\\cdot \\vec w\\)). La igualtat es dóna si i només si \\(\\vec w\\in V\\), que és equivalent a \\(\\vec v\\) essent paral·lel a \\(\\vec w\\). Observem que si \\(\\vec v\\) i \\(\\vec w\\) no són zero, aleshores tenim les desigualtats \\[-1 \\leq \\frac{\\vec v\\cdot \\vec w}{\\|\\vec v\\|\\|\\vec w\\|}\\leq 1.\\] Definició 4.3 Donats dos vectors no-nuls \\(\\vec v\\) i \\(\\vec w\\) d’\\(\\R^n\\), l’angle entre \\(\\vec v\\) i \\(\\vec w\\) és \\[\\theta = \\arccos\\left(\\frac{\\vec v\\cdot \\vec w}{\\|\\vec v\\|\\|\\vec w\\|}\\right).\\] 4.3 Mètode de Gram-Schmidt Aquest mètode permet calcular una base ortonormal d’un subespai de \\(\\R^n\\) a partir de projeccions ortogonals. Fixem les notacions següents: \\(V\\subset \\R^n\\) un subespai de dimensió \\(k\\) fixat, \\(\\calb =[\\vec v_1, \\dots , \\vec v_k]\\) una base de \\(V\\), \\(V_i=\\langle \\vec v_1, \\dots , \\vec v_i\\rangle \\subset V\\) el subespai (de dimensió \\(i\\)) generat pels \\(i\\) primers vectors de \\(\\calb\\). Tenim que \\(V_1\\subset V_2\\subset \\cdots \\subset V_k=V\\). Si \\(\\vec v_i\\in\\calb\\), denotarem per \\(\\vec v_i^\\parallel\\) la projecció ortogonal de \\(\\vec v_i\\) a \\(V_{i-1}\\) i per \\(\\vec v_i^\\perp\\) el vector \\(\\vec v_i - \\vec v_i^\\parallel\\), que és ortogonal a \\(V_{i-1}\\). El mètode de Gram-Schmidt construeix de forma iterativa una base ortonormal a cada \\(V_i\\): Considerem primer \\(V_1=\\langle \\vec v_1 \\rangle\\), i una base és \\(\\calb_1=[\\vec v_1]\\). En aquest cas, definim \\[\\vec u_1=\\frac{1}{\\|\\vec v_1\\|} \\vec v_1\\] i tenim que \\(\\calc_1=[\\vec u_1]\\) és una base ortonormal de \\(V_1\\). Considerem ara \\(V_2=\\langle \\vec v_1, \\vec v_2 \\rangle=\\langle \\vec u_1, \\vec v_2 \\rangle\\). Pel Corol·lari 4.1, \\(\\vec v_2\\) es pot escriure de forma única com: \\[\\vec v_2 = \\vec v_2^\\parallel + \\vec v_2^\\perp\\] amb \\(\\vec v_2^\\parallel\\in V_1\\) i \\(\\vec v_2\\perp \\in V_1^\\perp\\). A més, sabem com calcular-los: \\[\\vec v_2^\\parallel = \\proj_{V_1}(\\vec v_2)=(\\vec v_2 \\cdot \\vec u_1)\\vec u_1\\] i per tant \\[\\vec v_2^\\perp = \\vec v_2 - \\vec v_2^\\parallel = \\vec v_2 - (\\vec v_2 \\cdot \\vec u_1)\\vec u_1.\\] Definim \\(\\vec u_2=\\frac{1}{\\|\\vec v_2^\\perp\\|}\\vec v_2^\\perp\\), i tenim que \\(V_2=\\langle \\vec u_1, \\vec u_2\\rangle\\), amb \\(\\calc_2=[\\vec u_1,\\vec u_2]\\) una base ortonormal. Suposem ara que ja tenim \\(\\calc_{i-1}=[\\vec u_1, \\dots, \\vec u_{i-1}]\\) una base ortonormal de \\(V_{i-1}\\). Considerem \\(V_i=\\langle\\vec v_1, \\dots, \\vec v_{i-1},\\vec v_i\\rangle=\\langle\\vec u_1, \\dots, \\vec u_{i-1},\\vec v_i\\rangle\\), i escrivim: \\[\\vec v_i = \\vec v_i^\\parallel + \\vec v_i^\\perp\\] amb \\(\\vec v_i^\\parallel\\in V_{i-1}\\) i \\(\\vec v_i^\\perp \\in V_{i-1}^\\perp\\). Com que els vectors \\(\\vec u_i\\) són ortonormals, el càlcul és: \\[\\vec v_i^\\parallel = \\proj_{V_{i-1}}(\\vec v_i)=(\\vec v_i \\cdot \\vec u_1)\\vec u_1+ \\cdots + (\\vec v_i \\cdot \\vec u_{i-1})\\vec u_{i-1}\\] i per tant \\[\\vec v_i^\\perp = \\vec v_i - \\vec v_i^\\parallel = \\vec v_i - (\\vec v_i \\cdot \\vec u_1)\\vec u_1- \\cdots -(\\vec v_i \\cdot \\vec u_{i-1})\\vec u_{i-1}.\\] Definim \\(\\vec u_i=\\frac{1}{\\|\\vec v_i^\\perp\\|}\\vec v_i^\\perp\\), i tenim que \\(V_i=\\langle \\vec u_1, \\dots ,\\vec u_i\\rangle\\), amb \\(\\calc_i=[\\vec u_1,\\dots,\\vec u_i]\\) una base ortonormal. Exemple 4.2 Considerem \\(V=\\langle \\smat{1\\\\-1\\\\-1\\\\1}, \\smat{0\\\\1\\\\1\\\\0}\\rangle \\subset \\R^4\\) i volem calcular una base ortonormal. Comencem amb la base \\(\\calb=[\\smat{1\\\\-1\\\\-1\\\\1}, \\smat{0\\\\1\\\\1\\\\0}]\\) i, seguint el procediment, \\(V_1=\\langle \\smat{1\\\\-1\\\\-1\\\\1}\\rangle\\) i tant sols hem de fer-lo unitari:\\(\\|\\smat{1\\\\-1\\\\-1\\\\1}\\|=2\\) i per tant \\(V_1=\\langle \\smat{1\\\\-1\\\\-1\\\\1}\\rangle=\\langle \\smat{1/2\\\\-1/2\\\\-1/2\\\\1/2}\\rangle\\). Calculem ara el segon vector: \\[\\vec v_2^\\perp = \\smat{0\\\\1\\\\1\\\\0} - (\\smat{0\\\\1\\\\1\\\\0}\\cdot\\smat{1/2\\\\-1/2\\\\-1/2\\\\1/2})\\smat{1/2\\\\-1/2\\\\-1/2\\\\1/2}= \\smat{0\\\\1\\\\1\\\\0}+\\smat{1/2\\\\-1/2\\\\-1/2\\\\1/2}=\\smat{1/2\\\\1/2\\\\1/2\\\\1/2}.\\] Llavors \\(\\vec u_2=\\frac{1}{\\|\\vec v_2^\\perp\\|} \\vec v_2^\\perp=\\smat{1/2\\\\1/2\\\\1/2\\\\1/2}\\). Tenim, doncs, que \\(\\calc=[\\smat{1/2\\\\-1/2\\\\-1/2\\\\1/2},\\smat{1/2\\\\1/2\\\\1/2\\\\1/2}]\\) és una base ortonormal de \\(V\\). A més, la matriu del canvi de base és: \\[S_{\\calb,\\calc}=\\begin{pmatrix} 2 &amp; -1 \\\\ 0 &amp; 1\\end{pmatrix}\\] L’algorisme de Gram-Schmidt ens porta a la factorització \\(QR\\) d’una matriu \\(A\\) (amb les seves columnes linealment independents). Teorema 4.3 Donada una matriu \\(A\\in M_{m\\times n}(\\R)\\) amb les seves columnes linealment independents, existeixen una matriu \\(Q\\in M_{m\\times n}(\\R)\\) i \\(R\\in M_{n\\times n}\\) tals que: \\(A=QR\\), \\(Q^T Q=\\1_n\\) (diem que \\(Q\\) és una matriu ortogonal), \\(R\\) és una matriu triangular superior amb els coeficients de la diagonal positius. A més, aquesta factorització és única. Prova. La demostració és constructiva: anirem calculant els coeficients de les columnes \\(Q\\) i \\(R\\) aplicant el mètode de Gram-Schmidt. Fixem abans les notacions següents per a les columnes d’\\(A\\), de \\(Q\\) i els coeficients de \\(R\\): \\[A=\\begin{pmatrix} | &amp; | &amp; &amp; | \\\\ \\vec v_1 &amp; \\vec v_2 &amp; \\cdots &amp; \\vec v_n \\\\ | &amp; | &amp; &amp; | \\end{pmatrix} , Q=\\begin{pmatrix} | &amp; | &amp; &amp; | \\\\ \\vec u_1 &amp; \\vec u_2 &amp; \\cdots &amp; \\vec u_n \\\\ | &amp; | &amp; &amp; | \\end{pmatrix} \\text{ i } R=\\begin{pmatrix} r_{11} &amp; r_{12} &amp; \\cdots &amp; r_{1n} \\\\ 0 &amp; r_{22} &amp; \\cdots &amp; r_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; r_{nn}\\end{pmatrix}.\\] I les matrius formades per les primeres \\(j\\) columnes d’\\(A\\), \\(Q\\) i la submatriu quadrada \\(j\\times j\\) d’\\(R\\): \\[A_j=\\begin{pmatrix} | &amp; | &amp; &amp; | \\\\ \\vec v_1 &amp; \\vec v_2 &amp; \\cdots &amp; \\vec v_j\\\\ | &amp; | &amp; &amp; | \\end{pmatrix} , Q_j=\\begin{pmatrix} | &amp; | &amp; &amp; | \\\\ \\vec u_1 &amp; \\vec u_2 &amp; \\cdots &amp; \\vec u_j \\\\ | &amp; | &amp; &amp; | \\end{pmatrix} \\text{ i } R_j=\\begin{pmatrix} r_{11} &amp; r_{12} &amp; \\cdots &amp; r_{1j} \\\\ 0 &amp; r_{22} &amp; \\cdots &amp; r_{2j} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; r_{jj}\\end{pmatrix}.\\] La construcció comença amb \\(j=1\\), \\(j=2\\) i veurem com es fa el cas \\(j\\) a partir del \\(j-1\\): Cas \\(j=1\\): llavors \\(A_1=\\vec v_1\\): calculem el coeficient \\(R_1=(r_{11})=(\\|\\vec v_1\\|)\\) i la primera columna de \\(Q\\) com \\(Q_1=\\vec u_1=\\frac{1}{r_{11}}\\vec v_1\\). Veiem que, de moment, es compleix \\(A_1=Q_1R_1\\). A més, \\(Q_1^TQ_1=\\|\\vec u_1\\|=1\\) i \\(R_1\\) és triangular superior i amb la diagonal positiva (és un mòdul). Cas \\(j=2\\): considerem \\(r_{12}=\\vec u_1\\cdot \\vec v_2\\), el vector \\(\\vec v_2^\\perp=\\vec v_2-r_{12}u_1\\), \\(r_{22}=\\|\\vec v_2^\\perp\\|\\) i \\(\\vec u_2=\\frac{1}{r_{22}} v_2^\\perp\\). La primera columna de \\(Q_2R_2\\) és la mateixa que la de \\(Q_1R_1=A_1\\). La segona és \\[r_{12}u_1+r_{22}\\vec u_2=(u_1\\cdot v_2)\\vec u_1+\\|v_2-(\\vec u_1\\cdot \\vec v_2)\\vec u_1\\|\\frac{1}{\\|\\vec v_2-(\\vec u_1\\cdot \\vec v_2)\\vec u_1\\|}(\\vec v_2-(\\vec u_1\\cdot \\vec v_2)\\vec u_1)=\\vec v_2\\] i per tant tenim la segona columna de \\(A\\), llavors \\(A_2=Q_2R_2\\). També veiem que \\(Q_2^TQ_2=\\1_2\\) (\\(\\vec u_2\\) és perpendicular a \\(\\vec u_1\\) i tots dos són unitaris) i \\(R_2\\) és triangular superior i la diagonal positiva (són mòduls de vectors). Suposem que tenim les \\(j-1\\) primeres columnes de \\(Q\\), i les de \\(R\\) tal que \\(A_{j-1}=Q_{j-1}R_{j-1}\\), \\(Q_{j-1}^TQ_{j-1}=\\1_{j-1}\\) i \\(R_{j-1}\\) triangular superior i amb la diagonal positiva. Considerem \\(r_{ij}=\\vec u_i\\cdot \\vec v_j\\) per a \\(1\\leq i &lt;j\\), \\(\\vec v_j^\\perp=\\vec v_j-r_{1j}\\vec u_1-\\cdots-r_{(j-1)j}\\vec u_{j-1}\\), \\(r_{jj}=\\|\\vec v_j^\\perp\\|\\) i \\(\\vec u_j=\\frac{1}{r_{jj}} \\vec v_j^\\perp\\). Si fem \\(Q_jR_j\\), les primeres \\(j-1\\) columnes són les de \\(Q_{j-1}R_{j-1}\\), per tant són \\(A_{j-1}\\). Si calculem la columna \\(j\\)-èssima de \\(Q_jR_j\\): \\[\\begin{align*} r_{1j}\\vec u_1+\\cdots+r_{jj}\\vec u_j &amp; =(\\vec u_1\\cdot \\vec v_j)\\vec u_1+\\cdots+(\\vec u_{j-1}\\cdot \\vec v_j)\\vec u_{j-1}+\\|\\vec v_j^\\perp\\|\\frac{1}{\\|\\vec v_j^\\perp\\|} \\vec v_j^\\perp = \\\\ &amp; =(\\vec u_1\\cdot \\vec v_j)\\vec u_1+\\cdots+(\\vec u_{j-1}\\cdot \\vec v_j)\\vec u_{j-1}+ \\vec v_j^\\perp=\\vec v_j . \\end{align*}\\] I per tant \\(A_j=Q_jR_j\\). A més, la primera caixa \\((j-1)\\times(j-1)\\) superior esquerra de \\(Q_j^TQ_j\\) és una \\(\\1_{j-1}\\). L’última columna i fila són és \\(\\vec u_i\\cdot \\vec u_j\\), i per tant, com que \\(\\vec u_j\\) és perpendicular als altres \\(\\vec u_i\\) i unitari, tenim que \\(Q_j^TQ_j=\\1_j\\). Finalment, \\(R_j\\) és triangular superior per construcció, i la diagonal formada per mòduls, per tant positius. Falta veure la unicitat, també per inducció sobre \\(j\\): Cas \\(j=1\\): com que \\(R\\) és triangular superior, \\(v_1=r_{11}\\vec u_1\\) amb \\(\\|\\vec u_1\\|=1\\), per tant \\(\\vec u_1=\\frac{\\pm1}{\\|\\vec v_1\\|}v_1\\), però com que \\(r_{11}\\) és positiva positiva, \\(\\vec u_1=\\frac{1}{\\|\\vec v_1\\|}\\vec v_1\\) i \\(r_{11}=\\|\\vec v_1\\|\\). Suposem que les \\(j-1\\) primeres columnes de \\(Q\\) i de \\(R\\) ja estan fixades, volem veure que tant sols hi ha una elecció per la \\(j\\)-èssima: el vector \\(\\vec u_j\\) és un vector perpendicular a l’espai \\(V_{j-1}=\\langle \\vec v_1, \\dots, \\vec v_{j-1}\\rangle\\) i contingut a \\(V_j=\\langle \\vec v_1, \\dots, \\vec v_j\\rangle\\). Considerem \\(V_{j-1}^\\perp\\) el complement ortogonal de \\(V_{j-1}\\) a \\(V_j\\), que és de dimensió \\(1\\) (\\(\\dim(V_{j-1})+\\dim(V_{j-1}^\\perp)=\\dim(V_j)\\)), i conté \\(\\vec v_j^\\perp\\), per tant es té \\(V_{j-1}^\\perp=\\langle v_j^\\perp \\rangle\\). Llavors, com que ha de ser unitari, ja \\(j\\)-èssima columna de \\(Q\\) ha de ser \\(\\vec u_j=\\frac{\\pm1}{\\|\\vec v_j^\\perp\\|}\\vec v_j^\\perp\\), però com que \\(r_{jj}\\) és positiva positiva, \\(\\vec u_j=\\frac{1}{\\|\\vec v_j^\\perp\\|}\\vec v_j^\\perp\\) i \\(r_{jj}=\\|\\vec v_j^\\perp\\|\\), i tenim la unicitat de la \\(\\vec u_j\\) i el coeficient \\(r_{jj}\\). La resta de coeficients de la columna \\(j\\) d’\\(R\\) estan determinats per ser \\(R\\) triangular superior (per tant, per sota de \\(r_{jj}\\) són zero) i el fet de que els de sobre són les coordenades de \\(\\vec v_j-\\vec v_j^\\perp\\) en la base \\([\\vec u_1,\\dots,\\vec u_{j-1}]\\) de \\(V_{j-1}\\) (i les coordenades en una base són úniques pel Teorema 2.2). Exemple 4.3 Podem aprofitar els càlculs fets a l’Exemple 4.2 per trobar la factorització \\(QR\\) següent: \\[\\begin{pmatrix} 1 &amp; 0 \\\\ -1 &amp; 1 \\\\ -1 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix}= \\begin{pmatrix} 1/2 &amp; 1/2 \\\\ -1/2 &amp; 1/2 \\\\ -1/2 &amp; 1/2 \\\\ 1/2 &amp; 1/2 \\end{pmatrix} \\begin{pmatrix} 2 &amp; -1 \\\\ 0 &amp; 1 \\end{pmatrix}.\\] 4.4 Aplicacions i matrius ortogonals Definició 4.4 Diem que una aplicació lineal \\(f \\colon \\R^n\\to \\R^m\\) és ortogonal si conserva la longitud dels vectors, o sigui, si \\[\\|f(\\vec v)\\|=\\|\\vec v\\| \\text{ per a tot $\\vec v\\in\\R^n$.}\\] Si \\(f=f_A\\) amb \\(A\\in M_{m\\times n}(\\R)\\), direm que \\(A\\) és una matriu ortogonal. Exemple 4.4 Les rotacions definides a la Secció 2.2.4 són ortogonals: es pot interpretar geomètricament o bé fent els càlculs: un vector \\(\\smat{x\\\\y}\\) té norma \\[\\|\\begin{pmatrix}x \\\\ y \\end{pmatrix}\\|=\\left(\\begin{pmatrix} x &amp; y \\end{pmatrix}\\begin{pmatrix} x \\\\ y \\end{pmatrix}\\right)^{1/2} = \\sqrt{x^2+y^2},\\] i va a parar a: \\[\\begin{pmatrix} \\cos\\theta&amp;-\\sin\\theta\\\\ \\sin\\theta&amp;\\cos\\theta \\end{pmatrix} \\begin{pmatrix}x \\\\ y \\end{pmatrix} ,\\] que té norma \\[\\begin{align*} \\|f(\\smat{x\\\\y})\\|^2 &amp; =\\left( \\begin{pmatrix} \\cos\\theta&amp;-\\sin\\theta\\\\ \\sin\\theta&amp;\\cos\\theta \\end{pmatrix} \\begin{pmatrix}x \\\\ y \\end{pmatrix} \\right)^T \\begin{pmatrix} \\cos\\theta&amp;-\\sin\\theta\\\\ \\sin\\theta&amp;\\cos\\theta \\end{pmatrix} \\begin{pmatrix}x \\\\ y \\end{pmatrix} = \\\\ &amp; = \\begin{pmatrix}x &amp; y \\end{pmatrix} \\begin{pmatrix} \\cos\\theta&amp;\\sin\\theta\\\\ -\\sin\\theta&amp;\\cos\\theta \\end{pmatrix} \\begin{pmatrix} \\cos\\theta&amp;-\\sin\\theta\\\\ \\sin\\theta&amp;\\cos\\theta \\end{pmatrix} \\begin{pmatrix}x \\\\ y \\end{pmatrix} = \\\\ &amp; = \\begin{pmatrix}x &amp; y \\end{pmatrix} \\begin{pmatrix} 1&amp;0\\\\ 0&amp;1 \\end{pmatrix} \\begin{pmatrix}x \\\\ y \\end{pmatrix} = x^2+y^2\\\\ \\end{align*}\\] i per tant la rotació és ortogonal. Exercici 4.1 Demostreu que reflexions definides a la Secció 2.2.3 són ortogonals. Exemple 4.5 Les projeccions a un subespai \\(V\\subset \\R^n\\) (\\(V\\neq \\R^n)\\) no són ortogonals: considerem \\(\\vec 0 \\neq \\vec u \\in V^\\perp\\), llavors \\(\\proj_V(\\vec u)=\\vec 0\\) i per tant no es conserva la longitud de \\(\\vec u\\). El lema següent ens permet calcular el producte escalar en funció dels mòduls: Lema 4.2 Si \\(\\vec u, \\vec v\\) són vectors d’\\(\\R^n\\), llavors: \\[\\vec u \\cdot \\vec v = \\frac{1}{2}\\left(\\|\\vec u + \\vec v\\|^2 - \\|\\vec u\\|^2 - \\|\\vec v\\|^2\\right).\\] Prova. Considerem \\(\\vec u, \\vec v \\in \\R^n\\). Tenim que: \\[\\begin{align*} \\|\\vec u + \\vec v\\|^2 &amp; =(\\vec u+\\vec v)\\cdot(\\vec u+\\vec v) = \\vec u \\cdot (\\vec u+ \\vec v) + \\vec v \\cdot (\\vec u+ \\vec v) = \\\\ &amp; = \\vec u \\cdot \\vec u + \\vec u \\cdot \\vec v + \\vec v \\cdot \\vec u+\\vec v \\cdot \\vec v = \\|\\vec u\\|^2+2 (\\vec u \\cdot \\vec v) + \\|\\vec v\\|^2 \\end{align*}\\] I per tant: \\[\\vec u \\cdot \\vec v = \\frac{1}{2}\\left(\\|\\vec u + \\vec v\\|^2 - \\|\\vec u\\|^2 - \\|\\vec v\\|^2\\right).\\] I d’aquí deduïm: Teorema 4.4 Una aplicació lineal \\(f\\colon \\R^n \\to \\R^m\\) és ortogonal si i només si \\(f(\\vec u)\\cdot f(\\vec v)=\\vec u \\cdot \\vec v\\) per a tot \\(\\vec u\\) i \\(\\vec v\\) de \\(\\R^n\\). Prova. Si l’aplicació és ortogonal, es compleix que, per a tot \\(\\vec u,\\vec v \\in \\R^n\\), \\(\\|f(\\vec u)\\|=\\|\\vec u\\|\\) i \\(\\|f(\\vec v)\\|=\\|\\vec v\\|\\). Llavors: \\[\\begin{align*} f(\\vec u)\\cdot f\\vec v) &amp; = \\frac{1}{2}\\left(\\|f(\\vec u) +f(\\vec v)\\|^2 - \\|f(\\vec u)\\|^2 - \\|f(\\vec v)\\|^2\\right) \\\\ &amp; = \\frac{1}{2}\\left(\\|f(\\vec u+\\vec v)\\|^2 - \\|f(\\vec u)\\|^2 - \\|f(\\vec v)\\|^2\\right) \\\\ &amp; = \\frac{1}{2}\\left(\\|\\vec u + \\vec v\\|^2 - \\|\\vec u\\|^2 - \\|\\vec v\\|^2\\right) = \\vec u \\cdot \\vec v , \\end{align*}\\] on hem utilitzat el Lema 4.2 dues vegades i que \\(f\\) és lineal. Si \\(f(\\vec u)\\cdot f(\\vec v)=\\vec u \\cdot \\vec v\\) per a tot \\(\\vec u\\) i \\(\\vec v\\) de \\(\\R^n\\), com que \\(\\|\\vec u\\|^2=\\vec u \\cdot \\vec u\\), tenim que, agafant \\(\\vec u=\\vec v\\), \\(\\|f(\\vec u)\\|^2=\\|\\vec u\\|^2\\) i per tant \\(f\\) és ortogonal. Corol·lary 4.3 Si \\(f = f_A \\colon \\R^n \\to \\R^m\\) és una aplicació lineal i \\(\\vec e_1, \\dots, \\vec e_n\\) és la base estàndard d’\\(\\R^n\\), llavors: \\(f\\) és ortogonal si i només si \\(f(\\vec e_1), \\dots, f(\\vec e_n)\\) són vectors ortonormals. En particular, si \\(f\\) és ortogonal, \\(f(\\vec e_1), \\dots, f(\\vec e_n)\\) són linealment independents i si \\(n=m\\), són una base ortonormal. \\(f_A\\) és una aplicació lineal ortogonal si i només si \\(A\\) és una matriu ortogonal. Prova. Vegem primer (a). Si \\(f\\) és ortogonal, del fet que \\(f(\\vec e_i)\\cdot f(\\vec e_j)=\\vec e_i\\cdot \\vec e_j\\), es dedueix que \\(f(\\vec e_i)\\) són unitaris i ortogonals dos a dos, per tant, ortonormals. Suposem ara que \\(f(\\vec e_i)\\) són unitaris i ortogonals dos a dos i unitaris i \\(\\vec u \\in \\R^n\\). Tenim que existeixen nombres reals \\(\\lambda_i\\) tals que \\(\\vec u= \\lambda_1\\vec e_1+ \\cdots + \\lambda_n\\vec e_n\\). Llavors \\(\\|u\\|^2=\\lambda_1^2+ \\cdots + \\lambda_n^2\\), i \\(\\|f(\\vec u)\\|^2=\\|\\lambda_1 f(\\vec e_1)+\\cdots + \\lambda_n f(\\vec e_n)\\|^2=\\lambda_1^2+ \\cdots + \\lambda_n^2\\) (pel Teorema de Pitàgores, utilitzant que són ortogonals i unitaris). Vegem ara que els \\(f(\\vec e_1)\\) són linealment independents: més en general, tenim que si \\(\\vec u_1, \\dots, \\vec u_k\\) són vectors ortonormals, llavors són linealment independents: del teorema de Pitàgores es dedueix que: \\[\\|\\lambda_1\\vec u_1 + \\cdots + \\lambda_k \\vec u_k\\|^2=\\lambda_1^2 + \\cdots + \\lambda_k^2\\] I per tant si tenim una combinació \\(\\vec 0 = \\lambda_1\\vec u_1 + \\cdots + \\lambda_k \\vec u_k\\), tots els \\(\\lambda\\) han de ser zero. Llavors tenim que, en el nostre cas, \\(f(\\vec e_1), \\dots, f(\\vec e_n)\\) són linealment independents. Si \\(n=m\\), \\(f(\\vec e_1)\\), …, \\(f(\\vec e_m)\\) són \\(m\\) vectors de \\(\\R^m\\) linealment independents, i per tant, base. Per a veure (b), si \\(f_A\\) és ortogonal i fem el producte \\(A^TA\\), tenim que el coeficient \\((i,j)\\) és \\(f(\\vec e_i)\\cdot f(\\vec e_j)\\), per tant, com que són ortonormals, tenim \\(A^TA=\\1_n\\). Les columnes d’\\(A\\) són els vectors \\(f(\\vec e_j)\\), per tant, \\(A^TA=\\1_n\\) si i només si \\(f(\\vec e_1), \\dots, f(\\vec e_n)\\) són ortogonals i unitaris (per tant \\(f\\) és ortogonal). Teorema 4.5 Si \\(A,B \\in M_n(\\R)\\) són matrius ortogonals, llavors: \\(AB\\) també és ortogonal i \\(A^{-1}\\) és ortogonal (i \\(A^{-1}=A^T)\\). Prova. \\(AB\\) serà ortogonal si \\(f_{AB}\\) conserva la longitud dels vectors, però \\(f_{AB}=f_A \\circ f_B\\) i, per hipòtesi, tant \\(f_B\\) com \\(f_A\\) conserven la longitud, per tant, \\(f_{AB}\\) també. Per a veure (b): si \\(\\vec v = f_{A^{-1}}(\\vec w))\\), llavors \\(\\vec w=f_A(\\vec v)\\) i, per hipòtesi \\(\\|\\vec w\\|=\\|f_A(\\vec v)\\|=\\|\\vec v\\|\\), per tant \\(A^{-1}\\) és ortogonal. 4.5 Matriu d’una projecció ortogonal en una base ortonormal Proposició 4.2 Si \\(V\\subset \\R^n\\) un subespai vectorial, \\([\\vec v_1, \\dots, \\vec v_k]\\) és una base ortonormal de \\(V\\) i \\(Q\\) és la matriu que té per columnes els vectors \\(\\vec v_j\\), llavors, la matriu de la projecció ortogonal a \\(V\\) és: \\[[\\proj_V]=Q Q^T.\\] Prova. Com que \\([\\vec v_1, \\dots, \\vec v_k]\\) és una base ortonormal de \\(V\\), la projecció ortogonal a \\(V\\) del vector \\(\\vec u\\) és: \\[\\proj_V(\\vec u)=(\\vec u\\cdot \\vec v_1)\\vec v_1 + \\cdots + (\\vec u\\cdot \\vec v_k)\\vec v_k = Q \\begin{pmatrix} \\vec u\\cdot \\vec v_1 \\\\ \\vdots \\\\ \\vec u\\cdot \\vec v_k \\end{pmatrix}= Q \\begin{pmatrix} \\vec v_1^T \\vec u \\\\ \\vdots \\\\ \\vec v_k^T \\vec u \\end{pmatrix} = Q \\begin{pmatrix} \\vec v_1^T \\\\ \\vdots \\\\ \\vec v_k^T \\end{pmatrix} \\vec u =(QQ^T)\\vec u\\] I per tant la matriu associada a la projecció és \\(QQ^T\\). Exemple 4.6 Considerem el vector \\(\\smat{x\\\\y}\\neq\\smat{0\\\\0}\\) de \\(\\R^2\\) i calculem la matriu de la projecció a \\(V=\\langle \\smat{x\\\\y}\\rangle\\). Una base ortonormal és \\(\\vec v_1=\\frac{1}{\\sqrt{x^2+y^2}}\\smat{x\\\\y}\\) i per tant la matriu és (compareu-ho amb la Proposició 2.3): \\[[\\proj_V]=\\frac{1}{\\sqrt{x^2+y^2}} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\frac{1}{\\sqrt{x^2+y^2}} \\begin{pmatrix} x &amp; y \\end{pmatrix} = \\frac{1}{x^2+y^2} \\begin{pmatrix} x^2 &amp; xy \\\\ xy &amp; y^2 \\end{pmatrix} .\\] Exemple 4.7 Volem calcular la matriu de la projecció ortogonal de \\(\\R^4\\) a \\(V=\\langle \\smat{1\\\\-1\\\\-1\\\\1}, \\smat{0\\\\1\\\\1\\\\0}\\rangle \\subset \\R^4\\). A l’Exemple 4.2 ja hem calcular una base ortonormal, i era \\(\\calc=[\\smat{1/2\\\\-1/2\\\\-1/2\\\\1/2},\\smat{1/2\\\\1/2\\\\1/2\\\\1/2}]\\), per tant la matriu corresponent a la projecció ortogonal és: \\[\\begin{pmatrix} 1/2 &amp; 1/2 \\\\ -1/2 &amp; 1/2 \\\\ -1/2 &amp; 1/2 \\\\ 1/2 &amp; 1/2 \\end{pmatrix} \\begin{pmatrix} 1/2 &amp; -1/2 &amp; -1/2 &amp; 1/2 \\\\ 1/2 &amp; 1/2 &amp; 1/2 &amp; 1/2 \\end{pmatrix} \\begin{pmatrix} 1/2 &amp; 0 &amp; 0 &amp; 1/2 \\\\ 0 &amp; 1/2 &amp; 1/2 &amp; 0 \\\\ 0 &amp; 1/2 &amp; 1/2 &amp; 0 \\\\ 1/2 &amp; 0 &amp; 0 &amp; 1/2 \\end{pmatrix}.\\] 4.6 Mínims quadrats En aquesta secció suposarem que tenim un subespai \\(V\\subset\\R^n\\) fixat. A cada element d’\\(\\R^n\\), volem assignar-li un element de \\(V\\) que “millor l’aproxima”, en el sentit que la norma de l’error que produïm és mínima. Proposició 4.3 Si \\(V \\subset \\R^n\\) és un subespai, llavors, per a tot \\(\\vec w \\in \\R^n\\) i \\(\\vec v\\in V\\) es compleix: \\[\\|\\vec w - \\vec v\\| \\geq \\|\\vec w - \\proj_V(\\vec w)\\|.\\] Prova. Tenim que \\(\\vec w - \\vec v = (\\vec w - \\proj_V(w)) + (\\proj_V(w) - \\vec v)\\) amb el primer vector de \\(\\vec V^\\perp\\) i el segon de \\(V\\). Aplicant el Teorema de Pitàgores tenim: \\[\\|\\vec w - \\vec v\\|^2= \\|\\vec w - \\proj_V(\\vec w)\\|^2 + \\|\\proj_V(\\vec w) - \\vec v\\|^2 \\geq \\|\\vec w - \\proj_V(\\vec w)\\|^2 ,\\] i tenim la desigualtat que volem. Per tant la millor aproximació d’un vector \\(\\vec w\\) és la projecció ortogonal de \\(\\vec w\\), si entenem per “millor” aquell vector de \\(V\\) que està a menor distància del vector \\(\\vec w\\). 4.6.1 Recta de regressió Suposem que tenim un núvol de punts \\((x_1,y_1), \\dots , (x_k,y_k)\\) de \\(\\R^2\\) que, en principi, no estan alineats. Suposem, a més, que hi ha una dependència del tipus \\(y=f(x)\\) (per tant, al núvol de punts seria un problema que hi hagués dos punts amb la mateixa \\(x\\) i diferents \\(y\\)). Suposem que els punts estan aproximadament alineats i que el que volem trobar és la recta \\(y=a_0+a_1x\\) que aproxima millor aquests punts. Una altra manera de pensar-ho és que volem resoldre el sistema d’equacions amb incògnites \\(a_0\\) i \\(a_1\\): \\[\\begin{align*} a_ 0 + x_1 a_1 &amp; = y_1 \\\\ &amp; \\vdots \\\\ a_0 + x_k a_1 &amp; = y_k \\end{align*}\\] O, en forma matricial: \\[\\begin{pmatrix} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_k \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix} = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_k \\end{pmatrix}\\] Com que molt probablement els punts no estan alineats, com a sistema d’equacions, és un sistema incompatible. Dit d’una altra manera, el vector \\(\\vec y=\\smat{y_1 \\\\ \\vdots \\\\ y_k}\\) no pertany al subespai \\(V=\\langle \\smat{1 \\\\ \\vdots \\\\ 1} , \\smat{x_1 \\\\ \\vdots \\\\ x_k} \\rangle\\). Si apliquem la Proposició 4.3, el vector de \\(V\\) que serà més proper a \\(\\vec y\\) serà \\(\\proj_V(\\vec y)\\): \\[\\proj_V(\\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_k \\end{pmatrix})= a_0 \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} + a_1 \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_k \\end{pmatrix}= \\begin{pmatrix} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_k \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix} .\\] Podríem calcular \\(a_0\\) i \\(a_1\\) fent primer Gram-Schmidt per a trobar una base ortonormal de \\(V\\) i fent els productes escalars corresponents, o bé fent les consideracions següents: el vector \\[\\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_k \\end{pmatrix} - \\begin{pmatrix} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_k \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix}\\] és perpendicular a \\(V\\), per tant ha de complir que: \\[\\begin{pmatrix} 1 &amp; \\cdots &amp; 1 &amp; \\\\ x_1 &amp; \\cdots &amp; x_k \\end{pmatrix} \\left( \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_k \\end{pmatrix} - \\begin{pmatrix} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_k \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix}\\right) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\] I per tant: \\[\\begin{pmatrix} 1 &amp; \\cdots &amp; 1 &amp; \\\\ x_1 &amp; \\cdots &amp; x_k \\end{pmatrix} \\begin{pmatrix} 1 &amp; x_1 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_k \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; \\dots &amp; 1 &amp; \\\\ x_1 &amp; \\dots &amp; x_k \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_k \\end{pmatrix}\\] I queda el sistema compatible determinat: \\[\\begin{pmatrix} k &amp; \\sum x_i \\\\ \\sum x_i &amp; \\sum x_i^2 \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix} = \\begin{pmatrix} \\sum y_i \\\\ \\sum x_iy_i \\end{pmatrix}\\] Que es pot escriure: si \\(\\overline x = \\frac{1}{k} \\sum x_i\\), \\(\\overline y=\\frac{1}{k} \\sum y_i\\), \\(\\overline {x^2}= \\frac{1}{k} \\sum x_i^2\\) i \\(\\overline {xy}= \\frac{1}{k} \\sum x_iy_i\\): \\[\\begin{pmatrix} 1 &amp; \\overline x \\\\ \\overline x &amp; \\overline{x^2} \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix} = \\begin{pmatrix} \\overline y \\\\ \\overline{xy} \\end{pmatrix}\\] I la solució es pot escriure com: \\[\\begin{align*} (\\#eq:recta-reg) a_1=\\frac{\\overline{xy} - \\overline x \\, \\overline y}{\\overline{x^2} - (\\overline x)^2} \\text{ i } a_0= \\overline y - a_1 \\overline x . \\end{align*}\\] Exemple 4.8 Considerem les notes següents d’un grup de \\(21\\) alumnes corresponent a una avaluació parcial i la nota final que van treure: \\[\\begin{array}{|c|c||c|c||c|c|} \\hline\\text{Parcial} &amp; \\text{Final} &amp; \\text{Parcial} &amp; \\text{Final} &amp; \\text{Parcial} &amp; \\text{Final} \\\\ \\hline 8.15 &amp; 8.995 &amp; 6.25 &amp; 7.845 &amp; 2.2 &amp; 4.58 \\\\ 0.3 &amp; 1.98 &amp; 5.85 &amp; 7.075 &amp; 4.6 &amp;4.95 \\\\ 4.3&amp;5.01 &amp; 3.55 &amp; 5.715 &amp; 4.7 &amp; 7.08\\\\ 1.8&amp;2.88 &amp; 1.7 &amp; 2.82 &amp; 2.3 &amp; 3.86\\\\ 5.4&amp;5.08 &amp; 4.6 &amp; 7.13 &amp; 2.7 &amp;5.33 \\\\ 0.2 &amp;2.78 &amp; 7.7 &amp; 8.8 &amp; 7.95 &amp;8.595\\\\ 5.4 &amp; 7.77 &amp; 3.15 &amp;1.245 &amp; 3 &amp;4.95\\\\ \\hline \\end{array}\\] I volem aproximar la nota final a partir de la del parcial amb una recta: \\[\\text{final}= a_0 + a_1 \\text{parcial}\\] Per tant, podem aplicar la Fórmula (??) als valors (aproximem a 3 decimals): \\[\\overline{x}=\\overline{\\text{parcial}}= 4.085, \\quad \\overline{y}=\\overline{\\text{final}}= 5.451, \\quad \\overline{x^2}=21,839 \\text{ i } \\overline{xy}= 26,816.\\] I queda (amb 3 decimals): \\[\\text{final}= 1.843 + 0.883 \\text{parcial}.\\] La Figura 1 té una representació gràfica d’aquestes dades i de la recta de regressió. A més, l’equació de la recta es pot utilitzar per aproximar una nota final a partir d’una parcial: per exemple, un alumne que hagi tret un \\(4\\) al parcial, la predicció diria que trauria un \\(5,376\\) a la nota final. Recta de regressió 4.6.2 Cas general En el cas general, suposem que tenim un sistema d’equacions lineals: \\[A x = b\\] amb \\(A\\in M_{m\\times n}(\\R)\\) amb \\(m\\geq n\\) (més equacions que incògnites) i tal que \\(\\Rang(A)=n\\). És molt probable que aquest sistema no tingui solució, i per aquests casos ens interessa tenir la millor aproximació: Lema 4.3 Amb les hipòtesis anteriors, el sistema \\[A^T A x = A^T b\\] té solució única. Aquesta solució s’anomena solució per mínim quadrats del sistema \\(Ax=b\\). Prova. Observem que \\(A^TA \\in M_{n\\times n}(\\R)\\), pel que tant sols cal veure que \\(\\Rang(A^T A)=n\\) (i llavors serà un sistema compatible determinat), o el que és el mateix, que l’aplicació lineal \\(f_{A^TA}\\) és injectiva (llavors el sistema homogeni que té per matriu associada \\(A^TA\\) serà compatible determinat i per tant \\(\\Rang(A^T A)=n\\)). Ara bé, si \\(\\vec u \\in \\R^n\\) compleix \\(A^TA\\vec u = \\vec 0\\), tenim que el vector \\(\\vec v=A\\vec u\\) pertany a \\(\\ker A^T=(\\Ima A)^\\perp\\) i també a \\(\\Ima A\\). Com que \\((\\Ima A)\\cap (\\Ima A)^\\perp=\\{\\vec 0\\}\\), deduïm que \\(\\vec v = \\vec 0\\). Aplicant ara que \\(\\Rang(A)=n\\), tenim que \\(\\vec u=\\vec 0\\) i per tant \\(\\Rang(A^TA)=n\\). Lema 4.4 Continuem amb les hipòtesis anteriors. De tots els vectors \\(\\vec v \\in \\R^n\\), la solució per mínims quadrats del sistema \\(Ax=b\\) és la que és més propera a \\(b\\) en el sentit següent: si \\(x^*\\) és la solució del sistema \\(A^TAx^*=A^Tb\\) i \\(\\vec v\\in \\R^n\\), llavors \\[\\|Ax^* - b\\|^2 \\leq \\|A\\vec v- b\\|^2.\\] Prova. Es dedueix del fet que la solució per mínims quadrats sigui la projecció ortogonal de \\(b\\) a l’espai generat per les columnes d’\\(A\\), que, com que el rang d’\\(A\\) és \\(n\\), queda caracteritzada per complir: \\[A^T(Ax-b)=\\vec 0 .\\] D’aquí també es dedueix que la projecció del vector \\(b\\) a \\(V\\), el subespai generat per les columnes d’\\(A\\) és, en la base de \\(V\\) formada per les columnes d’\\(A\\) és: \\[x=(A^TA)^{-1}A^Tb\\] Per tant, el vector d’\\(\\R^n\\) és: \\[Ax=A(A^TA)^{-1}A^Tb\\] I per tant: Proposició 4.4 Si \\(V\\subset \\R^n\\) és un subespai vectorial i \\([\\vec v_1, \\dots , \\vec v_k]\\) és una base de \\(V\\), la projecció de \\(\\R^n\\) a \\(V\\) ve donada per la matriu: \\[[\\proj_V]=A(A^TA)^{-1}A^T\\] on \\(A\\) és la matriu que té per columnes els vectors \\(\\vec v_j\\). Exemple 4.9 Suposem que tenim un grup de 24 estudiants als que s’ha fet una avaluació parcial i un lliurament a mig semestre amb les notes següents. La tercera columna diu quina ha sigut la nota final de l’assignatura (amb més avaluacions entremig): \\[\\begin{array}{|c|c|c||c|c|c|} \\hline \\text{Lliurament} &amp; \\text{Parcial} &amp; \\text{Final} &amp; \\text{Lliurament} &amp; \\text{Parcial} &amp; \\text{Final}\\\\ \\hline 9.25 &amp; 9.75 &amp; 8.25 &amp; 9.5 &amp; 5.2 &amp; 7.27 \\\\ 9.5 &amp; 2.4 &amp; 3.66 &amp; 9 &amp; 7.1 &amp; 8.32 \\\\ 8 &amp; 1.6 &amp; 5.65 &amp; 8 &amp; 4 &amp; 7.61 \\\\ 8 &amp; 7.1 &amp; 6.71 &amp; 8.5 &amp; 3.4 &amp; 6.77 \\\\ 8.5 &amp; 5.7 &amp; 7.77 &amp; 8 &amp; 6 &amp; 7.74 \\\\ 6 &amp; 2.9 &amp; 6.5 &amp; 10 &amp; 5.9 &amp; 8.37 \\\\ 9 &amp; 9 &amp; 8.13 &amp; 10 &amp; 2.6 &amp; 8.13 \\\\ 8 &amp; 7.6 &amp; 7.15 &amp; 7.5 &amp; 1.6 &amp; 5.21 \\\\ 9 &amp; 6.2 &amp; 6.8 &amp; 10 &amp; 9 &amp; 8.52 \\\\ 10 &amp; 10 &amp; 9.07 &amp; 9 &amp; 8 &amp; 7.76 \\\\ 9 &amp; 6.5 &amp; 7.43 &amp; 9.75 &amp; 10 &amp; 8.71 \\\\ 8.5 &amp; 4 &amp; 6.39 &amp; 9.5 &amp; 7.4 &amp; 8.57 \\\\ \\hline \\end{array}\\] Volem aproximar la nota final a partir de les dues que se saben a mig semestre mitjançant una formula: \\[\\text{final} = a \\times \\text{Lliurament} + b \\times \\text{Parcial} + c\\] I per tant, fem mínim quadrats per trobar \\(a,b\\) i \\(c\\). En aquest cas, el sistema que té moltes més equacions que incògnites seria: \\[\\begin{pmatrix} 9.25 &amp; 9.75 &amp; 1 \\\\ 9.5 &amp; 2.4 &amp; 1 \\\\ 8 &amp; 1.6 &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 9.5 &amp; 7.4 &amp; 1 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix}= \\begin{pmatrix} 8.25 \\\\ 3.66 \\\\ 5.65 \\\\ \\vdots \\\\ 8.57 \\end{pmatrix}\\] I per tant hem de resoldre el sistema compatible determinat: \\[\\begin{pmatrix} 9.25 &amp; 9.5 &amp; 8 &amp; \\cdots &amp; 9.5 \\\\ 9.75 &amp; 2.4 &amp; 1.6 &amp; \\cdots &amp; 7.4 \\\\ 1 &amp; 1 &amp; 1 &amp; \\cdots &amp; 1 \\end{pmatrix} \\begin{pmatrix} 9.25 &amp; 9.75 &amp; 1 \\\\ 9.5 &amp; 2.4 &amp; 1 \\\\ 8 &amp; 1.6 &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 9.5 &amp; 7.4 &amp; 1 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix}= \\begin{pmatrix} 9.25 &amp; 9.5 &amp; 8 &amp; \\cdots &amp; 9.5 \\\\ 9.75 &amp; 2.4 &amp; 1.6 &amp; \\cdots &amp; 7.4 \\\\ 1 &amp; 1 &amp; 1 &amp; \\cdots &amp; 1 \\end{pmatrix} \\begin{pmatrix} 8.25 \\\\ 3.66 \\\\ 5.65 \\\\ \\vdots \\\\ 8.57 \\end{pmatrix}\\] Quedant: \\[\\left(\\begin{array}{ccc} 1885.375 &amp; 1287.5375 &amp; 211.5 \\\\ 1287.5375 &amp; 1015.0425 &amp; 142.95 \\\\ 211.5 &amp; 142.95 &amp; 24 \\end{array}\\right) \\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix}= \\begin{pmatrix} 1568.205 \\\\ 1108.1755 \\\\ 176.49 \\end{pmatrix}\\] Que té per solució (amb 3 decimals): \\[\\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix}= \\begin{pmatrix} 0.191 \\\\ 0.316 \\\\ 3.790 \\end{pmatrix}\\] Per tant l’aproximació és: \\[\\text{final} = 0.191 \\times \\text{Lliurament} + 0.316 \\times \\text{Parcial} + 3.79\\, .\\] Per exemple, d’un alumne que tregui un \\(4\\) al lliurament i un altre \\(4\\) al parcial, aquest model prediu que al final trauria un \\(5.818\\) 4.7 Formes bilineals i productes escalars Definició 4.5 Donat un espai vectorial \\(E\\) sobre un cos \\(\\K\\), una forma bilineal és una aplicació \\[\\phi \\colon E \\times E \\to \\K\\] tal que: \\(\\phi(\\vec u_1+\\vec u_2,v)=\\phi(\\vec u_1,\\vec v)+\\phi(\\vec u_2,\\vec v)\\) per a tots \\(\\vec u_1\\), \\(\\vec u_2\\) i \\(\\vec v\\) d’\\(E\\), \\(\\phi(\\lambda \\vec u,\\vec v)=\\lambda \\phi(\\vec u,\\vec v)\\) per a tots \\(\\vec u\\) i \\(\\vec v\\) d’\\(E\\) i \\(\\lambda \\in \\K\\), \\(\\phi(\\vec u,\\vec v_1,\\vec v_2)=\\phi(\\vec u,\\vec v_1)+\\phi(\\vec u,\\vec v_2)\\) per a tots \\(\\vec u\\), \\(\\vec v_1\\) i \\(\\vec v_2\\) d’\\(E\\) i \\(\\phi(\\vec u,\\lambda \\vec v)=\\lambda \\phi(\\vec u,\\vec v)\\) per a tots \\(\\vec u\\) i \\(\\vec v\\) d’\\(E\\) i \\(\\lambda \\in \\K\\). A més diem que una aplicació bilineal \\(\\phi\\) és: simètrica si \\(\\phi(\\vec u,\\vec v)=\\phi(\\vec v,\\vec u)\\) per a tots \\(\\vec u\\) i \\(\\vec v\\) d’\\(E\\), degenerada si existeix \\(\\vec u\\neq \\vec 0\\) tal que \\(\\phi(\\vec u,\\vec v)=0\\) per a tot \\(\\vec v\\) d’\\(E\\), i definida positiva si \\(\\phi(\\vec u,\\vec u)&gt;0\\) per a tot \\(\\vec u\\neq \\vec 0\\). Definició 4.6 Si \\(E\\) és un \\(\\K\\)-espai vectorial de dimensió finita i \\(\\calb=[\\vec v_1, \\ldots, \\vec v_n]\\) és una base d’\\(E\\), i \\(\\phi\\) és una aplicació bilineal, la matriu de l’aplicació bilineal en la base \\(\\calb\\) és: \\[[\\phi]_\\calb= \\begin{pmatrix} \\phi(\\vec v_1,\\vec v_1) &amp; \\phi(\\vec v_1,\\vec v_2) &amp; \\cdots &amp; \\phi(\\vec v_1,\\vec v_n)\\\\ \\phi(\\vec v_2,\\vec v_1) &amp; \\phi(\\vec v_2,\\vec v_2) &amp; \\cdots &amp; \\phi(\\vec v_2,\\vec v_n)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\phi(\\vec v_n,\\vec v_1) &amp; \\phi(\\vec v_n,\\vec v_2) &amp; \\cdots &amp; \\phi(\\vec v_n,\\vec v_n) \\end{pmatrix}.\\] Lema 4.5 Si \\(\\phi\\) és una aplicació bilineal definit sobre un \\(\\K\\)-espai vectorial \\(E\\) amb base \\(\\calb=[\\vec v_1, \\dots, \\vec v_n]\\) i \\([\\phi]_\\calb\\) és la matriu de \\(\\phi\\) en la base \\(\\calb\\), llavors: si \\(\\vec u\\) i \\(\\vec w\\) són vectors d’\\(E\\) amb coordenades en la base \\(\\calb\\): \\[[\\vec u]_\\calb = \\begin{pmatrix} \\lambda_1 \\\\ \\vdots \\\\ \\lambda_n \\end{pmatrix} \\text{ i } [\\vec w]_\\calb = \\begin{pmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_n \\end{pmatrix}.\\] llavors: \\[\\phi(\\vec u,\\vec w)=\\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i\\mu_j\\phi(\\vec v_i,\\vec v_j) =[\\vec u]_\\calb^T \\, [\\phi]_\\calb \\, [\\vec w]_\\calb .\\] Prova. La primera igualtat és per bilinealitat, i és igual a l’última expressió, que està escrita en forma matricial. Lema 4.6 Si tenim \\(\\calb=[\\vec v_1, \\ldots, \\vec v_n]\\) i \\(\\calc=[\\vec u_1,\\ldots, \\vec u_n]\\) bases de \\(\\K^n\\), \\([\\Id_n]_{\\calc,\\calb}\\) la matriu del canvi de base (la que té per columnes les coordenades dels vectors \\(\\vec u_j\\) en la base \\(\\calb\\)), i \\(\\phi\\) una aplicació bilineal, hi ha la relació següent entre les matrius de l’aplicació bilineal en cada base: \\[[\\phi]_\\calc=[\\Id_n]_{\\calc,\\calb}^T \\, [\\phi]_\\calb \\, [\\Id_n]_{\\calc,\\calb}\\] Prova. Les matrius de \\([\\phi]_\\calb\\) i \\([\\phi]_\\calc\\) han de complir que, per a tot \\(\\vec u\\) i \\(\\vec v\\) de \\(\\K^n\\) es compleixi: \\[\\begin{align*} \\tag{4.1} [\\vec u]_\\calc^T \\, [\\phi]_\\calc \\, [\\vec v]_\\calc = \\phi(\\vec u, \\vec v)= [\\vec u]_\\calb^T\\, [\\phi]_\\calb\\, [\\vec v]_\\calb . \\end{align*}\\] Però, per les propietats de la matriu \\([\\Id]_{\\calc,\\calb}\\) tenim: \\[[\\vec u]_\\calb=[\\Id]_{\\calc,\\calb} [\\vec u]_\\calc ,\\] i si ho substituïm a l’Equació (4.1), tenim, que per a tot \\(\\vec u\\) i \\(\\vec v\\) de \\(\\K^n\\): \\[[\\vec u]_\\calc^T \\, [\\phi]_\\calc \\, [\\vec v]_\\calc = ([\\Id]_{\\calc,\\calb} [\\vec u]_\\calc)^T \\, [\\phi]_\\calb \\, ([\\Id]_{\\calc,\\calb} [\\vec v]_\\calc) = [\\vec u]_\\calc^T \\, ([\\Id]_{\\calc,\\calb}^T [\\phi]_\\calb [\\Id]_{\\calc,\\calb}) \\, [\\vec v]_\\calc ,\\] pel que tenim la igualtat de matrius que volem. Exemple 4.10 Considerem la forma bilineal simètrica \\(\\phi\\colon \\R^2 \\times \\R^2 \\to \\R\\) donada per la matriu (en la base estàndard \\(\\calb=[\\vec e_1,\\vec e_2]\\)) \\[[\\phi]=\\begin{pmatrix} 0 &amp; 1/2 \\\\ 1/2 &amp; 0 \\end{pmatrix},\\] i volem calcular la matriu de \\(\\phi\\) en la base \\(\\calc=[\\smat{1\\\\1},\\smat{1\\\\-1}]\\). Llavors hem de calcular: \\[[\\Id]_{\\calc,\\calb}=\\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; -1 \\end{pmatrix}\\] I per tant la matriu \\[[\\phi]_\\calc = \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; -1 \\end{pmatrix}^T \\begin{pmatrix} 0 &amp; 1/2 \\\\ 1/2 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; -1 \\end{pmatrix}= \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix}.\\] Per tant, les matrius \\(\\smat{0 &amp; 1/2 \\\\ 1/2 &amp; 0}\\) i \\(\\smat{1 &amp; 0 \\\\ 0 &amp; -1}\\) representen la mateixa forma bilineal en bases diferents. Mirem ara un cas particular de forma bilineal: Definició 4.7 Un producte escalar sobre un \\(\\R\\)-espai vectorial \\(E\\) és una forma bilineal simètrica definida positiva. En general, escriurem el resultat amb un punt: \\(\\vec u \\cdot \\vec v\\). Si \\(\\calb\\) és una base finita d’un espai vectorial \\(E\\) i \\(\\cdot\\) és un producte escalar, la matriu \\([\\cdot]_\\calb\\) serà simètrica. Exemple 4.11 Si considerem \\(\\R^n\\) i \\(\\calb=[\\vec e_1, \\ldots, \\vec e_n]\\) la base estàndard, el producte escalar definit com \\(\\vec u \\cdot \\vec v=\\vec u^T \\vec v\\) és un producte escalar amb matriu la matriu identitat \\(\\1_n\\). Exemple 4.12 Si \\(E=C^\\infty(\\R)\\) definim el producte escalar: \\[f\\cdot g = \\int_{-1}^1 f(x)g(x)\\mathrm{d}x .\\] Com que a \\(C^\\infty(\\R)\\) no hi ha una base finita, no té sentit parlar de la matriu del producte escalar. Exemple 4.13 Considerem els nombres complexos \\(\\C\\) amb les operacions de l’Apèndix 2.1. Considerem ara \\(\\C^n\\) com a \\(\\C\\)-espai vectorial de dimensió \\(n\\) (suma coordenada a coordenada i multiplicació per un escalar a totes les coordenades). Definim la forma bilineal següent: \\[\\text{si } \\vec v=\\begin{pmatrix} v_1 \\\\ \\vdots \\\\ v_n \\end{pmatrix} \\text{ i } \\vec w=\\begin{pmatrix} w_1 \\\\ \\vdots \\\\ w_n \\end{pmatrix} \\text{ llavors } \\vec v\\cdot \\vec w= \\sum_{i=1}^n v_i\\overline{w}_i=\\vec v^T \\overline{\\vec w} \\text{ on } \\overline{\\vec w}=\\begin{pmatrix} \\overline w_1 \\\\ \\vdots \\\\ \\overline w_n \\end{pmatrix}.\\] Que compleix, a més de les propietats de ser forma bilineal: \\(\\vec v\\cdot \\vec w\\)=\\(\\overline{\\vec w\\cdot \\vec v}\\) per a tot \\(\\vec v,\\vec w\\in \\C^n\\) (diem que és hermítica) i \\(\\vec v\\cdot \\vec v=\\|\\vec v\\|^2\\in \\R\\) per a tot \\(\\vec v\\in\\C^n\\). 4.8 Tota matriu simètrica sobre \\(\\mathbb{R}\\) diagonalitza El resultat que volem demostrar a aquest apartat és: Teorema 4.6 Una matriu \\(A\\in M_n(\\R)\\) diagonalitza en una base ortonormal si, i només si, \\(A\\) és simètrica. Per demostrar el teorema espectral necessitem utilitzar el Teorema fonamental de l’àlgebra, que no demostrem aquí perquè la seva demostració surt dels objectius del curs: Teorema 4.7 Si \\(p(x)\\in\\C[x]\\) (\\(p(x)\\) és un polinomi amb coeficients a \\(\\C\\)) de grau \\(\\geq 1\\), existeix \\(z\\in \\C\\) tal que \\(p(z)=0\\). Prova. Demostració del teorema espectral. Una implicació és directa: si \\(A\\) diagonalitza en una base ortonormal vol dir que podem escriure: \\[A=Q D Q^T\\] amb \\(D\\) una matriu diagonal i \\(Q\\) una matriu ortogonal (\\(Q^{-1}=Q^T\\)). Per tant: \\[A^T=(QDQ^T)^T=(Q^T)^T D^T Q^T=Q D^T Q^T=A\\] i \\(A\\) és simètrica. Per demostrar l’altra implicació, ho farem per inducció sobre \\(n\\): Per a \\(n=1\\), tota matriu és simètrica i diagonal. Suposem cert per a \\(n-1\\), i volem demostrar-ho per \\(n\\): considerem \\(A\\in M_n(\\R)\\) i \\(p_A(x)\\) el polinomi característic. Pensem \\(p_A(x)\\in\\C[x]\\) i, pel teorema fonamental de l’àlgebra, existeix \\(\\lambda \\in \\C\\) tal que \\(p_A(\\lambda)=0\\). També tindrem un vector propi \\(\\vec v\\in \\C^n\\) tal que \\(A \\vec v=\\lambda \\vec v\\). Considerem ara la forma bilineal hermítica definida a l’Exemple 4.13: \\[A\\vec v \\cdot \\vec v = \\lambda \\vec v \\cdot \\vec v= \\lambda \\|\\vec v\\|^2 \\text{ amb $0\\neq \\|v\\|^2\\in R$}.\\] Però també: \\[A\\vec v \\cdot\\vec v = (A \\vec v)^T \\overline{\\vec v} = \\vec v^T A^T \\overline{\\vec v} = \\vec v^T \\overline{(\\overline{A^T} \\vec v)} = \\vec v^T A \\overline{\\vec v} = \\vec v^T (\\overline{\\lambda \\vec v})=\\overline{\\lambda} \\|\\vec v\\|^2 ,\\] on hem utilitzat que \\(A=A^T\\) (\\(A\\) és simètrica) i \\(\\overline A=A\\) (\\(A\\) té coeficients reals). Per tant, tenim que \\(\\lambda=\\overline{\\lambda}\\) i obtenim \\(\\lambda \\in \\R\\). Com que \\(p_A(\\lambda)=0\\), tenim que existeix un vector propi real \\(\\vec v\\neq \\vec 0\\) tal que \\(A\\vec v=\\lambda \\vec v\\). Podem considerar que \\(\\|\\vec v\\|=1\\) (si cal, substituïm \\(\\vec v\\) per \\(\\frac{1}{\\|\\vec v\\|}\\vec v\\)). Ampliem \\(\\vec v\\) amb \\(\\vec v_2\\), …, \\(\\vec v_n\\) de tal manera que \\([\\vec v,\\vec v_2, \\ldots ,\\vec v_n]\\) sigui una base ortonormal (primer ampliem fins a base, i després apliquem Gram-Schmidt). Si considerem \\(Q\\) la matriu que té per columnes els vectors \\(\\vec v\\), \\(\\vec v_2\\), …, \\(\\vec v_n\\), tenim: \\[QAQ^T=\\begin{pmatrix} \\lambda &amp; b_{12} &amp; \\cdots &amp; b_{1n} \\\\ 0 &amp; b_{22} &amp; \\cdots &amp; b_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; b_{n2} &amp; \\cdots &amp; b_{nn} \\end{pmatrix}\\] on hem utilitzat que \\(Q^{-1}=Q^T\\). Però com que \\(A=A^T\\), tenim \\((QAQ^T)^T=QAQ^T\\), i per tant \\(b_{12}=b_{13}=\\cdots=b_{1n}=0\\) i la matriu \\[B=\\begin{pmatrix} b_{22} &amp; \\cdots &amp; b_{2n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{n2} &amp; \\cdots &amp; b_{nn} \\end{pmatrix}\\] és simètrica. Ara restringim l’aplicació \\(f_A\\) a l’espai \\(\\langle v_2, \\dots , v_n\\rangle\\), que en la base \\([v_2, \\dots , v_n]\\) té per matriu \\[B=\\begin{pmatrix} b_{22} &amp; \\cdots &amp; b_{2n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{n2} &amp; \\cdots &amp; b_{nn} \\end{pmatrix} .\\] Per hipòtesis d’inducció, \\(B\\) diagonaliza en una base ortonormal, pel que existeix \\(P\\in M_{n-1}(\\R)\\) matriu ortogonal i \\(D\\in M_{n-1}(\\R)\\) matriu diagonal tals que \\(B=P^TDP\\). Amb això tenim: \\[QAQ^T=\\left(\\begin{array}{c|c} \\lambda &amp; 0 \\\\ \\hline 0 &amp; P^T D P \\end{array}\\right)= \\left(\\begin{array}{c|c} 1 &amp; 0 \\\\ \\hline 0 &amp; P^T \\end{array}\\right) \\left(\\begin{array}{c|c} \\lambda &amp; 0 \\\\ \\hline 0 &amp; D \\end{array}\\right) \\left(\\begin{array}{c|c} 1 &amp; 0 \\\\ \\hline 0 &amp; P \\end{array}\\right)\\] Per tant, considerem \\[Q&#39;=\\left(\\begin{array}{c|c} 1 &amp; 0 \\\\ \\hline 0 &amp; P \\end{array}\\right) Q \\text{ i } D&#39;=\\left(\\begin{array}{c|c} \\lambda &amp; 0 \\\\ \\hline 0 &amp; D \\end{array}\\right)\\] i tenim que \\(Q&#39;\\) és una matriu ortogonal i \\(A=Q&#39;^{-1} D&#39; Q&#39;\\), amb \\(D&#39;\\) una matriu diagonal, pel que \\(A\\) diagonalitza a una base ortonormal. 4.9 Descomposició en valors singulars A aquest apartat resoldrem la pregunta següent amb les eines que hem vist: si \\(f=f_A\\colon \\R^n \\to \\R^m\\) és una aplicació lineal, existeix una base ortonormal \\(\\calb=[\\vec v_1, \\dots, \\vec v_n]\\) d’\\(\\R^n\\) tal que \\(f(\\vec v_1), \\dots, f(\\vec v_n)\\) siguin ortogonals? Veurem que sí: Teorema 4.8 Si \\(f=f_A\\colon \\R^n \\to \\R^m\\) és una aplicació lineal, llavors existeix una base ortonormal \\(\\calb=[\\vec v_1, \\dots, \\vec v_n]\\) d’\\(\\R^n\\) tal que \\(f(\\vec v_1), \\dots, f(\\vec v_n)\\) són ortogonals. Prova. Considerem \\(f=f_A\\), i per tant \\(A\\in M_{m\\times n}(\\R)\\) la matriu tal que \\(f(\\vec v)=A\\vec v\\). Llavors, la matriu \\(A^TA\\in M_n(\\R)\\) és simètrica, i per tant diagonalitza en una base ortonormal, per tant, existeix una base ortonormal \\(\\calb=[\\vec v_1, \\dots, \\vec v_n]\\) de \\(\\R^n\\) i escalars \\(\\lambda_1, \\dots, \\lambda_n\\in \\R\\) tals que \\[A^T A \\vec v_i=\\lambda_i \\vec v_i \\text{ per a tot $i=1, \\dots, n$.}\\] Volem veure que \\(f(\\vec v_1), \\dots, f(\\vec v_n)\\) són ortogonals, o sigui, \\(f(\\vec v_i)\\cdot f(\\vec v_j)=0\\) si \\(i\\neq j\\): \\[\\begin{align*} f(\\vec v_i)\\cdot f(\\vec v_j) &amp; =(A\\vec v_i)\\cdot(A\\vec v_j)=(A\\vec v_i)^T(A\\vec v_j)=(\\vec v_i^T A^T)(A \\vec v_j)= \\\\ &amp; = \\vec v_i^T (A^T A \\vec v_j)=\\vec v_i^T (\\lambda_j \\vec v_j)=\\lambda_j (\\vec v_i \\cdot \\vec v_j)=0 . \\end{align*}\\] Observació. El Teorema 4.8 no diu que \\(\\calb\\) sigui única, i de fet, en general, no ho és: considereu l’aplicació identitat de \\(\\R^n \\to \\R^n\\), que envia qualsevol base ortonormal a vectors ortogonals. La interpretació geomètrica a \\(\\R^2\\) és la següent: la imatge d’una circumferència de radi \\(1\\) centrada a l’origen és una el·lipsi, els vectors propis d’\\(A^TA\\) corresponen a les direccions principals de l’el·lipsi, i les longituds dels semieixos principals són les arrels quadrades dels valors propis d’\\(A^TA\\). Posem nom a aquests valors: Definició 4.8 Donada una matriu \\(A\\in M_{m\\times n}(\\R)\\), definim els valors singulars d’\\(A\\) com les arrels quadrades dels valors propis de la matriu \\(A^TA\\): \\(\\sigma_1, \\dots, \\sigma_m\\) positius tals que \\(\\sigma_i^2\\) és valor propi d’\\(A^TA\\). Si ordenem els vectors de la base \\(\\calb\\) del Teorema 4.8 de tal manera que \\(\\|f(\\vec v_i)\\|\\geq \\|f(\\vec v_{i+1})\\|\\), aleshores \\(\\sigma_i=\\|f(\\vec v_i)\\|\\) a l’enunciat del Teorema 4.8. A partir de cert \\(r\\) tindrem \\(f(\\vec v_i)=\\vec 0\\) si \\(i&gt;r\\) (pot ser que \\(r=n\\)). Els vectors \\(\\vec w_i=\\frac{1}{\\|f(\\vec v_i)\\|} f(\\vec v_i)\\) per a \\(1\\leq i \\leq r\\) són linealment independents (perquè són ortogonals), i els podem ampliar a una base ortonormal \\(\\calc=[\\vec w_1, \\dots, \\vec w_m]\\) d’\\(\\R^m\\), per Gram-Schmidt. Proposició 4.5 Si \\(f=f_A\\colon \\R^n \\to \\R^m\\) és una aplicació lineal i \\(\\Rang(A)=r\\), llavors hi ha \\(r\\) valors singulars diferents de zero i els \\(n-r\\) restants valen zero. Prova. Tenim que en la base \\(\\calb\\) del Teorema 4.8 i \\(\\calc\\) la base del paràgraf anterior \\(f_A\\) té per matriu \\([f_A]_{\\calb.\\calc}\\), una matriu amb \\(\\sigma_i\\) a la diagonal i zeros a la resta. Llavors, com que el rang és la dimensió de la imatge, el rang de \\(f_A\\) és igual al de \\([f_A]_{\\calb,\\calc}\\), que és el nombre de \\(\\sigma_i\\) no nuls, i la resta han de ser zero. Observació. Amb tot el que hem fet, si considerem \\(A\\in M_{m\\times n}(\\R)\\) una matriu, \\(f_A\\colon \\R^n\\to\\R^m\\) l’aplicació lineal induïda, \\(\\calb=[\\vec v_1, \\dots, \\vec v_r, \\vec v_{r+1}, \\dots, \\vec v_n]\\) la base de \\(\\R^n\\) del Teorema 4.8, i \\(\\calc=[\\vec w_1, \\dots, \\vec w_m]\\) la base de \\(\\R^m\\) com a la discussió de després del teorema, llavors: \\([\\vec v_{r+1}, \\dots,\\vec v_n]\\) és una base ortonormal de \\(\\Ker(f_A)\\). \\([\\vec v_{1}, \\dots, \\vec v_r]\\) és una base ortonormal de \\(\\Ker(f_A)^\\bot\\). \\([\\vec w_1,\\dots,\\vec w_r]\\) és una base ortonormal de \\(\\Ima(f_A)\\). \\([\\vec w_{r+1},\\dots,\\vec w_m]\\) és una base ortonormal de \\(\\Ima(f_A)^\\bot\\). Podem aprofitar aquests raonaments per demostrar: Teorema 4.9 Si \\(A\\in M_{m\\times n}(\\R)\\), llavors existeixen \\(U \\in M_{m}(\\R)\\) i \\(V\\in M_n(\\R)\\) ortogonals, \\(D\\in M_{m\\times n}(\\R)\\) amb els únics elements no nuls a la diagonal i positius ordenats de manera decreixent, tals que \\(A=U D V^T\\). Aquesta descomposició s’anomena una descomposició en valors singulars d’\\(A\\). Prova. Considerem l’aplicació lineal \\(f_A\\) i veiem que necessitem bases ortonormals \\(\\calb\\) d’\\(\\R^n\\) i \\(\\calc\\) d’\\(\\R^m\\) tals que \\(D=[f_A]_{\\calb,\\calc}\\) sigui diagonal. El Teorema 4.8 ens dóna una base \\(\\calb\\) d’\\(\\R^n\\) que és ortonormal. Ordenem aquesta base de tal manera que \\(\\|f(\\vec v_i)\\|\\geq \\|f(\\vec v_{i+1})\\|\\). Sigui \\(V^T=V^{-1}\\) la matriu que té per columnes aquesta base, i sigui \\(r\\) tal que \\(f_(\\vec v_i)=\\vec 0\\) si \\(i&gt;r\\) (és el rang d’A). Considerem \\(\\calc=[\\vec w_1, \\dots, \\vec w_m]\\) la base de \\(\\R^m\\) de que hem considerat després del Teorema 4.8. Sigui \\(U\\) la matriu que té per columnes aquests vectors. Com que \\(f(\\vec v_i)=\\|f(\\vec v_i)\\| \\vec w_i\\) si \\(i\\leq r\\) i \\(f(\\vec v_i)=\\vec 0\\) si \\(i&gt;r\\), tenim que \\([f]_{\\calb,\\calc}\\) és diagonal amb els valors singulars d’\\(A\\) a la diagonal ordenats de més gran a més petit. Exemple 4.14 Considerem l’aplicació lineal \\(f_A\\colon \\R^2 \\to \\R^2\\) amb: \\[A=\\begin{pmatrix} 3 &amp; -9/5 \\\\ -1 &amp; 13/5 \\end{pmatrix}\\] i volem fer-ne una descomposició en valors singulars. Calculem primer \\(A^TA\\): \\[A^T A = \\begin{pmatrix} 10 &amp; -8 \\\\ -8 &amp; 10 \\end{pmatrix}\\] I la diagonalitzem. El polinomi característic és: \\[p_{A^TA}(x)=x^2 - 20 x + 36 = (x-18)(x-2)\\] Per tant, els valors propis d’\\(A^TA\\) són \\(\\lambda_1=18\\) i \\(\\lambda_2=2\\) i els corresponents vectors propis: \\[\\Ker(A^TA-18\\1_2)=\\langle \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\rangle \\text{ i } \\Ker(A^TA-2\\1_2)=\\langle \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\rangle\\] I com que els hem de considerar unitaris, obtenim: \\[V^T=\\begin{pmatrix} 1/\\sqrt{2} &amp; 1/\\sqrt{2} \\\\ -1/\\sqrt{2} &amp; 1/\\sqrt{2} \\end{pmatrix} \\text{ i per tant } V=\\begin{pmatrix} 1/\\sqrt{2} &amp; -1/\\sqrt{2} \\\\ 1/\\sqrt{2} &amp; 1/\\sqrt{2} \\end{pmatrix}.\\] La matriu diagonal són els valors singulars, que són les arrels quadrades dels valors propis d’\\(A^TA\\), per tant són \\(\\sigma_1=\\sqrt{18}=3\\sqrt{2}\\) i \\(\\sigma_2=\\sqrt{2}\\) i tenim: \\[D=\\begin{pmatrix} 3\\sqrt{2} &amp; 0 \\\\ 0 &amp; \\sqrt{2} \\end{pmatrix}.\\] I la matriu \\(U\\) té per columnes les imatges de \\(\\smat{1\\\\-1}\\) i \\(\\smat{1\\\\1}\\) per \\(f_A\\) dividides per les seves normes, per tant: \\[\\begin{pmatrix} 3 &amp; -9/5 \\\\ -1 &amp; 13/5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}= \\begin{pmatrix} 24/5 \\\\ -18/5 \\end{pmatrix}= 6 \\begin{pmatrix} 4/5 \\\\ -3/5 \\end{pmatrix}\\] i \\[\\begin{pmatrix} 3 &amp; -9/5 \\\\ -1 &amp; 13/5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}= \\begin{pmatrix} 6/5 \\\\ 8/5 \\end{pmatrix}= 2 \\begin{pmatrix} 3/5 \\\\ 4/5 \\end{pmatrix}\\] Per tant: \\[U=\\begin{pmatrix} 4/5 &amp; 3/5 \\\\ -3/5 &amp; 4/5 \\end{pmatrix}\\] I una descomposició d’\\(A\\) en valors singulars és: \\[\\begin{pmatrix} 3 &amp; -9/5 \\\\ -1 &amp; 13/5 \\end{pmatrix} =\\begin{pmatrix} 4/5 &amp; 3/5 \\\\ -3/5 &amp; 4/5 \\end{pmatrix} \\begin{pmatrix} 3\\sqrt{2} &amp; 0 \\\\ 0 &amp; \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} &amp; -1/\\sqrt{2} \\\\ 1/\\sqrt{2} &amp; 1/\\sqrt{2} \\end{pmatrix}.\\] A la Figura 2 podem veure com es modifica la circumferència unitat quan apliquem la matriu \\(A\\). Veiem que a la direcció \\(\\smat{4\\\\-3}\\) hi ha el primer eix principal amb semilongitud \\(3\\sqrt{2}\\) i a la direcció \\(\\smat{3\\\\4}\\) l’altre eix, amb semilongitud \\(\\sqrt{2}\\). Deformació de la circumferència unitat per l’aplicació lineal fA. 4.10 Classificació de formes bilineals simètriques sobre \\(\\mathbb{R}^n\\) A aquesta secció l’objectiu és classificar les formes bilineals. Definim abans què vol dir que siguin equivalents: Definició 4.9 Diem que dues formes bilineals \\(\\phi_1,\\phi_2\\colon \\K^n\\times\\K^n\\to \\K\\) amb matrius corresponents \\([\\phi_1]\\) i \\([\\phi_2]\\) són equivalents si existeix una base \\(\\calc\\) de \\(\\K^n\\) tal que \\[[\\phi_1]=[\\phi_2]_\\calc .\\] Dit d’una altra manera, si existeix una matriu invertible \\(\\cals\\in M_n(\\K)\\) (la matriu del canvi de base, que té per columnes els vectors de \\(\\calc\\)) tal que \\[[\\phi_1]=\\cals^T [\\phi_2] \\cals.\\] Escriurem \\(\\phi_1\\sim \\phi_2\\). Lema 4.7 Ser equivalents com a formes bilineals de \\(\\K^n\\times \\K^n\\) a \\(\\K\\) és una relació d’equivalència. O sigui: \\(\\phi \\sim \\phi\\) per a tot \\(\\phi\\) forma bilineal (reflexiva), \\(\\phi_1\\sim \\phi_2 \\Leftrightarrow \\phi_2 \\sim \\phi_1\\) per a totes \\(\\phi_1, \\phi_2\\) formes bilineals (simètrica) i \\(\\phi_1\\sim \\phi_2\\) i \\(\\phi_2\\sim \\phi_3\\) implica \\(\\phi_1\\sim \\phi_3\\) per a totes \\(\\phi_1,\\phi_2,\\phi_3\\) formes quadràtiques (transitiva). Prova. Cal veure en cada cas quina és la matriu \\(\\cals\\) del canvi de base: Per veure que \\(\\phi\\sim \\phi\\), considerem \\(\\cals=\\1_n\\), Si \\(\\phi_1\\sim \\phi_2\\) tenim que existeix \\(\\cals\\) una matriu de canvi de base (i per tant invertible) tal que: \\[[\\phi_1]=\\cals^T [\\phi_2] \\cals.\\] Però aquesta igualtat també es pot escriure com: \\[[\\phi_2]=(\\cals^{-1})^T [\\phi_1] \\cals^{-1}\\] i per tant \\(\\phi_2\\sim \\phi_1\\). Per hipòtesis tenim que existeixen \\(\\cals_1\\) i \\(\\cals_2\\) tals que: \\[[\\phi_1]=\\cals_1^T [\\phi_2] \\cals_1 \\text{ i } [\\phi_2]=\\cals_2^T [\\phi_3] \\cals_2.\\] Llavors: \\[[\\phi_1]=\\cals_1^T \\cals_2^T [\\phi_3] \\cals_2 \\cals_1= (\\cals_2 \\cals_1)^T [\\phi_3] (\\cals_2 \\cals_1)\\] i tenim \\(\\phi_1\\sim \\phi_3\\). A partir d’ara considerem que \\(\\K=\\R\\) i l’objectiu és classificar les formes bilineals \\(\\phi\\colon \\R^n\\times \\R^n \\to \\R\\): Teorema 4.10 Tota forma bilineal simètrica \\(\\phi\\colon \\R^n\\times \\R^n\\to \\R\\) és equivalent a una forma bilineal que té per matriu una matriu diagonal amb coeficients (a la diagonal): \\(1\\) a les \\(r\\) primeres files, \\(-1\\) a les \\(s\\) següents i \\(0\\) a les \\(t\\) últimes (\\(n=r+s+t\\)). O sigui una matriu de la forma: \\[\\begin{pmatrix} 1 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ &amp; \\ddots &amp; &amp; &amp; \\cdots &amp; &amp; &amp; \\cdots &amp; \\\\ 0 &amp; \\cdots &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0 &amp; -1 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ &amp; &amp; &amp; &amp; \\ddots &amp; &amp; &amp; \\cdots &amp; \\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; -1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ &amp; &amp; &amp; &amp; \\cdots &amp; &amp; &amp; \\ddots &amp; \\\\ 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{pmatrix}\\] A més, si definim la parella \\((r,s)\\) com la signatura de \\(\\phi\\) (\\(\\sign(\\phi)\\)), dues formes bilineals bilineals \\(\\phi_1,\\phi_2\\colon \\R^n\\times \\R^n\\to \\R\\) són equivalents si i només si \\(\\sign(\\phi_1)=\\sign(\\phi_2)\\). Prova. Considerem \\([\\phi]\\) la matriu simètrica \\(n\\times n\\) sobre \\(\\R\\) de la forma bilineal \\(\\phi\\). Pel Teorema espectral, existeix una matriu ortogonal \\(Q\\) i una matriu diagonal \\(D\\) amb valors \\(\\lambda_1, \\dots, \\lambda_n\\) tal que: \\[D=Q^T [\\phi] Q \\,.\\] Ara fem els canvis següents: Podem reordenar els elements de la diagonal de \\(D\\) reordenant les columnes de \\(Q\\): si \\(Q&#39;\\) és la matriu que resulta d’intercanviar les columnes \\(j\\) i \\(k\\) de \\(Q\\), llavors, \\(Q&#39;\\) continua sent ortogonal i el producte: \\[(Q&#39;)^T[\\phi]Q&#39;\\] també és diagonal, però intercanviant les posicions \\(i\\) i \\(j\\), o sigui, els valors \\(\\lambda_i\\) i \\(\\lambda_j\\). Aprofitem aquesta propietat per reordenar la diagonal de \\(D\\) i posar primer els \\(\\lambda_i&gt;0\\), llavors els \\(\\lambda_i&lt;0\\) i finalment els \\(\\lambda_i=0\\). Per tant, podem suposar que tenim \\(Q\\) una matriu ortogonal tal que \\[D=Q^T [\\phi] Q \\,.\\] amb \\(D\\) una matriu diagonal tal que a la diagonal té els \\(r\\) primers coeficients positius, els \\(s\\) següents negatius i els \\(t\\) últims zero. Considerem \\(A\\) la matriu diagonal següent, definida a partir dels coeficients de \\(D\\): el coeficient \\(a_{ii}\\) es defineix com: \\[a_{ii}=\\begin{cases} 1/\\sqrt{\\lambda_i} &amp; \\text{si $1\\leq i \\leq r$,} \\\\ 1/\\sqrt{-\\lambda_i} &amp; \\text{si $r &lt; i \\leq r+s$,} \\\\ 1 &amp; \\text{si $r+s &lt; i \\leq n$.} \\end{cases}\\] Tenim que \\(A^TDA\\) és una matriu diagonal amb \\(r\\) uns a les primeres files, \\(s\\) menys uns a les següents i zero a les últimes. Per tant: \\[[\\phi] \\sim A^TQ^T [\\phi] QA = (QA)^T [\\phi] QA\\] i \\((QA)^T [\\phi] QA\\) és diagonal i com diu l’enunciat del teorema. De moment hem vist que, com que “ser equivalent a té la propietat transitiva, si \\(\\phi_1\\) i \\(\\phi_2\\) tenen la mateixa signatura, les dues són equivalents a una mateixa forma bilineal i per tant \\(\\phi_1\\sim \\phi_2\\). Cal veure el recíproc, o el que és equivalent, que dues matrius diagonals equivalents \\(D\\) i \\(D&#39;\\) amb \\((r,s,t)\\) i \\((r&#39;,s&#39;,t&#39;)\\) coeficients \\(1\\), \\(-1\\) i \\(0\\) respectivament, llavors \\((r,s,t)=(r&#39;,s&#39;,t&#39;)\\). Observem que n’hi ha prou amb veure que \\(r\\leq r&#39;\\). Efectivament, intercanviant els rols de \\(D\\) i \\(D&#39;\\), conclourem que \\(r=r&#39;\\). Com que \\(t=\\dim\\ker(D)=\\dim\\ker(D&#39;)=t&#39;\\) i \\(r+s+t=n=r+s+t&#39;\\), en deduïm les igualtats \\(s=s&#39;\\) i \\(t=t&#39;\\). Per veure que \\(r\\leq r&#39;\\), considerem bases ortonormals \\(\\mathcal{B}=(u_1,\\ldots, u_n)\\) i \\(\\mathcal{B&#39;}=(u&#39;_1,\\ldots,u&#39;_n)\\) d’\\(\\R^n\\), respecte les quals la forma \\(\\phi\\) és, respectivament, \\(D\\) i \\(D&#39;\\). Considerem els subespais \\(V\\) i \\(V&#39;\\) donats per \\[V = \\langle u_1,\\ldots, u_r\\rangle,\\quad V&#39;=\\rangle u&#39;_1,\\ldots, u&#39;_{r&#39;}\\rangle,\\] i construirem una aplicació lineal injectiva \\(V\\to V&#39;\\). Sigui \\(g \\colon \\R^n\\to V&#39;\\) la projecció ortogonal al subespai \\(V&#39;\\), i sigui \\(f \\colon V \\to V&#39;\\) la restricció de \\(g\\) a \\(V\\). Vegem doncs que \\(f\\) és injectiva. Sigui \\(v\\in\\ker f\\). D’una banda, com que \\(v\\in V\\) i \\(\\phi|_V\\) té matriu identitat, tenim \\(\\phi(v,v) \\geq 0\\) amb igualtat si i només si \\(v=0\\). Però com que \\(v\\in \\ker f \\subseteq \\langle u&#39;_{r&#39;+1},\\ldots u&#39;_n\\rangle\\), tenim \\(\\phi(v,v)\\leq 0\\), ja que la restricció de \\(\\phi\\) a aquest subespai té matriu semidefinida negativa. Concloem que \\(\\varphi(v,v)=0\\) i per tant \\(v=0\\), com volíem veure. Exemple 4.15 Considerem la forma bilineal \\(\\phi\\colon \\R^4\\times\\R^4\\to\\R\\) donada per la matriu: \\[[\\phi]=\\left(\\begin{array}{rrrr} 1 &amp; 5 &amp; -1 &amp; 3 \\\\ 5 &amp; 1 &amp; 3 &amp; -1 \\\\ -1 &amp; 3 &amp; 1 &amp; 5 \\\\ 3 &amp; -1 &amp; 5 &amp; 1 \\end{array}\\right).\\] Una manera de classificar-la és de calcular el polinomi característic i estudiar el signe dels valors on s’anul·la: \\[p_{[\\phi]}(x)=\\det(A-x\\1_4)=x^{4} - 4 x^{3} - 64 x^{2} + 256 x .\\] I \\(p_{[\\phi]}(x)\\) s’anul·la als valors \\(\\{-8,0,4,8\\}\\), per tant té signatura \\((2,1)\\) i és equivalent a la forma bilineal que té per matriu: \\[\\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} .\\] Acabem aquesta secció aprofitant aquesta classificació per fer la corresponent de les formes quadràtiques. Definició 4.10 Una forma quadràtica sobre un cos \\(\\K\\) és una aplicació \\(q\\colon \\K^n\\to \\K\\) que es pot expressar com: \\[q(x_1, \\dots , x_n)=\\sum_{1\\leq i , j \\leq n} \\lambda_{ij}x_i x_j\\] amb \\(\\lambda_{ij}\\in \\K\\). També es pot escriure com: \\[q(x_1, \\dots , x_n)= \\vec x^T A \\vec x ,\\] on \\(\\vec x=\\smat{x_1 \\\\ \\vdots \\\\ x_n}\\) i \\(A\\) és la matriu (simètrica) que té per coeficients \\[a_{ij}=\\begin{cases} \\lambda_{ii} &amp; \\text{si $i=j$,} \\\\ \\frac{\\lambda_{ij}+\\lambda_{ji}}{2} &amp; \\text{si $i\\neq j$.}\\end{cases}\\] Observació. Amb el que hem vist, tenim una bijecció entre les matrius simètriques (que es poden pensar com formes bilineals en una base donada) i les formes quadràtiques. Escriurem \\([q]\\) per denotar la matriu simètrica corresponent a la forma quadràtica \\(q\\). Exemple 4.16 La forma quadràtica \\(q\\colon \\R^2 \\to \\R\\) definida per \\(q(x_1,x_2)=ax_1^2+bx_1x_2+cx_2^2\\) correspon a la matriu simètrica \\[\\begin{pmatrix} a &amp; b/2 \\\\ b/2 &amp; c \\end{pmatrix} .\\] Podem recuperar la forma quadràtica fent: \\[q(x_1,x_2)=\\begin{pmatrix} x_1 &amp; x_2 \\end{pmatrix}\\begin{pmatrix} a &amp; b/2 \\\\ b/2 &amp; c \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}=ax_1^2 + bx_1x_2+cx_2^2 .\\] Exemple 4.17 Considerem les formes quadràtiques \\(q_1\\) i \\(q_2\\) de \\(\\R^2\\) a \\(\\R\\) definides com \\(q_1(x_1,x_2)=x_1x_2\\) i \\(q_2(y_1,y_2)=y_1^2-y_2^2\\). Observem que si fem el canvi de base: \\[\\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 1/2 &amp; 1/2 \\\\ 1/2 &amp; -1/2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\\] es poden escriure: \\[y_1=\\frac12x_1+\\frac12x_2 \\text{ i } y_2=\\frac12x_1-\\frac12x_2\\] i tenim \\[q_2(y_1,y_2)=(\\frac12x_1+\\frac12x_2)^2 - (\\frac12x_1-\\frac12x_2)^2=x_1x_2 .\\] Per tant, tenen la mateixa forma. Aquesta igualtat també es pot veure en forma matricial observant que si \\(\\cals\\) és la matriu del canvi de base, tenim \\([q_1]=\\cals^T [q_2] \\cals\\), on \\([q_1]\\) i \\([q_2]\\) són les matrius simètriques corresponents a les formes bilineals \\(q_1\\) i \\(q_2\\) respectivament: \\[\\begin{pmatrix} 0 &amp; 1/2 \\\\ 1/2 &amp; 0 \\end{pmatrix}= \\begin{pmatrix} 1/2 &amp; 1/2 \\\\ 1/2 &amp; -1/2 \\end{pmatrix}^T \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} \\begin{pmatrix} 1/2 &amp; 1/2 \\\\ 1/2 &amp; -1/2 \\end{pmatrix}\\] Definició 4.11 Diem que dues formes quadràtiques \\(q_1,q_2\\colon \\K^n\\to \\K\\) són equivalents, amb matrius si existeix un canvi de base \\(\\cals\\) a \\(\\K^n\\) tal que \\[[q_1]=\\cals^T [q_2] \\cals.\\] Escriurem \\(q_1\\sim q_2\\). Com que els canvis de base afecten de la mateixa manera que afectaven a les formes bilineals (veure Lema 4.6), obtenim que “ser equivalent a” també es una relació d’equivalència a les formes quadràtiques i l’anàleg al Teorema 4.10: Teorema 4.11 Tota forma quadràtica \\(q\\colon \\R^n\\to \\R\\) és equivalent a una forma bilineal del tipus: \\[q_1(x_1,\\dots, x_n)=x_1^2+ \\cdots + x_r^2 - x_{r+1}^2-\\cdots -x_{r+s}^2 .\\] A més, si definim la parella \\((r,s)\\) com la signatura de \\(q\\) (\\(\\sign(q)\\)), dues formes quadràtiques \\(q_1,q_2\\colon \\R^n\\to \\R\\) són equivalents si i només si \\(\\sign(q_1)=\\sign(q_2)\\). Exemple 4.18 Classifiquem la forma quadràtica \\(q\\colon \\R^4 \\to \\R\\): \\[q(x_1,x_2,x_3,x_4)=x_1^2+10 x_1x_2 -2 x_1x_3+6x_1x_4 + x_2^2 -6x_2x_3-2x_2x_4+x_3^3+10x_3x_4-x_4^2.\\] Que té per matriu: \\[\\left(\\begin{array}{rrrr} 1 &amp; 5 &amp; -1 &amp; 3 \\\\ 5 &amp; 1 &amp; 3 &amp; -1 \\\\ -1 &amp; 3 &amp; 1 &amp; 5 \\\\ 3 &amp; -1 &amp; 5 &amp; 1 \\end{array}\\right)\\] Per tant, podem aprofitar els càlculs de l’Exemple 4.15, i tenim que és equivalent a: \\[q(y_1,y_2,y_3,y_4)=y_1^2+y_2^2-y_3^2.\\] 4.11 Exercicis recomanats Els exercicis que segueixen són útils per practicar el material presentat. La numeració és la de [1]. Secció 5.1: 12, 16, 18. Secció 5.2: 14, 34, 38. Secció 5.3: 5-11, 13-20, 32. Secció 5.4: 2, 8, 10. Secció 5.5: 4, 10, 16, 22. Secció 8.1: 6, 12, 16, 22. Secció 8.2: 4, 10, 18, 22. Secció 8.3: 6, 16, 18, 20. References "],["bibliografia.html", "Bibliografia", " Bibliografia "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
