<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítol 4. Ortogonalitat | Apunts d’Àlgebra Lineal</title>
  <meta name="description" content="Apunts d’un curs d’Àlgebra Lineal a la UAB" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítol 4. Ortogonalitat | Apunts d’Àlgebra Lineal" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Apunts d’un curs d’Àlgebra Lineal a la UAB" />
  <meta name="github-repo" content="mmasdeu/algebralineal" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítol 4. Ortogonalitat | Apunts d’Àlgebra Lineal" />
  
  <meta name="twitter:description" content="Apunts d’un curs d’Àlgebra Lineal a la UAB" />
  

<meta name="author" content="Marc Masdeu, Albert Ruiz" />


<meta name="date" content="2023-09-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="diagonalització.html"/>
<link rel="next" href="bibliografia.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Autogenerated by build.sh, do not edit! -->
<span class="math inline" style="display: none">
\(\newcommand{\bbdef}[1]{\expandafter\newcommand% 
	\csname#1\endcsname{\mathbb{#1}}}
% \bbdef{C} \bbdef{F} \bbdef{R} \bbdef{Z} \bbdef{Q} \bbdef{K} \bbdef{N}

% %%% SCRIPT COMMANDS:  \cala=\mathcal{A}, ... \calz=\mathcal{Z}
% \newcounter{let} \setcounter{let}{0}
% \loop\stepcounter{let}
% \expandafter\edef\csname cal\alph{let}\endcsname%
% {\noexpand\mathcal{\Alph{let}}}
% \ifnum\thelet<26\repeat


\renewcommand{\1}{\mathbf{1}}
\newcommand{\0}{\mathbf{0}}

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{r}|r@{}}
}{%
  \end{array}\right)
}

\newenvironment{llista-exercicis}{%
\subsection*{Exercicis recomanats}
Els exercicis que segueixen són útils per practicar el 
material presentat. La numeració és la de~\cite{Bret}.
\begin{description}}{\end{description}}

\newcommand{\smat}[1]{\left(\begin{smallmatrix}#1\end{smallmatrix}\right)}

\renewcommand{\setminus}{\smallsetminus}
\renewcommand{\Rang}{\operatorname{Rang}}
\renewcommand{\rref}{\operatorname{rref}}
\renewcommand{\rcef}{\operatorname{rcef}}
\renewcommand{\Ker}{\operatorname{Ker}}
\renewcommand{\Ima}{\operatorname{Im}}
\renewcommand{\Id}{\operatorname{Id}}
\renewcommand{\Map}{\operatorname{Map}}
\renewcommand{\sign}{\operatorname{Sign}}
\renewcommand{\refl}{\operatorname{Refl}}
\renewcommand{\Tr}{\operatorname{Tr}}
\renewcommand{\multalg}{\operatorname{MultAlg}}
\renewcommand{\multgeom}{\operatorname{MultGeom}}
\renewcommand{\proj}{\operatorname{Proj}}


\renewcommand{\Q}{\mathbb{Q}}
\renewcommand{\N}{\mathbb{N}}

\renewcommand{\Z}{\mathbb{Z}}
\renewcommand{\K}{\mathbb{K}}

\renewcommand{\R}{\mathbb{R}}
\renewcommand{\F}{\mathbb{F}}
\renewcommand{\CC}{\mathbb{C}}
\renewcommand{\C}{\mathbb{C}}


\renewcommand{\fX}{\mathfrak{X}}

\renewcommand{\SL}{\operatorname{SL}}
\renewcommand{\GL}{\operatorname{GL}}
\renewcommand{\PSL}{\operatorname{PSL}}
\renewcommand{\PGL}{\operatorname{PGL}}

%Some common abreviations
\renewcommand{\lto}{\longrightarrow}
\renewcommand{\dfn}{\ensuremath{:=}}
\renewcommand{\surjects}{\twoheadrightarrow}
\renewcommand{\injects}{\hookrightarrow}
\renewcommand{\id}{\operatorname{Id}}
\renewcommand{\tns}[1][]{\otimes_{\!#1}}
\renewcommand{\mtx}[4]{\left(\begin{matrix}#1&#2\\#3&#4\end{matrix}\right)}
\renewcommand{\mat}[1]{\left(\begin{matrix}#1\end{matrix}\right)}
\renewcommand{\smat}[1]{\left(\begin{smallmatrix}#1\end{smallmatrix}\right)}
\renewcommand{\smtx}[4]{\left(\begin{smallmatrix}#1&#2\\#3&#4\end{smallmatrix}\right)}

\renewcommand{\slz}{\operatorname{SL}_2(\bZ)}
\renewcommand{\to}{\longrightarrow}
\renewcommand{\dlog}{\operatorname{dlog}}
% \renewcommand{\Im}{\operatorname{Im}}
% \renewcommand{\Re}{\operatorname{Re}}

\renewcommand{\abs}[1]{|#1|}
\renewcommand{\slsh}[1]{|_{#1}}
\renewcommand{\qed}{\blacksquare}
\renewcommand{\Irr}{\operatorname{Irr}}
\renewcommand{\Aut}{\operatorname{Aut}}
\renewcommand{\Gal}{\operatorname{Gal}}
\renewcommand{\Mor}{\operatorname{Mor}}
\renewcommand{\Hom}{\operatorname{Hom}}
% \renewcommand{\implies}{\Longrightarrow}

% \renewcommand{\char}{\operatorname{char}}
\renewcommand{\car}{\operatorname{char}}
\renewcommand{\mcd}{\operatorname{mcd}}
\renewcommand{\gcd}{\operatorname{mcd}}
\renewcommand{\Eq}{\operatorname{Eq}}
\renewcommand{\res}{\operatorname{res}}
\renewcommand{\normaleq}{\trianglelefteq}
\renewcommand{\disc}{\operatorname{disc}}

\renewcommand{\cala}{\mathcal{A}}
\renewcommand{\calb}{\mathcal{B}}
\renewcommand{\calc}{\mathcal{C}}
\renewcommand{\cald}{\mathcal{D}}
\renewcommand{\cale}{\mathcal{E}}
\renewcommand{\calp}{\mathcal{P}}
\renewcommand{\calq}{\mathcal{Q}}
\renewcommand{\calr}{\mathcal{R}}
\renewcommand{\cals}{\mathcal{S}}
\renewcommand{\calt}{\mathcal{T}}\)</span>
<link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
<script src="https://tikzjax.com/v1/tikzjax.js"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
  "HTML-CSS": {
      styles: {".MathJax_Preview": {visibility: "hidden"},
	       ".MathJax_Display" : {stroke: "currentColor"}},
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
  },
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    TeX: {extensions: ["AMSmath.js","AMSsymbols.js","https://mat.uab.cat/~masdeu/xypic.js"],
    noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },}
});
</script>
<script>
  MathJax = {
    loader: {
      load: ['[custom]/xypic.js'],
      paths: {custom: '.'}
    },
    tex: {
      packages: {'[+]': ['xypic']}
    }
    
  };
</script>
<script type="text/javascript" id="MathJax-script" async
src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS_HTML">
</script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Àlgebra Lineal</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introducció</a></li>
<li class="chapter" data-level="1" data-path="matrius-i-equacions-lineals.html"><a href="matrius-i-equacions-lineals.html"><i class="fa fa-check"></i><b>1</b> Matrius i equacions lineals</a>
<ul>
<li class="chapter" data-level="1.1" data-path="matrius-i-equacions-lineals.html"><a href="matrius-i-equacions-lineals.html#matrius"><i class="fa fa-check"></i><b>1.1</b> Matrius</a></li>
<li class="chapter" data-level="1.2" data-path="matrius-i-equacions-lineals.html"><a href="matrius-i-equacions-lineals.html#subsec:opmat"><i class="fa fa-check"></i><b>1.2</b> Operacions amb matrius. Matriu invertible</a></li>
<li class="chapter" data-level="1.3" data-path="matrius-i-equacions-lineals.html"><a href="matrius-i-equacions-lineals.html#subsec:trans-el"><i class="fa fa-check"></i><b>1.3</b> Transformacions elementals en matrius</a></li>
<li class="chapter" data-level="1.4" data-path="matrius-i-equacions-lineals.html"><a href="matrius-i-equacions-lineals.html#criteri-dinvertibilitat.-rang-duna-matriu"><i class="fa fa-check"></i><b>1.4</b> Criteri d’invertibilitat. Rang d’una matriu</a></li>
<li class="chapter" data-level="1.5" data-path="matrius-i-equacions-lineals.html"><a href="matrius-i-equacions-lineals.html#resolució-de-sistemes-dequacions-lineals"><i class="fa fa-check"></i><b>1.5</b> Resolució de sistemes d’equacions lineals</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="matrius-i-equacions-lineals.html"><a href="matrius-i-equacions-lineals.html#exercicis-recomanats"><i class="fa fa-check"></i><b>1.5.1</b> Exercicis recomanats</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html"><i class="fa fa-check"></i><b>2</b> Espais vectorials i aplicacions lineals</a>
<ul>
<li class="chapter" data-level="2.1" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#subsec:matriusapl"><i class="fa fa-check"></i><b>2.1</b> Matrius com a aplicacions lineals</a></li>
<li class="chapter" data-level="2.2" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#subsec:linealgeom"><i class="fa fa-check"></i><b>2.2</b> Aplicacions lineals i geometria</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#homotècies"><i class="fa fa-check"></i><b>2.2.1</b> Homotècies</a></li>
<li class="chapter" data-level="2.2.2" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#projeccions-ortogonals"><i class="fa fa-check"></i><b>2.2.2</b> Projeccions ortogonals</a></li>
<li class="chapter" data-level="2.2.3" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#subsubsec:reflexio"><i class="fa fa-check"></i><b>2.2.3</b> Reflexions</a></li>
<li class="chapter" data-level="2.2.4" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#subsubsec:rotacio"><i class="fa fa-check"></i><b>2.2.4</b> Rotacions</a></li>
<li class="chapter" data-level="2.2.5" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#lliscaments"><i class="fa fa-check"></i><b>2.2.5</b> Lliscaments</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#subespais-generadors-i-bases"><i class="fa fa-check"></i><b>2.3</b> Subespais, generadors i bases</a></li>
<li class="chapter" data-level="2.4" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#suma-i-intersecció-de-subespais-vectorials"><i class="fa fa-check"></i><b>2.4</b> Suma i intersecció de subespais vectorials</a></li>
<li class="chapter" data-level="2.5" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#aplicacions-injectives-exhaustives-i-bijectives"><i class="fa fa-check"></i><b>2.5</b> Aplicacions injectives, exhaustives i bijectives</a></li>
<li class="chapter" data-level="2.6" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#coordenades-de-vectors"><i class="fa fa-check"></i><b>2.6</b> Coordenades de vectors</a></li>
<li class="chapter" data-level="2.7" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#espais-vectorials"><i class="fa fa-check"></i><b>2.7</b> Espais vectorials</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="espais-vectorials-i-aplicacions-lineals.html"><a href="espais-vectorials-i-aplicacions-lineals.html#exercicis-recomanats-1"><i class="fa fa-check"></i><b>2.7.1</b> Exercicis recomanats</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="diagonalització.html"><a href="diagonalització.html"><i class="fa fa-check"></i><b>3</b> Diagonalització</a>
<ul>
<li class="chapter" data-level="3.1" data-path="diagonalització.html"><a href="diagonalització.html#subsec:motiv-diag"><i class="fa fa-check"></i><b>3.1</b> Motivació</a></li>
<li class="chapter" data-level="3.2" data-path="diagonalització.html"><a href="diagonalització.html#determinants"><i class="fa fa-check"></i><b>3.2</b> Determinants</a></li>
<li class="chapter" data-level="3.3" data-path="diagonalització.html"><a href="diagonalització.html#polinomi-característic.-valors-i-vectors-propis"><i class="fa fa-check"></i><b>3.3</b> Polinomi característic. Valors i vectors propis</a></li>
<li class="chapter" data-level="3.4" data-path="diagonalització.html"><a href="diagonalització.html#vectors-propis-associats-a-un-valor-propi"><i class="fa fa-check"></i><b>3.4</b> Vectors propis associats a un valor propi</a></li>
<li class="chapter" data-level="3.5" data-path="diagonalització.html"><a href="diagonalització.html#sec:complexos"><i class="fa fa-check"></i><b>3.5</b> Interludi: els nombres complexos</a></li>
<li class="chapter" data-level="3.6" data-path="diagonalització.html"><a href="diagonalització.html#matrius-sobre-r"><i class="fa fa-check"></i><b>3.6</b> Matrius sobre <span class="math inline">\(\R\)</span></a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="diagonalització.html"><a href="diagonalització.html#exercicis-recomanats-2"><i class="fa fa-check"></i><b>3.6.1</b> Exercicis recomanats</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ortogonalitat.html"><a href="ortogonalitat.html"><i class="fa fa-check"></i><b>4</b> Ortogonalitat</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ortogonalitat.html"><a href="ortogonalitat.html#ortogonalitat-a-rn"><i class="fa fa-check"></i><b>4.1</b> Ortogonalitat a <span class="math inline">\(\R^n\)</span></a></li>
<li class="chapter" data-level="4.2" data-path="ortogonalitat.html"><a href="ortogonalitat.html#el-teorema-de-pitàgores"><i class="fa fa-check"></i><b>4.2</b> El teorema de Pitàgores</a></li>
<li class="chapter" data-level="4.3" data-path="ortogonalitat.html"><a href="ortogonalitat.html#mètode-de-gram-schmidt"><i class="fa fa-check"></i><b>4.3</b> Mètode de Gram-Schmidt</a></li>
<li class="chapter" data-level="4.4" data-path="ortogonalitat.html"><a href="ortogonalitat.html#aplicacions-i-matrius-ortogonals"><i class="fa fa-check"></i><b>4.4</b> Aplicacions i matrius ortogonals</a></li>
<li class="chapter" data-level="4.5" data-path="ortogonalitat.html"><a href="ortogonalitat.html#matriu-duna-projecció-ortogonal-en-una-base-ortonormal"><i class="fa fa-check"></i><b>4.5</b> Matriu d’una projecció ortogonal en una base ortonormal</a></li>
<li class="chapter" data-level="4.6" data-path="ortogonalitat.html"><a href="ortogonalitat.html#mínims-quadrats"><i class="fa fa-check"></i><b>4.6</b> Mínims quadrats</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="ortogonalitat.html"><a href="ortogonalitat.html#recta-de-regressió"><i class="fa fa-check"></i><b>4.6.1</b> Recta de regressió</a></li>
<li class="chapter" data-level="4.6.2" data-path="ortogonalitat.html"><a href="ortogonalitat.html#cas-general"><i class="fa fa-check"></i><b>4.6.2</b> Cas general</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="ortogonalitat.html"><a href="ortogonalitat.html#formes-bilineals-i-productes-escalars"><i class="fa fa-check"></i><b>4.7</b> Formes bilineals i productes escalars</a></li>
<li class="chapter" data-level="4.8" data-path="ortogonalitat.html"><a href="ortogonalitat.html#tota-matriu-simètrica-sobre-mathbbr-diagonalitza"><i class="fa fa-check"></i><b>4.8</b> Tota matriu simètrica sobre <span class="math inline">\(\mathbb{R}\)</span> diagonalitza</a></li>
<li class="chapter" data-level="4.9" data-path="ortogonalitat.html"><a href="ortogonalitat.html#descomposició-en-valors-singulars"><i class="fa fa-check"></i><b>4.9</b> Descomposició en valors singulars</a></li>
<li class="chapter" data-level="4.10" data-path="ortogonalitat.html"><a href="ortogonalitat.html#classificació-de-formes-bilineals-simètriques-sobre-mathbbrn"><i class="fa fa-check"></i><b>4.10</b> Classificació de formes bilineals simètriques sobre <span class="math inline">\(\mathbb{R}^n\)</span></a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="ortogonalitat.html"><a href="ortogonalitat.html#exercicis-recomanats-3"><i class="fa fa-check"></i><b>4.10.1</b> Exercicis recomanats</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
<li class="divider"></li>
<li><a href="https://github.com/mmasdeu/algebralineal" target="blank">Publicat amb bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Apunts d’Àlgebra Lineal</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ortogonalitat" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Capítol 4. </span> Ortogonalitat<a href="ortogonalitat.html#ortogonalitat" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Podeu trobar el contingut de la part d’ortogonalitat a <span class="citation">[<a href="#ref-Bret" role="doc-biblioref">1</a> Tema 5]</span>, i
de la part de formes quadràtiques a (<span class="citation">[<a href="#ref-Bret" role="doc-biblioref">1</a>]</span>,<span class="citation">[<a href="#ref-NaXa" role="doc-biblioref">2</a>]</span>).</p>
<div id="ortogonalitat-a-rn" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Ortogonalitat a <span class="math inline">\(\R^n\)</span><a href="ortogonalitat.html#ortogonalitat-a-rn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Considerem l’espai vectorial <span class="math inline">\(\R^n\)</span> i recordem el producte escalar
definit com: si <span class="math inline">\(\vec u=\smat{u_1\\u_2\\ \vdots\\u_n}\)</span> i
<span class="math inline">\(\vec v=\smat{v_1\\v_2\\ \vdots \\ v_n}\)</span> són vectors d’<span class="math inline">\(\R^n\)</span>, llavors
<span class="math inline">\(\vec u \cdot \vec v = \vec u^T \vec v=\sum_{i=1}^n u_iv_i\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-191" class="remark"><em>Observació</em>. </span>Observem que a les igualtats anteriors hi ha cert abús de notació: estem
definint el producte escalar (que denotem amb “<span class="math inline">\(\cdot\)</span>”), que resulta en
un nombre real. El terme entre les dues igualtats representa una matriu
<span class="math inline">\(1\times 1\)</span>, que obtenim del producte de la matriu fila formada amb les
entrades del vector <span class="math inline">\(\vec u\)</span> amb la matriu columna formada amb les
entrades del vector <span class="math inline">\(\vec v\)</span>. Per tant, estem identificant vectors amb
matrius columna, i escalars amb matrius <span class="math inline">\(1\times 1\)</span>, i ho seguirem fent
sense massa risc de confusió.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-192" class="definition"><strong>Definició 4.1  </strong></span></p>
<ul>
<li><p>Diem que dos vectors <span class="math inline">\(\vec u\)</span> i <span class="math inline">\(\vec v\)</span> són <em>ortogonals</em> si
<span class="math inline">\(\vec u\cdot\vec v=0\)</span>. Més generalment, diem que els vectors
<span class="math inline">\(\vec u_1, \dots, \vec u_k\)</span> de <span class="math inline">\(\R^n\)</span> <em>són ortogonals</em> si són
ortogonals dos a dos, o sigui, si <span class="math inline">\(\vec u_i\cdot\vec u_j=0\)</span> per a
tot <span class="math inline">\(i\neq j\)</span>.</p></li>
<li><p>Definim la <em>longitud d’un vector</em> com la quantitat
<span class="math inline">\(\|\vec u\|=\sqrt{\vec u\cdot \vec u}=\sqrt{\displaystyle\sum_{i=1}^n u_i^2}.\)</span></p></li>
<li><p>Diem que un vector <span class="math inline">\(\vec u\in\R^n\)</span> és <em>unitari</em> si <span class="math inline">\(\|\vec u\|=1\)</span>.</p></li>
<li><p>Diem que els vectors <span class="math inline">\(\vec u_1, \dots, \vec u_k\)</span> de <span class="math inline">\(\R^n\)</span> <em>són
ortonormals</em> si són unitaris i ortogonals dos a dos, o sigui, si
<span class="math display">\[
\vec u_i\cdot\vec u_j=
    \begin{cases}
     1 &amp; \text{si $i=j$} \\
     0 &amp; \text{si $i\neq j.$}
    \end{cases}
\]</span></p></li>
</ul>
</div>
<div class="remark">
<p><span id="unlabeled-div-193" class="remark"><em>Observació</em>. </span>Podem passar de vectors no nuls <span class="math inline">\(\vec u_1, \dots, \vec u_k\)</span> de <span class="math inline">\(\R^n\)</span>
ortogonals a ortonormals dividint cadascun per la seva norma: si
<span class="math inline">\(\vec u_1, \dots, \vec u_k\)</span> de <span class="math inline">\(\R^n\)</span> són no nuls, llavors
<span class="math inline">\(\frac{\vec u_1}{\|\vec u_1\|}, \dots, \frac{\vec u_k}{\|\vec u_k\|}\)</span>
són unitaris, i si <span class="math inline">\(\vec u_i\cdot \vec u_j=0\)</span>, llavors,
<span class="math inline">\(\frac{\vec u_i}{\|\vec u_i\|}\cdot \frac{\vec u_j}{\|\vec u_j\|}=\frac{1}{\|\vec u_i\| \|\vec u_j\|}(\vec u_i\cdot \vec u_j)=0\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-194" class="example"><strong>Exemple 4.1  </strong></span>La base estàndard d’<span class="math inline">\(\R^n\)</span> formada pels vectors <span class="math inline">\(\vec e_i\)</span> amb un <span class="math inline">\(1\)</span> a
la posició <span class="math inline">\(i\)</span> i <span class="math inline">\(0\)</span> a la resta de posicions és una base ortonormal.</p>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-195" class="lemma"><strong>Lema 4.1  </strong></span>Si els vectors no nuls <span class="math inline">\(\vec u_1, \dots, \vec u_k\)</span> de <span class="math inline">\(\R^n\)</span> són
ortogonals, llavors són linealment independents.<br />
En particular, si tenim <span class="math inline">\(n\)</span> vectors <span class="math inline">\(\vec u_1, \dots, \vec u_n\)</span> de
<span class="math inline">\(\R^n\)</span> ortonormals, formen una base.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-196" class="proof"><em>Prova</em>. </span>Suposem que tenim una combinació lineal dels vectors
<span class="math inline">\(\vec u_1, \dots, \vec u_k\)</span> que dóna zero:</p>
<p><span class="math display">\[
0 = \lambda_1 \vec u_1 + \cdots + \lambda_n\vec u_n \,.
\]</span>
Si veiem que
tots els <span class="math inline">\(\lambda_i\)</span> són zero ja estarem. Triem una <span class="math inline">\(i\)</span> i fent el
producte escalar per <span class="math inline">\(\vec u_i\)</span> tenim:
<span class="math display">\[
\begin{aligned}
    0 &amp;  = (\lambda_1 \vec u_1 + \cdots + \lambda_n\vec u_n)\cdot \vec u_i= \\
     &amp; = \lambda_1 (\vec u_1 \cdot \vec u_i) + \cdots + \lambda_n(\vec u_n\cdot \vec u_i)=\\
     &amp; = \lambda_i \|\vec u_i\|^2\quad \text{(tots els altres productes escalars són zero)}
\end{aligned}
\]</span>
Per tant, com que <span class="math inline">\(\vec u_i\neq\vec 0\)</span>, <span class="math inline">\(\lambda_i=0\)</span>.
Com que això és cert per cada <span class="math inline">\(i=1,\ldots,n\)</span>, els vectors són linealment
independents.</p>
<p>En el cas de tenir <span class="math inline">\(n\)</span> vectors ortonormals, són <span class="math inline">\(n\)</span> vectors ortogonals
no nuls, per tant linealment independents i com que la dimensió de
<span class="math inline">\(\R^n\)</span> és <span class="math inline">\(n\)</span>, són base
(Teorema <a href="espais-vectorials-i-aplicacions-lineals.html#thm:baseV">2.4</a>).</p>
</div>
<p>Veurem més endavant com trobar una base ortonormal d’un subespai <span class="math inline">\(V\)</span> de
<span class="math inline">\(\R^n\)</span>. Suposem però que tenim <span class="math inline">\([\vec u_1,\ldots,\vec u_k]\)</span> una base
ortonormal de <span class="math inline">\(V\)</span>, i considerem la <em>projecció ortogonal</em>
<span class="math inline">\(\proj_V\colon \R^n\rightarrow \R^n\)</span>, que envia un vector
<span class="math inline">\(\vec v\in \R^n\)</span> al vector <span class="math inline">\(\proj_V(\vec v) = \vec v^\parallel\)</span>, on</p>
<p><span class="math display">\[
\vec v^\parallel = (\vec v\cdot \vec u_1)\vec u_1 + \cdots + (\vec v\cdot \vec u_k)\vec u_k \in V.
\]</span></p>
<p>Observem que <span class="math inline">\(\proj_V\colon \R^n\rightarrow \R^n\)</span> és una aplicació
lineal (per les propietats de linealitat del producte escalar). Com que
la seva imatge està continguda a <span class="math inline">\(V\)</span> i <span class="math inline">\(\proj_V(\vec u_i) = \vec u_i\)</span>,
en deduïm que <span class="math inline">\(\Ima(\proj_V)=V\)</span>. A més,
<span class="math display">\[
\begin{aligned}
\Ker(\proj_V) &amp;= \{\vec w\in\R^n ~|~ \vec w^\parallel = 0\}\\
              &amp;= \{\vec w\in\R^n ~|~ \vec w\cdot \vec u_i = 0, \quad i=1,\ldots k\}\\
              &amp;= \{\vec w\in\R^n ~|~ \vec w\cdot \vec v = 0\quad\text{ per a tot }\vec w \in V\}.
\end{aligned}
\]</span>
La següent definició dona nom al nucli de <span class="math inline">\(\proj_V\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-197" class="definition"><strong>Definició 4.2  </strong></span>Donat un subespai <span class="math inline">\(V\subseteq \R^n\)</span>, el <em>complement ortogonal</em> de <span class="math inline">\(V\)</span>,
que escriurem <span class="math inline">\(V^\perp\)</span>, és el conjunt</p>
<p><span class="math display">\[
V^\perp = \{\vec w\in \R^n ~|~ \vec w\cdot \vec v = 0\text{ per a tot } \vec v\in V\}.
\]</span></p>
</div>
<p>El complement ortogonal satisfà les següents propietats bàsiques.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-198" class="proposition"><strong>Proposició 4.1  </strong></span>Sigui <span class="math inline">\(V\subseteq \R^n\)</span> un subespai d’<span class="math inline">\(\R^n\)</span>. Aleshores:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(V^\perp\)</span> també és un subespai d’<span class="math inline">\(\R^n\)</span>,</p></li>
<li><p><span class="math inline">\(V \cap V^\perp = \{\vec 0\}\)</span>,</p></li>
<li><p><span class="math inline">\(\dim(V)+\dim(V^\perp) = n\)</span>,</p></li>
<li><p><span class="math inline">\((V^\perp)^\perp = V\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-199" class="proof"><em>Prova</em>. </span>La primera afirmació es comprova de manera directa, fent servir
les propietats del producte escalar.</p>
<p>Per veure la segona, sigui <span class="math inline">\(\vec w\in V\cap V^\perp\)</span>. Per tant,
<span class="math inline">\(\vec w\cdot\vec w =\| \vec w\|= 0\)</span>. Però l’únic vector de longitud <span class="math inline">\(0\)</span>
és el vector nul.</p>
<p>Seguidament, observem que <span class="math inline">\(V=\Ima(\proj_V)\)</span> i que
<span class="math inline">\(V^\perp = \Ker(\proj_V)\)</span>. Per tant, per la fórmula del nucli–imatge
(Teorema <a href="espais-vectorials-i-aplicacions-lineals.html#thm:ker-ima">2.5</a>) tenim</p>
<p><span class="math display">\[
\dim(\Ker\proj_V) + \dim(\Ima\proj_V) = \dim(\R^n)=n,
\]</span>
com volíem
veure.</p>
<p>Finalment, per veure que <span class="math inline">\((V^\perp)^\perp = V\)</span>, denotem <span class="math inline">\(W=V^\perp\)</span>.
Aleshores, si <span class="math inline">\(\vec v\in V\)</span>, i <span class="math inline">\(\vec w\in W\)</span>, es té que
<span class="math inline">\(\vec v\cdot \vec w = 0\)</span>, i per tant <span class="math inline">\(\vec v\in W^\perp\)</span>. Concloem doncs
que <span class="math inline">\(V\subseteq (V^\perp)^\perp\)</span>. Però ara observem que
<span class="math inline">\(\dim((V^\perp)^\perp) = n - \dim(V^\perp) = n- (n-\dim(V)) = \dim(V)\)</span>.
Veiem que <span class="math inline">\((V^\perp)^\perp\)</span> és un subespai que conté <span class="math inline">\(V\)</span> i que té la
mateixa dimensió que <span class="math inline">\(V\)</span> i, per tant, ha de coincidir amb <span class="math inline">\(V\)</span>.</p>
</div>
<div class="corollary">
<p><span id="cor:V-Vper" class="corollary"><strong>Corol·lary 4.1  </strong></span>Si <span class="math inline">\(V\subseteq\R^n\)</span> és un subespai
d’<span class="math inline">\(\R^n\)</span>, aleshores <span class="math inline">\(\R^n=V\oplus V^\perp\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-200" class="proof"><em>Prova</em>. </span>Per l’apartat (b) de la proposició anterior, la intersecció és
buida. Per l’apartat (c), la suma de de les dimensions és <span class="math inline">\(n\)</span>. Per tant,
si prenem una base <span class="math inline">\(\calb\)</span> de <span class="math inline">\(V\)</span> i una base <span class="math inline">\(\calc\)</span> de <span class="math inline">\(V^\perp\)</span>, el
conjunt <span class="math inline">\(\calb\cup\calc\)</span> serà linealment independent i tindrà <span class="math inline">\(n\)</span>
elements. Per tant, serà base de <span class="math inline">\(\R^n\)</span>, que és el que volíem veure.</p>
</div>
<p>D’aquest corol·lari es dedueix que, donat <span class="math inline">\(V\)</span> un subespai d’<span class="math inline">\(\R^n\)</span>, tot
vector <span class="math inline">\(\vec w\in\R^n\)</span> es pot escriure de forma única com:</p>
<p><span class="math display">\[
\vec w = \vec w^\parallel + \vec w^\perp
\]</span>
amb
<span class="math inline">\(\vec w^\parallel \in V\)</span> i <span class="math inline">\(\vec w^\perp \in V^\perp\)</span>. A més, tenim que
<span class="math inline">\(\vec w^\perp=\proj_{V^\perp}(\vec w)\)</span>.</p>
</div>
<div id="el-teorema-de-pitàgores" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> El teorema de Pitàgores<a href="ortogonalitat.html#el-teorema-de-pitàgores" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El conegut teorema de Pitàgores es pot formular a <span class="math inline">\(\R^n\)</span>. Fixem-nos que
el teorema té dues implicacions.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-201" class="theorem"><strong>Teorema 4.1  </strong></span>Si <span class="math inline">\(\vec v\)</span> i <span class="math inline">\(\vec w\)</span> són dos vectors d’<span class="math inline">\(\R^n\)</span>, aleshores:</p>
<p><span class="math display">\[
\| \vec v + \vec w \|^2 = \|\vec v\|^2 + \|\vec  w\|^2 \iff \vec v \perp \vec w.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-202" class="proof"><em>Prova</em>. </span>Calculem:</p>
<p><span class="math display">\[
\| \vec v + \vec w\|^2 = (\vec v+\vec w)\cdot (\vec v+\vec w) = \vec v\cdot \vec v + \vec v\cdot \vec w + \vec w\cdot \vec v + \vec w\cdot \vec w=\|\vec v\|^2 + \|\vec w\|^2 +2(\vec v\cdot \vec w).
\]</span></p>
<p>Per tant, la igualtat es compleix si i només si
<span class="math inline">\(\vec v\cdot \vec w = 0\)</span>.</p>
</div>
<div class="corollary">
<p><span id="cor:unlabeled-div-203" class="corollary"><strong>Corol·lary 4.2  </strong></span>Sigui <span class="math inline">\(V\subseteq \R^n\)</span> un subespai. Aleshores</p>
<p><span class="math display">\[
\|\proj_V(\vec w)\| \leq \| \vec w\| \text{ per a tot $\vec w\in\R^n$,}
\]</span></p>
<p>i la desigualtat és una igualtat si i només si <span class="math inline">\(\vec w\in V\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-204" class="proof"><em>Prova</em>. </span>Apliquem el teorema anterior a <span class="math inline">\(\vec w^\parallel\)</span> i
<span class="math inline">\(\vec w^\perp\)</span> (notem <span class="math inline">\(\vec w = \vec w^\parallel + \vec w^\perp\)</span>).</p>
<p><span class="math display">\[
\|\vec w\|^2 = \|\vec w^\parallel\|^2 + \|\vec w^\perp\|^2,
\]</span>
i per
tant
<span class="math display">\[
\|\proj_V(\vec w)\|^2 \leq \|\vec w\|^2,
\]</span>
amb igualtat si i
només si <span class="math inline">\(\|\vec w^\perp\|^2 = 0\)</span>, és a dir si i només si
<span class="math inline">\(\vec w^\perp = 0\)</span>, que és equivalent a <span class="math inline">\(\vec w\in V\)</span>.</p>
</div>
<p>Una aplicació d’aquests resultats és una desigualtat molt famosa,
coneguda com la <em>desigualtat de Cauchy–Schwarz</em>:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-205" class="theorem"><strong>Teorema 4.2  </strong></span>Si <span class="math inline">\(\vec v\)</span> i <span class="math inline">\(\vec w\)</span> són vectors de <span class="math inline">\(\R^n\)</span>, aleshores</p>
<p><span class="math display">\[
|\vec v\cdot \vec w|\leq \|\vec v\|\|\vec w\|,
\]</span>
amb igualtat si i
només si <span class="math inline">\(\vec v\)</span> i <span class="math inline">\(\vec w\)</span> són paral·lels.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-206" class="proof"><em>Prova</em>. </span>Si <span class="math inline">\(\vec v=\vec 0\)</span>, aleshores la desigualtat es compleix
trivialment. En cas contrari, considerem el subespai
<span class="math inline">\(V=\langle \vec v\rangle\)</span>, i apliquem el resultat anterior. Ja havíem
vist la fórmula</p>
<p><span class="math display">\[
\proj_V(\vec w) = \frac{\vec w\cdot \vec v}{\|\vec v\|^2} \vec v,
\]</span>
i
per tant</p>
<p><span class="math display">\[
\|\proj_V(\vec w)\| = \frac{|\vec w\cdot \vec v|}{\|\vec v\|^2} \|\vec v\|,
\]</span></p>
<p>i obtenim
<span class="math display">\[
\frac{|\vec w\cdot \vec v|}{\|\vec v\|} \leq \|\vec w\|,
\]</span></p>
<p>que és la desigualtat que busquem un cop passem <span class="math inline">\(\|\vec v\|\)</span> a l’altra
banda (i utilitzem que <span class="math inline">\(\vec w\cdot \vec v=\vec v\cdot \vec w\)</span>). La
igualtat es dóna si i només si <span class="math inline">\(\vec w\in V\)</span>, que és equivalent a
<span class="math inline">\(\vec v\)</span> essent paral·lel a <span class="math inline">\(\vec w\)</span>.</p>
</div>
<p>Observem que si <span class="math inline">\(\vec v\)</span> i <span class="math inline">\(\vec w\)</span> no són zero, aleshores tenim les
desigualtats</p>
<p><span class="math display">\[
-1 \leq \frac{\vec v\cdot \vec w}{\|\vec v\|\|\vec w\|}\leq 1.
\]</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-207" class="definition"><strong>Definició 4.3  </strong></span>Donats dos vectors no-nuls <span class="math inline">\(\vec v\)</span> i <span class="math inline">\(\vec w\)</span> d’<span class="math inline">\(\R^n\)</span>, <em>l’angle entre
<span class="math inline">\(\vec v\)</span> i <span class="math inline">\(\vec w\)</span></em> és</p>
<p><span class="math display">\[
\theta = \arccos\left(\frac{\vec v\cdot \vec w}{\|\vec v\|\|\vec w\|}\right).
\]</span></p>
</div>
</div>
<div id="mètode-de-gram-schmidt" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Mètode de Gram-Schmidt<a href="ortogonalitat.html#mètode-de-gram-schmidt" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Aquest mètode permet calcular una base ortonormal d’un subespai de
<span class="math inline">\(\R^n\)</span> a partir de projeccions ortogonals. Fixem les notacions següents:</p>
<ul>
<li><p><span class="math inline">\(V\subset \R^n\)</span> un subespai de dimensió <span class="math inline">\(k\)</span> fixat,</p></li>
<li><p><span class="math inline">\(\calb =[\vec v_1, \dots , \vec v_k]\)</span> una base de <span class="math inline">\(V\)</span>,</p></li>
<li><p><span class="math inline">\(V_i=\langle \vec v_1, \dots , \vec v_i\rangle \subset V\)</span> el
subespai (de dimensió <span class="math inline">\(i\)</span>) generat pels <span class="math inline">\(i\)</span> primers vectors de
<span class="math inline">\(\calb\)</span>. Tenim que <span class="math inline">\(V_1\subset V_2\subset \cdots \subset V_k=V\)</span>.</p></li>
<li><p>Si <span class="math inline">\(\vec v_i\in\calb\)</span>, denotarem per <span class="math inline">\(\vec v_i^\parallel\)</span> la
projecció ortogonal de <span class="math inline">\(\vec v_i\)</span> a <span class="math inline">\(V_{i-1}\)</span> i per <span class="math inline">\(\vec v_i^\perp\)</span>
el vector <span class="math inline">\(\vec v_i - \vec v_i^\parallel\)</span>, que és ortogonal a
<span class="math inline">\(V_{i-1}\)</span>.</p></li>
</ul>
<p>El mètode de Gram-Schmidt construeix de forma iterativa una base
ortonormal a cada <span class="math inline">\(V_i\)</span>:</p>
<ul>
<li><p>Considerem primer <span class="math inline">\(V_1=\langle \vec v_1 \rangle\)</span>, i una base és
<span class="math inline">\(\calb_1=[\vec v_1]\)</span>. En aquest cas, definim
<span class="math display">\[
\vec u_1=\frac{1}{\|\vec v_1\|} \vec v_1
\]</span>
i tenim que
<span class="math inline">\(\calc_1=[\vec u_1]\)</span> és una base ortonormal de <span class="math inline">\(V_1\)</span>.</p></li>
<li><p>Considerem ara
<span class="math inline">\(V_2=\langle \vec v_1, \vec v_2 \rangle=\langle \vec u_1, \vec v_2 \rangle\)</span>.
Pel Corol·lari <a href="ortogonalitat.html#cor:V-Vper">4.1</a>, <span class="math inline">\(\vec v_2\)</span> es pot escriure de forma única
com:
<span class="math display">\[
\vec v_2 = \vec v_2^\parallel + \vec v_2^\perp
\]</span>
amb
<span class="math inline">\(\vec v_2^\parallel\in V_1\)</span> i <span class="math inline">\(\vec v_2\perp \in V_1^\perp\)</span>. A més,
sabem com calcular-los:
<span class="math display">\[
\vec v_2^\parallel = \proj_{V_1}(\vec v_2)=(\vec v_2 \cdot \vec u_1)\vec u_1
\]</span></p>
<p>i per tant
<span class="math display">\[
\vec v_2^\perp = \vec v_2 - \vec v_2^\parallel = \vec v_2 - (\vec v_2 \cdot \vec u_1)\vec u_1.
\]</span></p>
<p>Definim <span class="math inline">\(\vec u_2=\frac{1}{\|\vec v_2^\perp\|}\vec v_2^\perp\)</span>, i
tenim que <span class="math inline">\(V_2=\langle \vec u_1, \vec u_2\rangle\)</span>, amb
<span class="math inline">\(\calc_2=[\vec u_1,\vec u_2]\)</span> una base ortonormal.</p></li>
<li><p>Suposem ara que ja tenim
<span class="math inline">\(\calc_{i-1}=[\vec u_1, \dots, \vec u_{i-1}]\)</span> una base ortonormal de
<span class="math inline">\(V_{i-1}\)</span>. Considerem
<span class="math inline">\(V_i=\langle\vec v_1, \dots, \vec v_{i-1},\vec v_i\rangle=\langle\vec u_1, \dots, \vec u_{i-1},\vec v_i\rangle\)</span>,
i escrivim:
<span class="math display">\[
\vec v_i = \vec v_i^\parallel + \vec v_i^\perp
\]</span>
amb
<span class="math inline">\(\vec v_i^\parallel\in V_{i-1}\)</span> i
<span class="math inline">\(\vec v_i^\perp \in V_{i-1}^\perp\)</span>. Com que els vectors <span class="math inline">\(\vec u_i\)</span>
són ortonormals, el càlcul és:
<span class="math display">\[
\vec v_i^\parallel = \proj_{V_{i-1}}(\vec v_i)=(\vec v_i \cdot \vec u_1)\vec u_1+ \cdots + (\vec v_i \cdot \vec u_{i-1})\vec u_{i-1}
\]</span></p>
<p>i per tant
<span class="math display">\[
\vec v_i^\perp = \vec v_i - \vec v_i^\parallel = \vec v_i - (\vec v_i \cdot \vec u_1)\vec u_1- \cdots -(\vec v_i \cdot \vec u_{i-1})\vec u_{i-1}.
\]</span></p>
<p>Definim <span class="math inline">\(\vec u_i=\frac{1}{\|\vec v_i^\perp\|}\vec v_i^\perp\)</span>, i
tenim que <span class="math inline">\(V_i=\langle \vec u_1, \dots ,\vec u_i\rangle\)</span>, amb
<span class="math inline">\(\calc_i=[\vec u_1,\dots,\vec u_i]\)</span> una base ortonormal.</p></li>
</ul>
<div class="example">
<p><span id="exm:GramSchmidt" class="example"><strong>Exemple 4.2  </strong></span>Considerem
<span class="math inline">\(V=\langle \smat{1\\-1\\-1\\1}, \smat{0\\1\\1\\0}\rangle \subset \R^4\)</span> i
volem calcular una base ortonormal.</p>
<p>Comencem amb la base <span class="math inline">\(\calb=[\smat{1\\-1\\-1\\1}, \smat{0\\1\\1\\0}]\)</span> i,
seguint el procediment, <span class="math inline">\(V_1=\langle \smat{1\\-1\\-1\\1}\rangle\)</span> i tant
sols hem de fer-lo unitari:<span class="math inline">\(\|\smat{1\\-1\\-1\\1}\|=2\)</span> i per tant
<span class="math inline">\(V_1=\langle \smat{1\\-1\\-1\\1}\rangle=\langle \smat{1/2\\-1/2\\-1/2\\1/2}\rangle\)</span>.</p>
<p>Calculem ara el segon vector:</p>
<p><span class="math display">\[
\vec v_2^\perp = \smat{0\\1\\1\\0} - (\smat{0\\1\\1\\0}\cdot\smat{1/2\\-1/2\\-1/2\\1/2})\smat{1/2\\-1/2\\-1/2\\1/2}= \smat{0\\1\\1\\0}+\smat{1/2\\-1/2\\-1/2\\1/2}=\smat{1/2\\1/2\\1/2\\1/2}.
\]</span></p>
<p>Llavors
<span class="math inline">\(\vec u_2=\frac{1}{\|\vec v_2^\perp\|} \vec v_2^\perp=\smat{1/2\\1/2\\1/2\\1/2}\)</span>.</p>
<p>Tenim, doncs, que
<span class="math inline">\(\calc=[\smat{1/2\\-1/2\\-1/2\\1/2},\smat{1/2\\1/2\\1/2\\1/2}]\)</span> és una
base ortonormal de <span class="math inline">\(V\)</span>.</p>
<p>A més, la matriu del canvi de base és:</p>
<p><span class="math display">\[
S_{\calb,\calc}=\begin{pmatrix} 2 &amp; -1 \\ 0 &amp; 1\end{pmatrix}
\]</span></p>
</div>
<p>L’algorisme de Gram-Schmidt ens porta a la factorització <span class="math inline">\(QR\)</span> d’una
matriu <span class="math inline">\(A\)</span> (amb les seves columnes linealment independents).</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-208" class="theorem"><strong>Teorema 4.3  </strong></span>Donada una matriu <span class="math inline">\(A\in M_{m\times n}(\R)\)</span> amb les seves columnes
linealment independents, existeixen una matriu <span class="math inline">\(Q\in M_{m\times n}(\R)\)</span>
i <span class="math inline">\(R\in M_{n\times n}\)</span> tals que:</p>
<ul>
<li><p><span class="math inline">\(A=QR\)</span>,</p></li>
<li><p><span class="math inline">\(Q^T Q=\1_n\)</span> (diem que <span class="math inline">\(Q\)</span> és una matriu ortogonal),</p></li>
<li><p><span class="math inline">\(R\)</span> és una matriu triangular superior amb els coeficients de la
diagonal positius.</p></li>
</ul>
<p>A més, aquesta factorització és única.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-209" class="proof"><em>Prova</em>. </span>La demostració és constructiva: anirem calculant els
coeficients de les columnes <span class="math inline">\(Q\)</span> i <span class="math inline">\(R\)</span> aplicant el mètode de
Gram-Schmidt. Fixem abans les notacions següents per a les columnes
d’<span class="math inline">\(A\)</span>, de <span class="math inline">\(Q\)</span> i els coeficients de <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[
A=\begin{pmatrix} | &amp; | &amp; &amp; | \\ \vec v_1 &amp; \vec v_2 &amp; \cdots &amp; \vec v_n \\ | &amp; | &amp; &amp; | \end{pmatrix} ,
Q=\begin{pmatrix} | &amp; | &amp; &amp; | \\ \vec u_1 &amp; \vec u_2 &amp; \cdots &amp; \vec u_n \\ | &amp; | &amp; &amp; | \end{pmatrix}
\text{ i }
R=\begin{pmatrix} r_{11} &amp; r_{12} &amp; \cdots &amp; r_{1n} \\ 0 &amp; r_{22} &amp; \cdots &amp; r_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; r_{nn}\end{pmatrix}.
\]</span></p>
<p>I les matrius formades per les primeres <span class="math inline">\(j\)</span> columnes d’<span class="math inline">\(A\)</span>, <span class="math inline">\(Q\)</span> i la
submatriu quadrada <span class="math inline">\(j\times j\)</span> d’<span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[
A_j=\begin{pmatrix} | &amp; | &amp; &amp; | \\ \vec v_1 &amp; \vec v_2 &amp; \cdots &amp; \vec v_j\\ | &amp; | &amp; &amp; | \end{pmatrix} ,
Q_j=\begin{pmatrix} | &amp; | &amp; &amp; | \\ \vec u_1 &amp; \vec u_2 &amp; \cdots &amp; \vec u_j \\ | &amp; | &amp; &amp; | \end{pmatrix}
\text{ i }
R_j=\begin{pmatrix} r_{11} &amp; r_{12} &amp; \cdots &amp; r_{1j} \\ 0 &amp; r_{22} &amp; \cdots &amp; r_{2j} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; r_{jj}\end{pmatrix}.
\]</span></p>
<p>La construcció comença amb <span class="math inline">\(j=1\)</span>, <span class="math inline">\(j=2\)</span> i veurem com es fa el cas <span class="math inline">\(j\)</span> a
partir del <span class="math inline">\(j-1\)</span>:</p>
<ul>
<li><p>Cas <span class="math inline">\(j=1\)</span>: llavors <span class="math inline">\(A_1=\vec v_1\)</span>: calculem el coeficient
<span class="math inline">\(R_1=(r_{11})=(\|\vec v_1\|)\)</span> i la primera columna de <span class="math inline">\(Q\)</span> com
<span class="math inline">\(Q_1=\vec u_1=\frac{1}{r_{11}}\vec v_1\)</span>. Veiem que, de moment, es
compleix <span class="math inline">\(A_1=Q_1R_1\)</span>. A més, <span class="math inline">\(Q_1^TQ_1=\|\vec u_1\|=1\)</span> i <span class="math inline">\(R_1\)</span> és
triangular superior i amb la diagonal positiva (és un mòdul).</p></li>
<li><p>Cas <span class="math inline">\(j=2\)</span>: considerem <span class="math inline">\(r_{12}=\vec u_1\cdot \vec v_2\)</span>, el vector
<span class="math inline">\(\vec v_2^\perp=\vec v_2-r_{12}u_1\)</span>, <span class="math inline">\(r_{22}=\|\vec v_2^\perp\|\)</span> i
<span class="math inline">\(\vec u_2=\frac{1}{r_{22}} v_2^\perp\)</span>. La primera columna de
<span class="math inline">\(Q_2R_2\)</span> és la mateixa que la de <span class="math inline">\(Q_1R_1=A_1\)</span>. La segona és
<span class="math display">\[
r_{12}u_1+r_{22}\vec u_2=(u_1\cdot v_2)\vec u_1+\|v_2-(\vec u_1\cdot \vec v_2)\vec u_1\|\frac{1}{\|\vec v_2-(\vec u_1\cdot \vec v_2)\vec u_1\|}(\vec v_2-(\vec u_1\cdot \vec v_2)\vec u_1)=\vec v_2
\]</span></p>
<p>i per tant tenim la segona columna de <span class="math inline">\(A\)</span>, llavors <span class="math inline">\(A_2=Q_2R_2\)</span>.
També veiem que <span class="math inline">\(Q_2^TQ_2=\1_2\)</span> (<span class="math inline">\(\vec u_2\)</span> és perpendicular a
<span class="math inline">\(\vec u_1\)</span> i tots dos són unitaris) i <span class="math inline">\(R_2\)</span> és triangular superior i
la diagonal positiva (són mòduls de vectors).</p></li>
<li><p>Suposem que tenim les <span class="math inline">\(j-1\)</span> primeres columnes de <span class="math inline">\(Q\)</span>, i les de <span class="math inline">\(R\)</span>
tal que <span class="math inline">\(A_{j-1}=Q_{j-1}R_{j-1}\)</span>, <span class="math inline">\(Q_{j-1}^TQ_{j-1}=\1_{j-1}\)</span> i
<span class="math inline">\(R_{j-1}\)</span> triangular superior i amb la diagonal positiva. Considerem
<span class="math inline">\(r_{ij}=\vec u_i\cdot \vec v_j\)</span> per a <span class="math inline">\(1\leq i &lt;j\)</span>,
<span class="math inline">\(\vec v_j^\perp=\vec v_j-r_{1j}\vec u_1-\cdots-r_{(j-1)j}\vec u_{j-1}\)</span>,
<span class="math inline">\(r_{jj}=\|\vec v_j^\perp\|\)</span> i
<span class="math inline">\(\vec u_j=\frac{1}{r_{jj}} \vec v_j^\perp\)</span>. Si fem <span class="math inline">\(Q_jR_j\)</span>, les
primeres <span class="math inline">\(j-1\)</span> columnes són les de <span class="math inline">\(Q_{j-1}R_{j-1}\)</span>, per tant són
<span class="math inline">\(A_{j-1}\)</span>. Si calculem la columna <span class="math inline">\(j\)</span>-èssima de <span class="math inline">\(Q_jR_j\)</span>:
<span class="math display">\[
\begin{aligned}
     r_{1j}\vec u_1+\cdots+r_{jj}\vec u_j &amp; =(\vec u_1\cdot \vec v_j)\vec u_1+\cdots+(\vec u_{j-1}\cdot \vec v_j)\vec u_{j-1}+\|\vec v_j^\perp\|\frac{1}{\|\vec v_j^\perp\|} \vec v_j^\perp = \\
     &amp; =(\vec u_1\cdot \vec v_j)\vec u_1+\cdots+(\vec u_{j-1}\cdot \vec v_j)\vec u_{j-1}+ \vec v_j^\perp=\vec v_j .
\end{aligned}
\]</span>
I per tant <span class="math inline">\(A_j=Q_jR_j\)</span>. A més, la primera caixa
<span class="math inline">\((j-1)\times(j-1)\)</span> superior esquerra de <span class="math inline">\(Q_j^TQ_j\)</span> és una
<span class="math inline">\(\1_{j-1}\)</span>. L’última columna i fila són és <span class="math inline">\(\vec u_i\cdot \vec u_j\)</span>,
i per tant, com que <span class="math inline">\(\vec u_j\)</span> és perpendicular als altres
<span class="math inline">\(\vec u_i\)</span> i unitari, tenim que <span class="math inline">\(Q_j^TQ_j=\1_j\)</span>. Finalment, <span class="math inline">\(R_j\)</span> és
triangular superior per construcció, i la diagonal formada per
mòduls, per tant positius.</p></li>
</ul>
<p>Falta veure la unicitat, també per inducció sobre <span class="math inline">\(j\)</span>:</p>
<ul>
<li><p>Cas <span class="math inline">\(j=1\)</span>: com que <span class="math inline">\(R\)</span> és triangular superior, <span class="math inline">\(v_1=r_{11}\vec u_1\)</span>
amb <span class="math inline">\(\|\vec u_1\|=1\)</span>, per tant
<span class="math inline">\(\vec u_1=\frac{\pm1}{\|\vec v_1\|}v_1\)</span>, però com que <span class="math inline">\(r_{11}\)</span> és
positiva positiva, <span class="math inline">\(\vec u_1=\frac{1}{\|\vec v_1\|}\vec v_1\)</span> i
<span class="math inline">\(r_{11}=\|\vec v_1\|\)</span>.</p></li>
<li><p>Suposem que les <span class="math inline">\(j-1\)</span> primeres columnes de <span class="math inline">\(Q\)</span> i de <span class="math inline">\(R\)</span> ja estan
fixades, volem veure que tant sols hi ha una elecció per la
<span class="math inline">\(j\)</span>-èssima: el vector <span class="math inline">\(\vec u_j\)</span> és un vector perpendicular a
l’espai <span class="math inline">\(V_{j-1}=\langle \vec v_1, \dots, \vec v_{j-1}\rangle\)</span> i
contingut a <span class="math inline">\(V_j=\langle \vec v_1, \dots, \vec v_j\rangle\)</span>.
Considerem <span class="math inline">\(V_{j-1}^\perp\)</span> el complement ortogonal de <span class="math inline">\(V_{j-1}\)</span> a
<span class="math inline">\(V_j\)</span>, que és de dimensió <span class="math inline">\(1\)</span>
(<span class="math inline">\(\dim(V_{j-1})+\dim(V_{j-1}^\perp)=\dim(V_j)\)</span>), i conté
<span class="math inline">\(\vec v_j^\perp\)</span>, per tant es té
<span class="math inline">\(V_{j-1}^\perp=\langle v_j^\perp \rangle\)</span>. Llavors, com que ha de
ser unitari, ja <span class="math inline">\(j\)</span>-èssima columna de <span class="math inline">\(Q\)</span> ha de ser
<span class="math inline">\(\vec u_j=\frac{\pm1}{\|\vec v_j^\perp\|}\vec v_j^\perp\)</span>, però com
que <span class="math inline">\(r_{jj}\)</span> és positiva positiva,
<span class="math inline">\(\vec u_j=\frac{1}{\|\vec v_j^\perp\|}\vec v_j^\perp\)</span> i
<span class="math inline">\(r_{jj}=\|\vec v_j^\perp\|\)</span>, i tenim la unicitat de la <span class="math inline">\(\vec u_j\)</span> i
el coeficient <span class="math inline">\(r_{jj}\)</span>. La resta de coeficients de la columna <span class="math inline">\(j\)</span>
d’<span class="math inline">\(R\)</span> estan determinats per ser <span class="math inline">\(R\)</span> triangular superior (per tant,
per sota de <span class="math inline">\(r_{jj}\)</span> són zero) i el fet de que els de sobre són les
coordenades de <span class="math inline">\(\vec v_j-\vec v_j^\perp\)</span> en la base
<span class="math inline">\([\vec u_1,\dots,\vec u_{j-1}]\)</span> de <span class="math inline">\(V_{j-1}\)</span> (i les coordenades en
una base són úniques pel
Teorema <a href="espais-vectorials-i-aplicacions-lineals.html#thm:base-coord-uniq">2.2</a>).</p></li>
</ul>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-210" class="example"><strong>Exemple 4.3  </strong></span>Podem aprofitar els càlculs fets a l’Exemple
<a href="ortogonalitat.html#exm:GramSchmidt">4.2</a> per trobar la factorització <span class="math inline">\(QR\)</span>
següent:
<span class="math display">\[
\begin{pmatrix}
1 &amp; 0 \\ -1 &amp; 1 \\ -1 &amp; 1 \\ 1 &amp; 0
\end{pmatrix}=
\begin{pmatrix}
1/2 &amp; 1/2 \\ -1/2 &amp; 1/2 \\ -1/2 &amp; 1/2 \\ 1/2 &amp; 1/2
\end{pmatrix}
\begin{pmatrix}
2 &amp; -1 \\ 0 &amp; 1
\end{pmatrix}.
\]</span></p>
</div>
</div>
<div id="aplicacions-i-matrius-ortogonals" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Aplicacions i matrius ortogonals<a href="ortogonalitat.html#aplicacions-i-matrius-ortogonals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-211" class="definition"><strong>Definició 4.4  </strong></span>Diem que una aplicació lineal <span class="math inline">\(f \colon \R^n\to \R^m\)</span> és <em>ortogonal</em> si
conserva la longitud dels vectors, o sigui, si</p>
<p><span class="math display">\[
\|f(\vec v)\|=\|\vec v\| \text{ per a tot $\vec v\in\R^n$.}
\]</span>
Si
<span class="math inline">\(f=f_A\)</span> amb <span class="math inline">\(A\in M_{m\times n}(\R)\)</span>, direm que <span class="math inline">\(A\)</span> és una <em>matriu
ortogonal</em>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-212" class="example"><strong>Exemple 4.4  </strong></span>Les rotacions definides a la Secció
<a href="espais-vectorials-i-aplicacions-lineals.html#subsubsec:rotacio">2.2.4</a> són ortogonals: es pot interpretar
geomètricament o bé fent els càlculs: un vector <span class="math inline">\(\smat{x\\y}\)</span> té norma</p>
<p><span class="math display">\[
\|\begin{pmatrix}x \\ y \end{pmatrix}\|=\left(\begin{pmatrix} x &amp; y \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix}\right)^{1/2} = \sqrt{x^2+y^2},
\]</span></p>
<p>i va a parar a:
<span class="math display">\[
\begin{pmatrix}
\cos\theta&amp;-\sin\theta\\
\sin\theta&amp;\cos\theta
\end{pmatrix}
\begin{pmatrix}x \\ y \end{pmatrix} ,
\]</span>
que té norma
<span class="math display">\[
\begin{aligned}
\|f(\smat{x\\y})\|^2 &amp; =\left( \begin{pmatrix}
\cos\theta&amp;-\sin\theta\\
\sin\theta&amp;\cos\theta
\end{pmatrix}
\begin{pmatrix}x \\ y \end{pmatrix}  \right)^T
\begin{pmatrix}
\cos\theta&amp;-\sin\theta\\
\sin\theta&amp;\cos\theta
\end{pmatrix}
\begin{pmatrix}x \\ y \end{pmatrix} = \\
&amp; = \begin{pmatrix}x &amp;  y \end{pmatrix}
\begin{pmatrix}
\cos\theta&amp;\sin\theta\\
-\sin\theta&amp;\cos\theta
\end{pmatrix}
\begin{pmatrix}
\cos\theta&amp;-\sin\theta\\
\sin\theta&amp;\cos\theta
\end{pmatrix}
\begin{pmatrix}x \\ y \end{pmatrix} = \\
&amp; = \begin{pmatrix}x &amp;  y \end{pmatrix}
\begin{pmatrix}
1&amp;0\\
0&amp;1
\end{pmatrix}
\begin{pmatrix}x \\ y \end{pmatrix} = x^2+y^2\\
\end{aligned}
\]</span>
i per tant la rotació és ortogonal.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-213" class="exercise"><strong>Exercici 4.1  </strong></span>Demostreu que reflexions definides a la Secció
<a href="espais-vectorials-i-aplicacions-lineals.html#subsubsec:reflexio">2.2.3</a> són ortogonals.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-214" class="example"><strong>Exemple 4.5  </strong></span>Les projeccions a un subespai <span class="math inline">\(V\subset \R^n\)</span> (<span class="math inline">\(V\neq \R^n)\)</span> no són
ortogonals: considerem <span class="math inline">\(\vec 0 \neq \vec u \in V^\perp\)</span>, llavors
<span class="math inline">\(\proj_V(\vec u)=\vec 0\)</span> i per tant no es conserva la longitud de
<span class="math inline">\(\vec u\)</span>.</p>
</div>
<p>El lema següent ens permet calcular el producte escalar en funció dels
mòduls:</p>
<div class="lemma">
<p><span id="lem:prod-esc-modul" class="lemma"><strong>Lema 4.2  </strong></span>Si <span class="math inline">\(\vec u, \vec v\)</span>
són vectors d’<span class="math inline">\(\R^n\)</span>, llavors:</p>
<p><span class="math display">\[
\vec u \cdot \vec v = \frac{1}{2}\left(\|\vec u + \vec v\|^2 - \|\vec u\|^2 - \|\vec v\|^2\right).
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-215" class="proof"><em>Prova</em>. </span>Considerem <span class="math inline">\(\vec u, \vec v \in \R^n\)</span>. Tenim que:</p>
<p><span class="math display">\[
\begin{aligned}
\|\vec u + \vec v\|^2 &amp; =(\vec u+\vec v)\cdot(\vec u+\vec v) = \vec u \cdot (\vec u+ \vec v) + \vec v \cdot (\vec u+ \vec v) = \\
&amp; = \vec u \cdot \vec u  + \vec u \cdot \vec v + \vec v \cdot \vec u+\vec v \cdot \vec v = \|\vec u\|^2+2 (\vec u \cdot \vec v) + \|\vec v\|^2
\end{aligned}
\]</span>
I per tant:</p>
<p><span class="math display">\[
\vec u \cdot \vec v = \frac{1}{2}\left(\|\vec u + \vec v\|^2 - \|\vec u\|^2 - \|\vec v\|^2\right).
\]</span></p>
</div>
<p>I d’aquí deduïm:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-216" class="theorem"><strong>Teorema 4.4  </strong></span>Una aplicació lineal <span class="math inline">\(f\colon \R^n \to \R^m\)</span> és ortogonal si i només si
<span class="math inline">\(f(\vec u)\cdot f(\vec v)=\vec u \cdot \vec v\)</span> per a tot <span class="math inline">\(\vec u\)</span> i
<span class="math inline">\(\vec v\)</span> de <span class="math inline">\(\R^n\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-217" class="proof"><em>Prova</em>. </span>Si l’aplicació és ortogonal, es compleix que, per a tot
<span class="math inline">\(\vec u,\vec v \in \R^n\)</span>, <span class="math inline">\(\|f(\vec u)\|=\|\vec u\|\)</span> i
<span class="math inline">\(\|f(\vec v)\|=\|\vec v\|\)</span>. Llavors:
<span class="math display">\[
\begin{aligned}
    f(\vec u)\cdot f\vec v) &amp; = \frac{1}{2}\left(\|f(\vec u) +f(\vec v)\|^2 - \|f(\vec u)\|^2 - \|f(\vec v)\|^2\right) \\
    &amp; = \frac{1}{2}\left(\|f(\vec u+\vec v)\|^2 - \|f(\vec u)\|^2 - \|f(\vec v)\|^2\right) \\
     &amp; = \frac{1}{2}\left(\|\vec u + \vec v\|^2 - \|\vec u\|^2 - \|\vec v\|^2\right) = \vec u \cdot \vec v  ,
\end{aligned}
\]</span>
on hem utilitzat el Lema
<a href="ortogonalitat.html#lem:prod-esc-modul">4.2</a> dues vegades i que <span class="math inline">\(f\)</span> és lineal.</p>
<p>Si <span class="math inline">\(f(\vec u)\cdot f(\vec v)=\vec u \cdot \vec v\)</span> per a tot <span class="math inline">\(\vec u\)</span> i
<span class="math inline">\(\vec v\)</span> de <span class="math inline">\(\R^n\)</span>, com que <span class="math inline">\(\|\vec u\|^2=\vec u \cdot \vec u\)</span>, tenim
que, agafant <span class="math inline">\(\vec u=\vec v\)</span>, <span class="math inline">\(\|f(\vec u)\|^2=\|\vec u\|^2\)</span> i per tant
<span class="math inline">\(f\)</span> és ortogonal.</p>
</div>
<div class="corollary">
<p><span id="cor:unlabeled-div-218" class="corollary"><strong>Corol·lary 4.3  </strong></span>Si <span class="math inline">\(f = f_A \colon \R^n \to \R^m\)</span> és una aplicació lineal i
<span class="math inline">\(\vec e_1, \dots, \vec e_n\)</span> és la base estàndard d’<span class="math inline">\(\R^n\)</span>, llavors:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(f\)</span> és ortogonal si i només si <span class="math inline">\(f(\vec e_1), \dots, f(\vec e_n)\)</span> són
vectors ortonormals. En particular, si <span class="math inline">\(f\)</span> és ortogonal,
<span class="math inline">\(f(\vec e_1), \dots, f(\vec e_n)\)</span> són linealment independents i si
<span class="math inline">\(n=m\)</span>, són una base ortonormal.</p></li>
<li><p><span class="math inline">\(f_A\)</span> és una aplicació lineal ortogonal si i només si <span class="math inline">\(A\)</span> és una
matriu ortogonal.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-219" class="proof"><em>Prova</em>. </span>Vegem primer (a). Si <span class="math inline">\(f\)</span> és ortogonal, del fet que
<span class="math inline">\(f(\vec e_i)\cdot f(\vec e_j)=\vec e_i\cdot \vec e_j\)</span>, es dedueix que
<span class="math inline">\(f(\vec e_i)\)</span> són unitaris i ortogonals dos a dos, per tant,
ortonormals.</p>
<p>Suposem ara que <span class="math inline">\(f(\vec e_i)\)</span> són unitaris i ortogonals dos a dos i
unitaris i <span class="math inline">\(\vec u \in \R^n\)</span>. Tenim que existeixen nombres reals
<span class="math inline">\(\lambda_i\)</span> tals que
<span class="math inline">\(\vec u= \lambda_1\vec e_1+ \cdots + \lambda_n\vec e_n\)</span>. Llavors
<span class="math inline">\(\|u\|^2=\lambda_1^2+ \cdots + \lambda_n^2\)</span>, i
<span class="math inline">\(\|f(\vec u)\|^2=\|\lambda_1 f(\vec e_1)+\cdots + \lambda_n f(\vec e_n)\|^2=\lambda_1^2+ \cdots + \lambda_n^2\)</span>
(pel Teorema de Pitàgores, utilitzant que són ortogonals i unitaris).</p>
<p>Vegem ara que els <span class="math inline">\(f(\vec e_1)\)</span> són linealment independents: més en
general, tenim que si <span class="math inline">\(\vec u_1, \dots, \vec u_k\)</span> són vectors
ortonormals, llavors són linealment independents: del teorema de
Pitàgores es dedueix que:</p>
<p><span class="math display">\[
\|\lambda_1\vec u_1 + \cdots + \lambda_k \vec u_k\|^2=\lambda_1^2 + \cdots + \lambda_k^2
\]</span></p>
<p>I per tant si tenim una combinació
<span class="math inline">\(\vec 0 = \lambda_1\vec u_1 + \cdots + \lambda_k \vec u_k\)</span>, tots els
<span class="math inline">\(\lambda\)</span> han de ser zero. Llavors tenim que, en el nostre cas,
<span class="math inline">\(f(\vec e_1), \dots, f(\vec e_n)\)</span> són linealment independents.</p>
<p>Si <span class="math inline">\(n=m\)</span>, <span class="math inline">\(f(\vec e_1)\)</span>, …, <span class="math inline">\(f(\vec e_m)\)</span> són <span class="math inline">\(m\)</span> vectors de <span class="math inline">\(\R^m\)</span>
linealment independents, i per tant, base.</p>
<p>Per a veure (b), si <span class="math inline">\(f_A\)</span> és ortogonal i fem el producte <span class="math inline">\(A^TA\)</span>, tenim
que el coeficient <span class="math inline">\((i,j)\)</span> és <span class="math inline">\(f(\vec e_i)\cdot f(\vec e_j)\)</span>, per tant,
com que són ortonormals, tenim <span class="math inline">\(A^TA=\1_n\)</span>.</p>
<p>Les columnes d’<span class="math inline">\(A\)</span> són els vectors <span class="math inline">\(f(\vec e_j)\)</span>, per tant, <span class="math inline">\(A^TA=\1_n\)</span>
si i només si <span class="math inline">\(f(\vec e_1), \dots, f(\vec e_n)\)</span> són ortogonals i
unitaris (per tant <span class="math inline">\(f\)</span> és ortogonal).</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-220" class="theorem"><strong>Teorema 4.5  </strong></span>Si <span class="math inline">\(A,B \in M_n(\R)\)</span> són matrius ortogonals, llavors:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(AB\)</span> també és ortogonal i</p></li>
<li><p><span class="math inline">\(A^{-1}\)</span> és ortogonal (i <span class="math inline">\(A^{-1}=A^T)\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-221" class="proof"><em>Prova</em>. </span><span class="math inline">\(AB\)</span> serà ortogonal si <span class="math inline">\(f_{AB}\)</span> conserva la longitud dels
vectors, però <span class="math inline">\(f_{AB}=f_A \circ f_B\)</span> i, per hipòtesi, tant <span class="math inline">\(f_B\)</span> com
<span class="math inline">\(f_A\)</span> conserven la longitud, per tant, <span class="math inline">\(f_{AB}\)</span> també.</p>
<p>Per a veure (b): si <span class="math inline">\(\vec v = f_{A^{-1}}(\vec w))\)</span>, llavors
<span class="math inline">\(\vec w=f_A(\vec v)\)</span> i, per hipòtesi
<span class="math inline">\(\|\vec w\|=\|f_A(\vec v)\|=\|\vec v\|\)</span>, per tant <span class="math inline">\(A^{-1}\)</span> és
ortogonal.</p>
</div>
</div>
<div id="matriu-duna-projecció-ortogonal-en-una-base-ortonormal" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Matriu d’una projecció ortogonal en una base ortonormal<a href="ortogonalitat.html#matriu-duna-projecció-ortogonal-en-una-base-ortonormal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="proposition">
<p><span id="prp:unlabeled-div-222" class="proposition"><strong>Proposició 4.2  </strong></span>Si <span class="math inline">\(V\subset \R^n\)</span> un subespai vectorial, <span class="math inline">\([\vec v_1, \dots, \vec v_k]\)</span>
és una base ortonormal de <span class="math inline">\(V\)</span> i <span class="math inline">\(Q\)</span> és la matriu que té per columnes els
vectors <span class="math inline">\(\vec v_j\)</span>, llavors, la matriu de la projecció ortogonal a <span class="math inline">\(V\)</span>
és:
<span class="math display">\[
[\proj_V]=Q Q^T.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-223" class="proof"><em>Prova</em>. </span>Com que <span class="math inline">\([\vec v_1, \dots, \vec v_k]\)</span> és una base ortonormal de
<span class="math inline">\(V\)</span>, la projecció ortogonal a <span class="math inline">\(V\)</span> del vector <span class="math inline">\(\vec u\)</span> és:</p>
<p><span class="math display">\[
\proj_V(\vec u)=(\vec u\cdot \vec v_1)\vec v_1 + \cdots + (\vec u\cdot \vec v_k)\vec v_k = Q \begin{pmatrix}
\vec u\cdot \vec v_1 \\ \vdots \\ \vec u\cdot \vec v_k \end{pmatrix}= Q \begin{pmatrix}
\vec v_1^T \vec u \\ \vdots \\ \vec v_k^T \vec u
\end{pmatrix} = Q
\begin{pmatrix}
\vec v_1^T \\
\vdots \\
\vec v_k^T
\end{pmatrix} \vec u
=(QQ^T)\vec u
\]</span>
I per tant la matriu associada a la projecció és
<span class="math inline">\(QQ^T\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-224" class="example"><strong>Exemple 4.6  </strong></span>Considerem el vector <span class="math inline">\(\smat{x\\y}\neq\smat{0\\0}\)</span> de <span class="math inline">\(\R^2\)</span> i calculem
la matriu de la projecció a <span class="math inline">\(V=\langle \smat{x\\y}\rangle\)</span>. Una base
ortonormal és <span class="math inline">\(\vec v_1=\frac{1}{\sqrt{x^2+y^2}}\smat{x\\y}\)</span> i per tant
la matriu és (compareu-ho amb la Proposició
<a href="espais-vectorials-i-aplicacions-lineals.html#prp:projR2">2.3</a>):</p>
<p><span class="math display">\[
[\proj_V]=\frac{1}{\sqrt{x^2+y^2}} \begin{pmatrix} x \\ y \end{pmatrix} \frac{1}{\sqrt{x^2+y^2}} \begin{pmatrix} x &amp; y \end{pmatrix} = \frac{1}{x^2+y^2} \begin{pmatrix} x^2 &amp; xy \\ xy &amp; y^2 \end{pmatrix} .
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-225" class="example"><strong>Exemple 4.7  </strong></span>Volem calcular la matriu de la projecció ortogonal de <span class="math inline">\(\R^4\)</span> a
<span class="math inline">\(V=\langle \smat{1\\-1\\-1\\1}, \smat{0\\1\\1\\0}\rangle \subset \R^4\)</span>.
A l’Exemple
<a href="ortogonalitat.html#exm:GramSchmidt">4.2</a> ja hem calcular una base ortonormal, i
era <span class="math inline">\(\calc=[\smat{1/2\\-1/2\\-1/2\\1/2},\smat{1/2\\1/2\\1/2\\1/2}]\)</span>, per
tant la matriu corresponent a la projecció ortogonal és:</p>
<p><span class="math display">\[
\begin{pmatrix}
1/2 &amp; 1/2 \\ -1/2 &amp; 1/2 \\ -1/2 &amp; 1/2 \\ 1/2 &amp; 1/2
\end{pmatrix}
\begin{pmatrix}
1/2 &amp; -1/2 &amp; -1/2 &amp; 1/2 \\ 1/2 &amp; 1/2 &amp; 1/2 &amp; 1/2
\end{pmatrix}
\begin{pmatrix}
1/2 &amp; 0 &amp; 0 &amp; 1/2 \\
0 &amp; 1/2 &amp; 1/2 &amp; 0 \\
0 &amp; 1/2 &amp; 1/2 &amp; 0 \\
1/2 &amp; 0 &amp; 0 &amp; 1/2
\end{pmatrix}.
\]</span></p>
</div>
</div>
<div id="mínims-quadrats" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Mínims quadrats<a href="ortogonalitat.html#mínims-quadrats" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En aquesta secció suposarem que tenim un subespai <span class="math inline">\(V\subset\R^n\)</span> fixat.
A cada element d’<span class="math inline">\(\R^n\)</span>, volem assignar-li un element de <span class="math inline">\(V\)</span> que “millor
l’aproxima”, en el sentit que la norma de l’error que produïm és mínima.</p>
<div class="proposition">
<p><span id="prp:proj-com-aprox" class="proposition"><strong>Proposició 4.3  </strong></span>Si <span class="math inline">\(V \subset \R^n\)</span>
és un subespai, llavors, per a tot <span class="math inline">\(\vec w \in \R^n\)</span> i <span class="math inline">\(\vec v\in V\)</span> es
compleix:
<span class="math display">\[
\|\vec w - \vec v\| \geq \|\vec w - \proj_V(\vec w)\|.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-226" class="proof"><em>Prova</em>. </span>Tenim que
<span class="math inline">\(\vec w - \vec v = (\vec w - \proj_V(w)) + (\proj_V(w) - \vec v)\)</span> amb el
primer vector de <span class="math inline">\(\vec V^\perp\)</span> i el segon de <span class="math inline">\(V\)</span>. Aplicant el Teorema
de Pitàgores tenim:</p>
<p><span class="math display">\[
\|\vec w - \vec v\|^2= \|\vec w - \proj_V(\vec w)\|^2 + \|\proj_V(\vec w) - \vec v\|^2 \geq \|\vec w - \proj_V(\vec w)\|^2 ,
\]</span></p>
<p>i tenim la desigualtat que volem.</p>
</div>
<p>Per tant la millor aproximació d’un vector <span class="math inline">\(\vec w\)</span> és la projecció
ortogonal de <span class="math inline">\(\vec w\)</span>, si entenem per “millor” aquell vector de <span class="math inline">\(V\)</span> que
està a menor distància del vector <span class="math inline">\(\vec w\)</span>.</p>
<div id="recta-de-regressió" class="section level3 hasAnchor" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Recta de regressió<a href="ortogonalitat.html#recta-de-regressió" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suposem que tenim un núvol de punts <span class="math inline">\((x_1,y_1), \dots , (x_k,y_k)\)</span> de
<span class="math inline">\(\R^2\)</span> que, en principi, no estan alineats. Suposem, a més, que hi ha
una dependència del tipus <span class="math inline">\(y=f(x)\)</span> (per tant, al núvol de punts seria un
problema que hi hagués dos punts amb la mateixa <span class="math inline">\(x\)</span> i diferents <span class="math inline">\(y\)</span>).
Suposem que els punts estan <em>aproximadament</em> alineats i que el que volem
trobar és la recta <span class="math inline">\(y=a_0+a_1x\)</span> que aproxima millor aquests punts.</p>
<p>Una altra manera de pensar-ho és que volem resoldre el sistema
d’equacions amb incògnites <span class="math inline">\(a_0\)</span> i <span class="math inline">\(a_1\)</span>:
<span class="math display">\[
\begin{aligned}
a_ 0 + x_1 a_1 &amp; = y_1 \\ &amp; \vdots  \\ a_0 + x_k a_1 &amp; = y_k
\end{aligned}
\]</span>
O, en forma matricial:</p>
<p><span class="math display">\[
\begin{pmatrix} 1 &amp; x_1 \\ \vdots &amp; \vdots \\ 1 &amp; x_k \end{pmatrix}
\begin{pmatrix} a_0 \\ a_1 \end{pmatrix} =
\begin{pmatrix} y_1 \\ \vdots \\ y_k \end{pmatrix}
\]</span>
Com que molt
probablement els punts no estan alineats, com a sistema d’equacions, és
un sistema incompatible. Dit d’una altra manera, el vector
<span class="math inline">\(\vec y=\smat{y_1 \\ \vdots \\ y_k}\)</span> no pertany al subespai
<span class="math inline">\(V=\langle \smat{1 \\ \vdots \\ 1} , \smat{x_1 \\ \vdots \\ x_k} \rangle\)</span>.
Si apliquem la Proposició
<a href="ortogonalitat.html#prp:proj-com-aprox">4.3</a>, el vector de <span class="math inline">\(V\)</span> que serà més proper a
<span class="math inline">\(\vec y\)</span> serà <span class="math inline">\(\proj_V(\vec y)\)</span>:</p>
<p><span class="math display">\[
\proj_V(\begin{pmatrix} y_1 \\ \vdots \\ y_k \end{pmatrix})= a_0 \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} + a_1 \begin{pmatrix} x_1 \\ \vdots \\ x_k \end{pmatrix}= \begin{pmatrix} 1 &amp; x_1 \\ \vdots &amp; \vdots \\ 1 &amp; x_k \end{pmatrix}
\begin{pmatrix} a_0 \\ a_1 \end{pmatrix} .
\]</span>
Podríem calcular <span class="math inline">\(a_0\)</span> i
<span class="math inline">\(a_1\)</span> fent primer Gram-Schmidt per a trobar una base ortonormal de <span class="math inline">\(V\)</span> i
fent els productes escalars corresponents, o bé fent les consideracions
següents: el vector</p>
<p><span class="math display">\[
\begin{pmatrix} y_1 \\ \vdots \\ y_k \end{pmatrix} -
\begin{pmatrix} 1 &amp; x_1 \\ \vdots &amp; \vdots \\ 1 &amp; x_k \end{pmatrix}
\begin{pmatrix} a_0 \\ a_1 \end{pmatrix}
\]</span>
és perpendicular a <span class="math inline">\(V\)</span>, per
tant ha de complir que:
<span class="math display">\[
\begin{pmatrix}
1 &amp; \cdots &amp; 1 &amp; \\ x_1 &amp; \cdots &amp; x_k
\end{pmatrix}
\left(
\begin{pmatrix} y_1 \\ \vdots \\ y_k \end{pmatrix} -
\begin{pmatrix} 1 &amp; x_1 \\ \vdots &amp; \vdots \\ 1 &amp; x_k \end{pmatrix}
\begin{pmatrix} a_0 \\ a_1 \end{pmatrix}\right) = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\]</span></p>
<p>I per tant:
<span class="math display">\[
\begin{pmatrix}
1 &amp; \cdots &amp; 1 &amp; \\ x_1 &amp; \cdots &amp; x_k
\end{pmatrix}
\begin{pmatrix} 1 &amp; x_1 \\ \vdots &amp; \vdots \\ 1 &amp; x_k \end{pmatrix}
\begin{pmatrix} a_0 \\ a_1 \end{pmatrix} =
\begin{pmatrix}
1 &amp; \dots &amp; 1 &amp; \\ x_1 &amp; \dots &amp; x_k
\end{pmatrix}
\begin{pmatrix} y_1 \\ \vdots \\ y_k \end{pmatrix}
\]</span>
I queda el sistema
compatible determinat:
<span class="math display">\[
\begin{pmatrix}
k &amp; \sum x_i \\ \sum x_i &amp; \sum x_i^2
\end{pmatrix}
\begin{pmatrix} a_0 \\ a_1 \end{pmatrix} =
\begin{pmatrix} \sum y_i \\ \sum x_iy_i \end{pmatrix}
\]</span>
Que es pot
escriure: si <span class="math inline">\(\overline x = \frac{1}{k} \sum x_i\)</span>,
<span class="math inline">\(\overline y=\frac{1}{k} \sum y_i\)</span>,
<span class="math inline">\(\overline {x^2}= \frac{1}{k} \sum x_i^2\)</span> i
<span class="math inline">\(\overline {xy}= \frac{1}{k} \sum x_iy_i\)</span>:
<span class="math display">\[
\begin{pmatrix}
1 &amp; \overline x \\ \overline x &amp; \overline{x^2}
\end{pmatrix}
\begin{pmatrix} a_0 \\ a_1 \end{pmatrix} =
\begin{pmatrix}  \overline y \\ \overline{xy} \end{pmatrix}
\]</span>
I la
solució es pot escriure com:
<span class="math display" id="eq:recta-reg">\[
\begin{aligned}
\tag{4.1}
a_1=\frac{\overline{xy} - \overline x \, \overline y}{\overline{x^2} - (\overline x)^2}
\text{ i }
a_0= \overline y - a_1 \overline x .
\end{aligned}
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-227" class="example"><strong>Exemple 4.8  </strong></span>Considerem les notes següents d’un grup de <span class="math inline">\(21\)</span> alumnes corresponent a
una avaluació parcial i la nota final que van treure:</p>
<p><span class="math display">\[
\begin{array}{|c|c||c|c||c|c|}
\hline\text{Parcial} &amp; \text{Final} &amp; \text{Parcial} &amp; \text{Final} &amp; \text{Parcial} &amp; \text{Final}  \\ \hline
8.15 &amp; 8.995 &amp; 6.25 &amp; 7.845 &amp; 2.2 &amp; 4.58 \\
0.3 &amp; 1.98 &amp; 5.85 &amp; 7.075 &amp; 4.6 &amp;4.95 \\
4.3&amp;5.01 &amp; 3.55 &amp; 5.715 &amp; 4.7 &amp; 7.08\\
1.8&amp;2.88 &amp; 1.7 &amp; 2.82 &amp; 2.3 &amp; 3.86\\
5.4&amp;5.08 &amp; 4.6 &amp; 7.13 &amp; 2.7 &amp;5.33 \\
0.2 &amp;2.78 &amp; 7.7 &amp; 8.8 &amp; 7.95 &amp;8.595\\
5.4 &amp; 7.77 &amp; 3.15 &amp;1.245 &amp; 3 &amp;4.95\\ \hline
\end{array}
\]</span>
I volem aproximar la nota final a partir de la del parcial
amb una recta:
<span class="math display">\[
\text{final}= a_0 + a_1 \text{parcial}
\]</span>
Per tant,
podem aplicar la Fórmula
<a href="ortogonalitat.html#eq:recta-reg">(4.1)</a> als valors (aproximem a 3 decimals):</p>
<p><span class="math display">\[
\overline{x}=\overline{\text{parcial}}= 4.085, \quad
\overline{y}=\overline{\text{final}}= 5.451, \quad
\overline{x^2}=21,839 \text{ i }
\overline{xy}= 26,816.
\]</span>
I queda (amb 3 decimals):</p>
<p><span class="math display">\[
\text{final}= 1.843 + 0.883 \text{parcial}.
\]</span>
La Figura
<a href="ortogonalitat.html#fig:recta-reg" reference-type="ref" reference="fig:recta-reg">1</a> té
una representació gràfica d’aquestes dades i de la recta de regressió. A
més, l’equació de la recta es pot utilitzar per aproximar una nota final
a partir d’una parcial: per exemple, un alumne que hagi tret un <span class="math inline">\(4\)</span> al
parcial, la predicció diria que trauria un <span class="math inline">\(5,376\)</span> a la nota final.</p>
<figure id="fig:recta-reg">
<figcaption>
Recta de regressió<span id="fig:recta-reg" label="fig:recta-reg"></span>
</figcaption>
</figure>
</div>
</div>
<div id="cas-general" class="section level3 hasAnchor" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> Cas general<a href="ortogonalitat.html#cas-general" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En el cas general, suposem que tenim un sistema d’equacions lineals:</p>
<p><span class="math display">\[
A x = b
\]</span>
amb <span class="math inline">\(A\in M_{m\times n}(\R)\)</span> amb <span class="math inline">\(m\geq n\)</span> (més equacions
que incògnites) i tal que <span class="math inline">\(\Rang(A)=n\)</span>. És molt probable que aquest
sistema no tingui solució, i per aquests casos ens interessa tenir la
millor aproximació:</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-228" class="lemma"><strong>Lema 4.3  </strong></span>Amb les hipòtesis anteriors, el sistema
<span class="math display">\[
A^T A x = A^T b
\]</span>
té solució
única. Aquesta solució s’anomena <em>solució per mínim quadrats del sistema
<span class="math inline">\(Ax=b\)</span></em>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-229" class="proof"><em>Prova</em>. </span>Observem que <span class="math inline">\(A^TA \in M_{n\times n}(\R)\)</span>, pel que tant sols
cal veure que <span class="math inline">\(\Rang(A^T A)=n\)</span> (i llavors serà un sistema compatible
determinat), o el que és el mateix, que l’aplicació lineal <span class="math inline">\(f_{A^TA}\)</span> és
injectiva (llavors el sistema homogeni que té per matriu associada
<span class="math inline">\(A^TA\)</span> serà compatible determinat i per tant <span class="math inline">\(\Rang(A^T A)=n\)</span>).</p>
<p>Ara bé, si <span class="math inline">\(\vec u \in \R^n\)</span> compleix <span class="math inline">\(A^TA\vec u = \vec 0\)</span>, tenim que
el vector <span class="math inline">\(\vec v=A\vec u\)</span> pertany a <span class="math inline">\(\ker A^T=(\Ima A)^\perp\)</span> i també a
<span class="math inline">\(\Ima A\)</span>. Com que <span class="math inline">\((\Ima A)\cap (\Ima A)^\perp=\{\vec 0\}\)</span>, deduïm que
<span class="math inline">\(\vec v = \vec 0\)</span>. Aplicant ara que <span class="math inline">\(\Rang(A)=n\)</span>, tenim que
<span class="math inline">\(\vec u=\vec 0\)</span> i per tant <span class="math inline">\(\Rang(A^TA)=n\)</span>.</p>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-230" class="lemma"><strong>Lema 4.4  </strong></span>Continuem amb les hipòtesis anteriors. De tots els vectors
<span class="math inline">\(\vec v \in \R^n\)</span>, la solució per mínims quadrats del sistema <span class="math inline">\(Ax=b\)</span> és
la que és més propera a <span class="math inline">\(b\)</span> en el sentit següent: si <span class="math inline">\(x^*\)</span> és la solució
del sistema <span class="math inline">\(A^TAx^*=A^Tb\)</span> i <span class="math inline">\(\vec v\in \R^n\)</span>, llavors</p>
<p><span class="math display">\[
\|Ax^* - b\|^2 \leq \|A\vec v- b\|^2.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-231" class="proof"><em>Prova</em>. </span>Es dedueix del fet que la solució per mínims quadrats sigui la
projecció ortogonal de <span class="math inline">\(b\)</span> a l’espai generat per les columnes d’<span class="math inline">\(A\)</span>,
que, com que el rang d’<span class="math inline">\(A\)</span> és <span class="math inline">\(n\)</span>, queda caracteritzada per complir:</p>
<p><span class="math display">\[
A^T(Ax-b)=\vec 0 .
\]</span></p>
</div>
<p>D’aquí també es dedueix que la projecció del vector <span class="math inline">\(b\)</span> a <span class="math inline">\(V\)</span>, el
subespai generat per les columnes d’<span class="math inline">\(A\)</span> és, en la base de <span class="math inline">\(V\)</span> formada
per les columnes d’<span class="math inline">\(A\)</span> és:
<span class="math display">\[
x=(A^TA)^{-1}A^Tb
\]</span>
Per tant, el vector
d’<span class="math inline">\(\R^n\)</span> és:
<span class="math display">\[
Ax=A(A^TA)^{-1}A^Tb
\]</span>
I per tant:</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-232" class="proposition"><strong>Proposició 4.4  </strong></span>Si <span class="math inline">\(V\subset \R^n\)</span> és un subespai vectorial i
<span class="math inline">\([\vec v_1, \dots , \vec v_k]\)</span> és una base de <span class="math inline">\(V\)</span>, la projecció de
<span class="math inline">\(\R^n\)</span> a <span class="math inline">\(V\)</span> ve donada per la matriu:
<span class="math display">\[
[\proj_V]=A(A^TA)^{-1}A^T
\]</span>
on
<span class="math inline">\(A\)</span> és la matriu que té per columnes els vectors <span class="math inline">\(\vec v_j\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-233" class="example"><strong>Exemple 4.9  </strong></span>Suposem que tenim un grup de 24 estudiants als que s’ha fet una
avaluació parcial i un lliurament a mig semestre amb les notes següents.
La tercera columna diu quina ha sigut la nota final de l’assignatura
(amb més avaluacions entremig):
<span class="math display">\[
\begin{array}{|c|c|c||c|c|c|}
\hline \text{Lliurament} &amp; \text{Parcial} &amp; \text{Final} &amp;
\text{Lliurament} &amp; \text{Parcial} &amp; \text{Final}\\ \hline
9.25 &amp; 9.75 &amp; 8.25 &amp;
9.5 &amp; 5.2 &amp; 7.27 \\
9.5 &amp; 2.4 &amp; 3.66 &amp;
9 &amp; 7.1 &amp; 8.32 \\
8 &amp; 1.6 &amp; 5.65 &amp;
8 &amp; 4 &amp; 7.61 \\
8 &amp; 7.1 &amp; 6.71 &amp;
8.5 &amp; 3.4 &amp; 6.77 \\
8.5 &amp; 5.7 &amp; 7.77 &amp;
8 &amp; 6 &amp; 7.74 \\
6 &amp; 2.9 &amp; 6.5 &amp;
10 &amp; 5.9 &amp; 8.37 \\
9 &amp; 9 &amp; 8.13 &amp;
10 &amp; 2.6 &amp; 8.13 \\
8 &amp; 7.6 &amp; 7.15 &amp;
7.5 &amp; 1.6 &amp; 5.21 \\
9 &amp; 6.2 &amp; 6.8 &amp;
10 &amp; 9 &amp; 8.52 \\
10 &amp; 10 &amp; 9.07 &amp;
9 &amp; 8 &amp; 7.76 \\
9 &amp; 6.5 &amp; 7.43 &amp;
9.75 &amp; 10 &amp; 8.71 \\
8.5 &amp; 4 &amp; 6.39 &amp;
9.5 &amp; 7.4 &amp; 8.57 \\ \hline
\end{array}
\]</span>
Volem aproximar la nota final a partir de les dues que se
saben a mig semestre mitjançant una formula:</p>
<p><span class="math display">\[
\text{final} = a \times \text{Lliurament} + b \times \text{Parcial} + c
\]</span></p>
<p>I per tant, fem mínim quadrats per trobar <span class="math inline">\(a,b\)</span> i <span class="math inline">\(c\)</span>. En aquest cas, el
sistema que té moltes més equacions que incògnites seria:</p>
<p><span class="math display">\[
\begin{pmatrix}
9.25 &amp; 9.75 &amp; 1 \\
9.5 &amp; 2.4 &amp; 1 \\
8 &amp; 1.6 &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots \\
9.5 &amp; 7.4 &amp; 1
\end{pmatrix}
\begin{pmatrix} a \\ b \\ c \end{pmatrix}=
\begin{pmatrix} 8.25 \\ 3.66 \\ 5.65 \\ \vdots \\ 8.57 \end{pmatrix}
\]</span>
I
per tant hem de resoldre el sistema compatible determinat:</p>
<p><span class="math display">\[
\begin{pmatrix}
9.25 &amp; 9.5 &amp; 8 &amp; \cdots &amp; 9.5 \\
9.75 &amp; 2.4 &amp; 1.6 &amp; \cdots &amp; 7.4 \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1
\end{pmatrix}
\begin{pmatrix}
9.25 &amp; 9.75 &amp; 1 \\
9.5 &amp; 2.4 &amp; 1 \\
8 &amp; 1.6 &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots \\
9.5 &amp; 7.4 &amp; 1
\end{pmatrix}
\begin{pmatrix} a \\ b \\ c \end{pmatrix}=
\begin{pmatrix}
9.25 &amp; 9.5 &amp; 8 &amp; \cdots &amp; 9.5 \\
9.75 &amp; 2.4 &amp; 1.6 &amp; \cdots &amp; 7.4 \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1
\end{pmatrix}
\begin{pmatrix} 8.25 \\ 3.66 \\ 5.65 \\ \vdots \\ 8.57 \end{pmatrix}
\]</span></p>
<p>Quedant:
<span class="math display">\[
\left(\begin{array}{ccc}
1885.375 &amp; 1287.5375 &amp; 211.5 \\
1287.5375 &amp; 1015.0425 &amp; 142.95 \\
211.5 &amp; 142.95 &amp; 24
\end{array}\right)
\begin{pmatrix} a \\ b \\ c \end{pmatrix}=
\begin{pmatrix}
1568.205 \\ 1108.1755 \\ 176.49
\end{pmatrix}
\]</span>
Que té per solució (amb 3 decimals):</p>
<p><span class="math display">\[
\begin{pmatrix} a \\ b \\ c \end{pmatrix}=
\begin{pmatrix} 0.191 \\
0.316 \\
3.790 \end{pmatrix}
\]</span>
Per tant l’aproximació és:</p>
<p><span class="math display">\[
\text{final} = 0.191 \times \text{Lliurament} + 0.316 \times \text{Parcial} + 3.79\, .
\]</span></p>
<p>Per exemple, d’un alumne que tregui un <span class="math inline">\(4\)</span> al lliurament i un altre <span class="math inline">\(4\)</span>
al parcial, aquest model prediu que al final trauria un <span class="math inline">\(5.818\)</span></p>
</div>
</div>
</div>
<div id="formes-bilineals-i-productes-escalars" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Formes bilineals i productes escalars<a href="ortogonalitat.html#formes-bilineals-i-productes-escalars" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-234" class="definition"><strong>Definició 4.5  </strong></span>Donat un espai vectorial <span class="math inline">\(E\)</span> sobre un cos <span class="math inline">\(\K\)</span>, una <em>forma bilineal</em> és
una aplicació
<span class="math display">\[
\phi \colon E \times E \to \K
\]</span>
tal que:</p>
<ul>
<li><p><span class="math inline">\(\phi(\vec u_1+\vec u_2,v)=\phi(\vec u_1,\vec v)+\phi(\vec u_2,\vec v)\)</span>
per a tots <span class="math inline">\(\vec u_1\)</span>, <span class="math inline">\(\vec u_2\)</span> i <span class="math inline">\(\vec v\)</span> d’<span class="math inline">\(E\)</span>,</p></li>
<li><p><span class="math inline">\(\phi(\lambda \vec u,\vec v)=\lambda \phi(\vec u,\vec v)\)</span> per a tots
<span class="math inline">\(\vec u\)</span> i <span class="math inline">\(\vec v\)</span> d’<span class="math inline">\(E\)</span> i <span class="math inline">\(\lambda \in \K\)</span>,</p></li>
<li><p><span class="math inline">\(\phi(\vec u,\vec v_1,\vec v_2)=\phi(\vec u,\vec v_1)+\phi(\vec u,\vec v_2)\)</span>
per a tots <span class="math inline">\(\vec u\)</span>, <span class="math inline">\(\vec v_1\)</span> i <span class="math inline">\(\vec v_2\)</span> d’<span class="math inline">\(E\)</span> i</p></li>
<li><p><span class="math inline">\(\phi(\vec u,\lambda \vec v)=\lambda \phi(\vec u,\vec v)\)</span> per a tots
<span class="math inline">\(\vec u\)</span> i <span class="math inline">\(\vec v\)</span> d’<span class="math inline">\(E\)</span> i <span class="math inline">\(\lambda \in \K\)</span>.</p></li>
</ul>
<p>A més diem que una aplicació bilineal <span class="math inline">\(\phi\)</span> és:</p>
<ul>
<li><p><em>simètrica</em> si <span class="math inline">\(\phi(\vec u,\vec v)=\phi(\vec v,\vec u)\)</span> per a tots
<span class="math inline">\(\vec u\)</span> i <span class="math inline">\(\vec v\)</span> d’<span class="math inline">\(E\)</span>,</p></li>
<li><p><em>degenerada</em> si existeix <span class="math inline">\(\vec u\neq \vec 0\)</span> tal que
<span class="math inline">\(\phi(\vec u,\vec v)=0\)</span> per a tot <span class="math inline">\(\vec v\)</span> d’<span class="math inline">\(E\)</span>, i</p></li>
<li><p><em>definida positiva</em> si <span class="math inline">\(\phi(\vec u,\vec u)&gt;0\)</span> per a tot
<span class="math inline">\(\vec u\neq \vec 0\)</span>.</p></li>
</ul>
</div>
<div class="definition">
<p><span id="def:mat-apl-bil" class="definition"><strong>Definició 4.6  </strong></span>Si <span class="math inline">\(E\)</span> és un <span class="math inline">\(\K\)</span>-espai
vectorial de dimensió finita i <span class="math inline">\(\calb=[\vec v_1, \ldots, \vec v_n]\)</span> és
una base d’<span class="math inline">\(E\)</span>, i <span class="math inline">\(\phi\)</span> és una aplicació bilineal, <em>la matriu de
l’aplicació bilineal en la base <span class="math inline">\(\calb\)</span></em> és:</p>
<p><span class="math display">\[
[\phi]_\calb= \begin{pmatrix}
\phi(\vec v_1,\vec v_1) &amp; \phi(\vec v_1,\vec v_2) &amp; \cdots &amp; \phi(\vec v_1,\vec v_n)\\
\phi(\vec v_2,\vec v_1) &amp; \phi(\vec v_2,\vec v_2) &amp; \cdots &amp; \phi(\vec v_2,\vec v_n)\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\phi(\vec v_n,\vec v_1) &amp; \phi(\vec v_n,\vec v_2) &amp; \cdots &amp; \phi(\vec v_n,\vec v_n)
\end{pmatrix}.
\]</span></p>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-235" class="lemma"><strong>Lema 4.5  </strong></span>Si <span class="math inline">\(\phi\)</span> és una aplicació bilineal definit sobre un <span class="math inline">\(\K\)</span>-espai
vectorial <span class="math inline">\(E\)</span> amb base <span class="math inline">\(\calb=[\vec v_1, \dots, \vec v_n]\)</span> i
<span class="math inline">\([\phi]_\calb\)</span> és la matriu de <span class="math inline">\(\phi\)</span> en la base <span class="math inline">\(\calb\)</span>, llavors: si
<span class="math inline">\(\vec u\)</span> i <span class="math inline">\(\vec w\)</span> són vectors d’<span class="math inline">\(E\)</span> amb coordenades en la base
<span class="math inline">\(\calb\)</span>:
<span class="math display">\[
[\vec u]_\calb = \begin{pmatrix}
\lambda_1 \\ \vdots \\ \lambda_n
\end{pmatrix}
\text{ i }
[\vec w]_\calb = \begin{pmatrix}
\mu_1 \\ \vdots \\ \mu_n
\end{pmatrix}.
\]</span>
llavors:</p>
<p><span class="math display">\[
\phi(\vec u,\vec w)=\sum_{i=1}^n \sum_{j=1}^n \lambda_i\mu_j\phi(\vec v_i,\vec v_j) =[\vec u]_\calb^T \, [\phi]_\calb \, [\vec w]_\calb .
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-236" class="proof"><em>Prova</em>. </span>La primera igualtat és per bilinealitat, i és igual a l’última
expressió, que està escrita en forma matricial.</p>
</div>
<div class="lemma">
<p><span id="lem:canvi-base-forma-bil" class="lemma"><strong>Lema 4.6  </strong></span>Si
tenim <span class="math inline">\(\calb=[\vec v_1, \ldots, \vec v_n]\)</span> i
<span class="math inline">\(\calc=[\vec u_1,\ldots, \vec u_n]\)</span> bases de <span class="math inline">\(\K^n\)</span>,
<span class="math inline">\([\Id_n]_{\calc,\calb}\)</span> la matriu del canvi de base (la que té per
columnes les coordenades dels vectors <span class="math inline">\(\vec u_j\)</span> en la base <span class="math inline">\(\calb\)</span>), i
<span class="math inline">\(\phi\)</span> una aplicació bilineal, hi ha la relació següent entre les
matrius de l’aplicació bilineal en cada base:</p>
<p><span class="math display">\[
[\phi]_\calc=[\Id_n]_{\calc,\calb}^T \, [\phi]_\calb \, [\Id_n]_{\calc,\calb}
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-237" class="proof"><em>Prova</em>. </span>Les matrius de <span class="math inline">\([\phi]_\calb\)</span> i <span class="math inline">\([\phi]_\calc\)</span> han de complir
que, per a tot <span class="math inline">\(\vec u\)</span> i <span class="math inline">\(\vec v\)</span> de <span class="math inline">\(\K^n\)</span> es compleixi:</p>
<p><span class="math display" id="eq:canvi-base-apl-bil">\[
\begin{aligned}
\tag{4.2}
[\vec u]_\calc^T \, [\phi]_\calc \, [\vec v]_\calc = \phi(\vec u, \vec v)=  [\vec u]_\calb^T\, [\phi]_\calb\, [\vec v]_\calb .
\end{aligned}
\]</span>
Però, per les propietats de la matriu
<span class="math inline">\([\Id]_{\calc,\calb}\)</span> tenim:</p>
<p><span class="math display">\[
[\vec u]_\calb=[\Id]_{\calc,\calb} [\vec u]_\calc ,
\]</span>
i si ho
substituïm a l’Equació
<a href="ortogonalitat.html#eq:canvi-base-apl-bil">(4.2)</a>, tenim, que per a tot <span class="math inline">\(\vec u\)</span> i
<span class="math inline">\(\vec v\)</span> de <span class="math inline">\(\K^n\)</span>:</p>
<p><span class="math display">\[
[\vec u]_\calc^T \, [\phi]_\calc \, [\vec v]_\calc = ([\Id]_{\calc,\calb} [\vec u]_\calc)^T \, [\phi]_\calb \, ([\Id]_{\calc,\calb} [\vec v]_\calc) = [\vec u]_\calc^T \, ([\Id]_{\calc,\calb}^T [\phi]_\calb [\Id]_{\calc,\calb}) \, [\vec v]_\calc ,
\]</span></p>
<p>pel que tenim la igualtat de matrius que volem.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-238" class="example"><strong>Exemple 4.10  </strong></span>Considerem la forma bilineal simètrica
<span class="math inline">\(\phi\colon \R^2 \times \R^2 \to \R\)</span> donada per la matriu (en la base
estàndard <span class="math inline">\(\calb=[\vec e_1,\vec e_2]\)</span>)</p>
<p><span class="math display">\[
[\phi]=\begin{pmatrix} 0 &amp; 1/2 \\ 1/2 &amp; 0 \end{pmatrix},
\]</span>
i volem
calcular la matriu de <span class="math inline">\(\phi\)</span> en la base
<span class="math inline">\(\calc=[\smat{1\\1},\smat{1\\-1}]\)</span>. Llavors hem de calcular:</p>
<p><span class="math display">\[
[\Id]_{\calc,\calb}=\begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}
\]</span>
I
per tant la matriu</p>
<p><span class="math display">\[
[\phi]_\calc = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}^T
\begin{pmatrix} 0 &amp; 1/2 \\ 1/2 &amp; 0 \end{pmatrix}
\begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}=
\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{pmatrix}.
\]</span>
Per tant, les matrius
<span class="math inline">\(\smat{0 &amp; 1/2 \\ 1/2 &amp; 0}\)</span> i <span class="math inline">\(\smat{1 &amp; 0 \\ 0 &amp; -1}\)</span> representen la
mateixa forma bilineal en bases diferents.</p>
</div>
<p>Mirem ara un cas particular de forma bilineal:</p>
<div class="definition">
<p><span id="def:unlabeled-div-239" class="definition"><strong>Definició 4.7  </strong></span>Un <em>producte escalar</em> sobre un <span class="math inline">\(\R\)</span>-espai vectorial <span class="math inline">\(E\)</span> és una forma
bilineal simètrica definida positiva. En general, escriurem el resultat
amb un punt: <span class="math inline">\(\vec u \cdot \vec v\)</span>.</p>
</div>
<p>Si <span class="math inline">\(\calb\)</span> és una base finita d’un espai vectorial <span class="math inline">\(E\)</span> i <span class="math inline">\(\cdot\)</span> és un
producte escalar, la matriu <span class="math inline">\([\cdot]_\calb\)</span> serà simètrica.</p>
<div class="example">
<p><span id="exm:unlabeled-div-240" class="example"><strong>Exemple 4.11  </strong></span>Si considerem <span class="math inline">\(\R^n\)</span> i <span class="math inline">\(\calb=[\vec e_1, \ldots, \vec e_n]\)</span> la base
estàndard, el producte escalar definit com
<span class="math inline">\(\vec u \cdot \vec v=\vec u^T \vec v\)</span> és un producte escalar amb matriu
la matriu identitat <span class="math inline">\(\1_n\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-241" class="example"><strong>Exemple 4.12  </strong></span>Si <span class="math inline">\(E=C^\infty(\R)\)</span> definim el producte escalar:</p>
<p><span class="math display">\[
f\cdot g = \int_{-1}^1 f(x)g(x)\mathrm{d}x .
\]</span>
Com que a
<span class="math inline">\(C^\infty(\R)\)</span> no hi ha una base finita, no té sentit parlar de la
matriu del producte escalar.</p>
</div>
<div class="example">
<p><span id="exm:complexos" class="example"><strong>Exemple 4.13  </strong></span>Considerem els nombres
complexos <span class="math inline">\(\C\)</span> amb les operacions de
l’Apèndix <a href="espais-vectorials-i-aplicacions-lineals.html#def:aplicaciolinealKn">2.1</a>.</p>
<p>Considerem ara <span class="math inline">\(\C^n\)</span> com a <span class="math inline">\(\C\)</span>-espai vectorial de dimensió <span class="math inline">\(n\)</span> (suma
coordenada a coordenada i multiplicació per un escalar a totes les
coordenades). Definim la forma bilineal següent:
<span class="math display">\[
\text{si }
\vec v=\begin{pmatrix}
v_1 \\ \vdots \\ v_n
\end{pmatrix}
\text{ i }
\vec w=\begin{pmatrix}
w_1 \\ \vdots \\ w_n
\end{pmatrix}
\text{ llavors }
\vec v\cdot \vec w= \sum_{i=1}^n v_i\overline{w}_i=\vec v^T \overline{\vec w} \text{ on }
\overline{\vec w}=\begin{pmatrix}
\overline w_1 \\ \vdots \\ \overline w_n
\end{pmatrix}.
\]</span>
Que compleix, a més de les propietats de ser forma
bilineal:</p>
<ul>
<li><p><span class="math inline">\(\vec v\cdot \vec w\)</span>=<span class="math inline">\(\overline{\vec w\cdot \vec v}\)</span> per a tot
<span class="math inline">\(\vec v,\vec w\in \C^n\)</span> (diem que és hermítica) i</p></li>
<li><p><span class="math inline">\(\vec v\cdot \vec v=\|\vec v\|^2\in \R\)</span> per a tot <span class="math inline">\(\vec v\in\C^n\)</span>.</p></li>
</ul>
</div>
</div>
<div id="tota-matriu-simètrica-sobre-mathbbr-diagonalitza" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Tota matriu simètrica sobre <span class="math inline">\(\mathbb{R}\)</span> diagonalitza<a href="ortogonalitat.html#tota-matriu-simètrica-sobre-mathbbr-diagonalitza" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El resultat que volem demostrar a aquest apartat és:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-242" class="theorem"><strong>Teorema 4.6  </strong></span>Una matriu <span class="math inline">\(A\in M_n(\R)\)</span> diagonalitza en una base ortonormal si, i
només si, <span class="math inline">\(A\)</span> és simètrica.</p>
</div>
<p>Per demostrar el teorema espectral necessitem utilitzar el <em>Teorema
fonamental de l’àlgebra</em>, que no demostrem aquí perquè la seva
demostració surt dels objectius del curs:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-243" class="theorem"><strong>Teorema 4.7  </strong></span>Si <span class="math inline">\(p(x)\in\C[x]\)</span> (<span class="math inline">\(p(x)\)</span> és un polinomi amb coeficients a <span class="math inline">\(\C\)</span>) de grau
<span class="math inline">\(\geq 1\)</span>, existeix <span class="math inline">\(z\in \C\)</span> tal que <span class="math inline">\(p(z)=0\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-244" class="proof"><em>Prova</em>. </span><em>Demostració del teorema espectral.</em> Una implicació és directa: si <span class="math inline">\(A\)</span>
diagonalitza en una base ortonormal vol dir que podem escriure:</p>
<p><span class="math display">\[
A=Q D Q^T
\]</span>
amb <span class="math inline">\(D\)</span> una matriu diagonal i <span class="math inline">\(Q\)</span> una matriu ortogonal
(<span class="math inline">\(Q^{-1}=Q^T\)</span>). Per tant:
<span class="math display">\[
A^T=(QDQ^T)^T=(Q^T)^T D^T Q^T=Q D^T Q^T=A
\]</span></p>
<p>i <span class="math inline">\(A\)</span> és simètrica.</p>
<p>Per demostrar l’altra implicació, ho farem per inducció sobre <span class="math inline">\(n\)</span>:</p>
<ul>
<li><p>Per a <span class="math inline">\(n=1\)</span>, tota matriu és simètrica i diagonal.</p></li>
<li><p>Suposem cert per a <span class="math inline">\(n-1\)</span>, i volem demostrar-ho per <span class="math inline">\(n\)</span>: considerem
<span class="math inline">\(A\in M_n(\R)\)</span> i <span class="math inline">\(p_A(x)\)</span> el polinomi característic. Pensem
<span class="math inline">\(p_A(x)\in\C[x]\)</span> i, pel teorema fonamental de l’àlgebra, existeix
<span class="math inline">\(\lambda \in \C\)</span> tal que <span class="math inline">\(p_A(\lambda)=0\)</span>. També tindrem un vector
propi <span class="math inline">\(\vec v\in \C^n\)</span> tal que <span class="math inline">\(A \vec v=\lambda \vec v\)</span>.</p>
<p>Considerem ara la forma bilineal hermítica definida a l’Exemple
<a href="ortogonalitat.html#exm:complexos">4.13</a>:
<span class="math display">\[
A\vec v \cdot \vec v = \lambda \vec v \cdot \vec v= \lambda \|\vec v\|^2 \text{ amb $0\neq \|v\|^2\in R$}.
\]</span></p>
<p>Però també:
<span class="math display">\[
A\vec v \cdot\vec  v = (A \vec v)^T \overline{\vec v} = \vec v^T A^T \overline{\vec v} = \vec v^T \overline{(\overline{A^T} \vec v)} = \vec v^T A \overline{\vec v} = \vec v^T (\overline{\lambda \vec v})=\overline{\lambda} \|\vec v\|^2 ,
\]</span></p>
<p>on hem utilitzat que <span class="math inline">\(A=A^T\)</span> (<span class="math inline">\(A\)</span> és simètrica) i <span class="math inline">\(\overline A=A\)</span>
(<span class="math inline">\(A\)</span> té coeficients reals).</p>
<p>Per tant, tenim que <span class="math inline">\(\lambda=\overline{\lambda}\)</span> i obtenim
<span class="math inline">\(\lambda \in \R\)</span>.</p>
<p>Com que <span class="math inline">\(p_A(\lambda)=0\)</span>, tenim que existeix un vector propi real
<span class="math inline">\(\vec v\neq \vec 0\)</span> tal que <span class="math inline">\(A\vec v=\lambda \vec v\)</span>. Podem
considerar que <span class="math inline">\(\|\vec v\|=1\)</span> (si cal, substituïm <span class="math inline">\(\vec v\)</span> per
<span class="math inline">\(\frac{1}{\|\vec v\|}\vec v\)</span>).</p>
<p>Ampliem <span class="math inline">\(\vec v\)</span> amb <span class="math inline">\(\vec v_2\)</span>, …, <span class="math inline">\(\vec v_n\)</span> de tal manera que
<span class="math inline">\([\vec v,\vec v_2, \ldots ,\vec v_n]\)</span> sigui una base ortonormal
(primer ampliem fins a base, i després apliquem Gram-Schmidt). Si
considerem <span class="math inline">\(Q\)</span> la matriu que té per columnes els vectors <span class="math inline">\(\vec v\)</span>,
<span class="math inline">\(\vec v_2\)</span>, …, <span class="math inline">\(\vec v_n\)</span>, tenim:
<span class="math display">\[
QAQ^T=\begin{pmatrix} \lambda &amp; b_{12} &amp; \cdots &amp; b_{1n} \\
    0 &amp; b_{22} &amp; \cdots &amp; b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; b_{n2} &amp; \cdots  &amp; b_{nn}
    \end{pmatrix}
\]</span>
on hem utilitzat que <span class="math inline">\(Q^{-1}=Q^T\)</span>. Però com que
<span class="math inline">\(A=A^T\)</span>, tenim <span class="math inline">\((QAQ^T)^T=QAQ^T\)</span>, i per tant
<span class="math inline">\(b_{12}=b_{13}=\cdots=b_{1n}=0\)</span> i la matriu
<span class="math display">\[
B=\begin{pmatrix} b_{22} &amp; \cdots &amp; b_{2n} \\
    \vdots &amp; \ddots &amp; \vdots \\
    b_{n2} &amp; \cdots  &amp; b_{nn}
    \end{pmatrix}
\]</span>
és simètrica.</p>
<p>Ara restringim l’aplicació <span class="math inline">\(f_A\)</span> a l’espai
<span class="math inline">\(\langle v_2, \dots , v_n\rangle\)</span>, que en la base
<span class="math inline">\([v_2, \dots , v_n]\)</span> té per matriu
<span class="math display">\[
B=\begin{pmatrix} b_{22} &amp; \cdots &amp; b_{2n} \\
    \vdots &amp; \ddots &amp; \vdots \\
    b_{n2} &amp; \cdots  &amp; b_{nn}
    \end{pmatrix} .
\]</span>
Per hipòtesis d’inducció, <span class="math inline">\(B\)</span> diagonaliza en
una base ortonormal, pel que existeix <span class="math inline">\(P\in M_{n-1}(\R)\)</span> matriu
ortogonal i <span class="math inline">\(D\in M_{n-1}(\R)\)</span> matriu diagonal tals que <span class="math inline">\(B=P^TDP\)</span>.
Amb això tenim:
<span class="math display">\[
QAQ^T=\left(\begin{array}{c|c}
        \lambda &amp; 0 \\ \hline
        0 &amp; P^T D P
    \end{array}\right)=
    \left(\begin{array}{c|c}
        1 &amp; 0 \\ \hline
        0 &amp; P^T
    \end{array}\right)
    \left(\begin{array}{c|c}
        \lambda &amp; 0 \\ \hline
        0 &amp; D
    \end{array}\right)
    \left(\begin{array}{c|c}
        1 &amp; 0 \\ \hline
        0 &amp;  P
    \end{array}\right)
\]</span>
Per tant, considerem
<span class="math display">\[
Q&#39;=\left(\begin{array}{c|c}
        1 &amp; 0 \\ \hline
        0 &amp;  P
    \end{array}\right) Q \text{ i }
    D&#39;=\left(\begin{array}{c|c}
        \lambda &amp; 0 \\ \hline
        0 &amp; D
    \end{array}\right)
\]</span>
i tenim que <span class="math inline">\(Q&#39;\)</span> és una matriu ortogonal i
<span class="math inline">\(A=Q&#39;^{-1} D&#39; Q&#39;\)</span>, amb <span class="math inline">\(D&#39;\)</span> una matriu diagonal, pel que <span class="math inline">\(A\)</span>
diagonalitza a una base ortonormal.</p></li>
</ul>
</div>
</div>
<div id="descomposició-en-valors-singulars" class="section level2 hasAnchor" number="4.9">
<h2><span class="header-section-number">4.9</span> Descomposició en valors singulars<a href="ortogonalitat.html#descomposició-en-valors-singulars" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A aquest apartat resoldrem la pregunta següent amb les eines que hem
vist: si <span class="math inline">\(f=f_A\colon \R^n \to \R^m\)</span> és una aplicació lineal, existeix
una base ortonormal <span class="math inline">\(\calb=[\vec v_1, \dots, \vec v_n]\)</span> d’<span class="math inline">\(\R^n\)</span> tal que
<span class="math inline">\(f(\vec v_1), \dots, f(\vec v_n)\)</span> siguin ortogonals?</p>
<p>Veurem que sí:</p>
<div class="theorem">
<p><span id="thm:val-sing" class="theorem"><strong>Teorema 4.8  </strong></span>Si <span class="math inline">\(f=f_A\colon \R^n \to \R^m\)</span> és
una aplicació lineal, llavors existeix una base ortonormal
<span class="math inline">\(\calb=[\vec v_1, \dots, \vec v_n]\)</span> d’<span class="math inline">\(\R^n\)</span> tal que
<span class="math inline">\(f(\vec v_1), \dots, f(\vec v_n)\)</span> són ortogonals.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-245" class="proof"><em>Prova</em>. </span>Considerem <span class="math inline">\(f=f_A\)</span>, i per tant <span class="math inline">\(A\in M_{m\times n}(\R)\)</span> la
matriu tal que <span class="math inline">\(f(\vec v)=A\vec v\)</span>. Llavors, la matriu <span class="math inline">\(A^TA\in M_n(\R)\)</span>
és simètrica, i per tant diagonalitza en una base ortonormal, per tant,
existeix una base ortonormal <span class="math inline">\(\calb=[\vec v_1, \dots, \vec v_n]\)</span> de
<span class="math inline">\(\R^n\)</span> i escalars <span class="math inline">\(\lambda_1, \dots, \lambda_n\in \R\)</span> tals que</p>
<p><span class="math display">\[
A^T A \vec v_i=\lambda_i \vec v_i \text{ per a tot $i=1, \dots, n$.}
\]</span></p>
<p>Volem veure que <span class="math inline">\(f(\vec v_1), \dots, f(\vec v_n)\)</span> són ortogonals, o
sigui, <span class="math inline">\(f(\vec v_i)\cdot f(\vec v_j)=0\)</span> si <span class="math inline">\(i\neq j\)</span>:
<span class="math display">\[
\begin{aligned}
f(\vec v_i)\cdot f(\vec v_j) &amp; =(A\vec v_i)\cdot(A\vec v_j)=(A\vec v_i)^T(A\vec v_j)=(\vec v_i^T A^T)(A \vec v_j)= \\
&amp;  = \vec v_i^T (A^T A \vec v_j)=\vec v_i^T (\lambda_j \vec v_j)=\lambda_j (\vec v_i \cdot \vec v_j)=0 .
\end{aligned}
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-246" class="remark"><em>Observació</em>. </span>El Teorema <a href="ortogonalitat.html#thm:val-sing">4.8</a> no diu que <span class="math inline">\(\calb\)</span> sigui única, i de fet, en
general, no ho és: considereu l’aplicació identitat de <span class="math inline">\(\R^n \to \R^n\)</span>,
que envia qualsevol base ortonormal a vectors ortogonals.</p>
</div>
<p>La interpretació geomètrica a <span class="math inline">\(\R^2\)</span> és la següent: la imatge d’una
circumferència de radi <span class="math inline">\(1\)</span> centrada a l’origen és una el·lipsi, els
vectors propis d’<span class="math inline">\(A^TA\)</span> corresponen a les direccions principals de
l’el·lipsi, i les longituds dels semieixos principals són les arrels
quadrades dels valors propis d’<span class="math inline">\(A^TA\)</span>. Posem nom a aquests valors:</p>
<div class="definition">
<p><span id="def:unlabeled-div-247" class="definition"><strong>Definició 4.8  </strong></span>Donada una matriu <span class="math inline">\(A\in M_{m\times n}(\R)\)</span>, definim els <em>valors
singulars d’<span class="math inline">\(A\)</span></em> com les arrels quadrades dels valors propis de la
matriu <span class="math inline">\(A^TA\)</span>: <span class="math inline">\(\sigma_1, \dots, \sigma_m\)</span> positius tals que
<span class="math inline">\(\sigma_i^2\)</span> és valor propi d’<span class="math inline">\(A^TA\)</span>.</p>
</div>
<p>Si ordenem els vectors de la base <span class="math inline">\(\calb\)</span> del Teorema
<a href="ortogonalitat.html#thm:val-sing">4.8</a> de tal manera que
<span class="math inline">\(\|f(\vec v_i)\|\geq \|f(\vec v_{i+1})\|\)</span>, aleshores
<span class="math inline">\(\sigma_i=\|f(\vec v_i)\|\)</span> a l’enunciat del Teorema
<a href="ortogonalitat.html#thm:val-sing">4.8</a>.</p>
<p>A partir de cert <span class="math inline">\(r\)</span> tindrem <span class="math inline">\(f(\vec v_i)=\vec 0\)</span> si <span class="math inline">\(i&gt;r\)</span> (pot ser que
<span class="math inline">\(r=n\)</span>). Els vectors <span class="math inline">\(\vec w_i=\frac{1}{\|f(\vec v_i)\|} f(\vec v_i)\)</span> per
a <span class="math inline">\(1\leq i \leq r\)</span> són linealment independents (perquè són ortogonals),
i els podem ampliar a una base ortonormal
<span class="math inline">\(\calc=[\vec w_1, \dots, \vec w_m]\)</span> d’<span class="math inline">\(\R^m\)</span>, per Gram-Schmidt.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-248" class="proposition"><strong>Proposició 4.5  </strong></span>Si <span class="math inline">\(f=f_A\colon \R^n \to \R^m\)</span> és una aplicació lineal i <span class="math inline">\(\Rang(A)=r\)</span>,
llavors hi ha <span class="math inline">\(r\)</span> valors singulars diferents de zero i els <span class="math inline">\(n-r\)</span>
restants valen zero.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-249" class="proof"><em>Prova</em>. </span>Tenim que en la base <span class="math inline">\(\calb\)</span> del Teorema
<a href="ortogonalitat.html#thm:val-sing">4.8</a> i <span class="math inline">\(\calc\)</span> la base del paràgraf anterior <span class="math inline">\(f_A\)</span>
té per matriu <span class="math inline">\([f_A]_{\calb.\calc}\)</span>, una matriu amb <span class="math inline">\(\sigma_i\)</span> a la
diagonal i zeros a la resta. Llavors, com que el rang és la dimensió de
la imatge, el rang de <span class="math inline">\(f_A\)</span> és igual al de <span class="math inline">\([f_A]_{\calb,\calc}\)</span>, que és
el nombre de <span class="math inline">\(\sigma_i\)</span> no nuls, i la resta han de ser zero.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-250" class="remark"><em>Observació</em>. </span>Amb tot el que hem fet, si considerem <span class="math inline">\(A\in M_{m\times n}(\R)\)</span> una
matriu, <span class="math inline">\(f_A\colon \R^n\to\R^m\)</span> l’aplicació lineal induïda,
<span class="math inline">\(\calb=[\vec v_1, \dots, \vec v_r, \vec v_{r+1}, \dots, \vec v_n]\)</span> la
base de <span class="math inline">\(\R^n\)</span> del Teorema
<a href="ortogonalitat.html#thm:val-sing">4.8</a>, i <span class="math inline">\(\calc=[\vec w_1, \dots, \vec w_m]\)</span> la base
de <span class="math inline">\(\R^m\)</span> com a la discussió de després del teorema, llavors:</p>
<ul>
<li><p><span class="math inline">\([\vec v_{r+1}, \dots,\vec v_n]\)</span> és una base ortonormal de
<span class="math inline">\(\Ker(f_A)\)</span>.</p></li>
<li><p><span class="math inline">\([\vec v_{1}, \dots, \vec v_r]\)</span> és una base ortonormal de
<span class="math inline">\(\Ker(f_A)^\bot\)</span>.</p></li>
<li><p><span class="math inline">\([\vec w_1,\dots,\vec w_r]\)</span> és una base ortonormal de <span class="math inline">\(\Ima(f_A)\)</span>.</p></li>
<li><p><span class="math inline">\([\vec w_{r+1},\dots,\vec w_m]\)</span> és una base ortonormal de
<span class="math inline">\(\Ima(f_A)^\bot\)</span>.</p></li>
</ul>
</div>
<p>Podem aprofitar aquests raonaments per demostrar:</p>
<div class="theorem">
<p><span id="thm:SVD" class="theorem"><strong>Teorema 4.9  </strong></span>Si <span class="math inline">\(A\in M_{m\times n}(\R)\)</span>, llavors
existeixen <span class="math inline">\(U \in M_{m}(\R)\)</span> i <span class="math inline">\(V\in M_n(\R)\)</span> ortogonals,
<span class="math inline">\(D\in M_{m\times n}(\R)\)</span> amb els únics elements no nuls a la diagonal i
positius ordenats de manera decreixent, tals que <span class="math inline">\(A=U D V^T\)</span>. Aquesta
descomposició s’anomena una <em>descomposició en valors singulars d’<span class="math inline">\(A\)</span></em>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-251" class="proof"><em>Prova</em>. </span>Considerem l’aplicació lineal <span class="math inline">\(f_A\)</span> i veiem que necessitem
bases ortonormals <span class="math inline">\(\calb\)</span> d’<span class="math inline">\(\R^n\)</span> i <span class="math inline">\(\calc\)</span> d’<span class="math inline">\(\R^m\)</span> tals que
<span class="math inline">\(D=[f_A]_{\calb,\calc}\)</span> sigui diagonal.</p>
<p>El Teorema <a href="ortogonalitat.html#thm:val-sing">4.8</a> ens dóna una base <span class="math inline">\(\calb\)</span> d’<span class="math inline">\(\R^n\)</span> que és
ortonormal. Ordenem aquesta base de tal manera que
<span class="math inline">\(\|f(\vec v_i)\|\geq \|f(\vec v_{i+1})\|\)</span>. Sigui <span class="math inline">\(V^T=V^{-1}\)</span> la matriu
que té per columnes aquesta base, i sigui <span class="math inline">\(r\)</span> tal que
<span class="math inline">\(f_(\vec v_i)=\vec 0\)</span> si <span class="math inline">\(i&gt;r\)</span> (és el rang d’A).</p>
<p>Considerem <span class="math inline">\(\calc=[\vec w_1, \dots, \vec w_m]\)</span> la base de <span class="math inline">\(\R^m\)</span> de que
hem considerat després del
Teorema <a href="ortogonalitat.html#thm:val-sing">4.8</a>. Sigui <span class="math inline">\(U\)</span> la matriu que té per columnes
aquests vectors.</p>
<p>Com que <span class="math inline">\(f(\vec v_i)=\|f(\vec v_i)\| \vec w_i\)</span> si <span class="math inline">\(i\leq r\)</span> i
<span class="math inline">\(f(\vec v_i)=\vec 0\)</span> si <span class="math inline">\(i&gt;r\)</span>, tenim que <span class="math inline">\([f]_{\calb,\calc}\)</span> és diagonal
amb els valors singulars d’<span class="math inline">\(A\)</span> a la diagonal ordenats de més gran a més
petit.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-252" class="example"><strong>Exemple 4.14  </strong></span>Considerem l’aplicació lineal <span class="math inline">\(f_A\colon \R^2 \to \R^2\)</span> amb:</p>
<p><span class="math display">\[
A=\begin{pmatrix}
3 &amp; -9/5 \\ -1 &amp; 13/5
\end{pmatrix}
\]</span>
i volem fer-ne una descomposició en valors singulars.</p>
<p>Calculem primer <span class="math inline">\(A^TA\)</span>:
<span class="math display">\[
A^T A = \begin{pmatrix}
10 &amp; -8 \\ -8 &amp; 10
\end{pmatrix}
\]</span>
I la diagonalitzem. El polinomi característic és:</p>
<p><span class="math display">\[
p_{A^TA}(x)=x^2 - 20 x + 36 = (x-18)(x-2)
\]</span>
Per tant, els valors
propis d’<span class="math inline">\(A^TA\)</span> són <span class="math inline">\(\lambda_1=18\)</span> i <span class="math inline">\(\lambda_2=2\)</span> i els corresponents
vectors propis:</p>
<p><span class="math display">\[
\Ker(A^TA-18\1_2)=\langle \begin{pmatrix} 1 \\ -1 \end{pmatrix} \rangle \text{ i }
\Ker(A^TA-2\1_2)=\langle \begin{pmatrix} 1 \\ 1 \end{pmatrix} \rangle
\]</span></p>
<p>I com que els hem de considerar unitaris, obtenim:
<span class="math display">\[
V^T=\begin{pmatrix}
1/\sqrt{2} &amp; 1/\sqrt{2} \\ -1/\sqrt{2} &amp; 1/\sqrt{2}
\end{pmatrix}
\text{ i per tant }
V=\begin{pmatrix}
1/\sqrt{2} &amp; -1/\sqrt{2} \\ 1/\sqrt{2} &amp; 1/\sqrt{2}
\end{pmatrix}.
\]</span></p>
<p>La matriu diagonal són els valors singulars, que són les arrels
quadrades dels valors propis d’<span class="math inline">\(A^TA\)</span>, per tant són
<span class="math inline">\(\sigma_1=\sqrt{18}=3\sqrt{2}\)</span> i <span class="math inline">\(\sigma_2=\sqrt{2}\)</span> i tenim:</p>
<p><span class="math display">\[
D=\begin{pmatrix}
3\sqrt{2} &amp; 0 \\ 0 &amp; \sqrt{2}
\end{pmatrix}.
\]</span></p>
<p>I la matriu <span class="math inline">\(U\)</span> té per columnes les imatges de <span class="math inline">\(\smat{1\\-1}\)</span> i
<span class="math inline">\(\smat{1\\1}\)</span> per <span class="math inline">\(f_A\)</span> dividides per les seves normes, per tant:</p>
<p><span class="math display">\[
\begin{pmatrix}
3 &amp; -9/5 \\ -1 &amp; 13/5
\end{pmatrix}
\begin{pmatrix}
1 \\ -1
\end{pmatrix}=
\begin{pmatrix}
24/5 \\ -18/5
\end{pmatrix}=
6 \begin{pmatrix}
4/5 \\ -3/5
\end{pmatrix}
\]</span>
i
<span class="math display">\[
\begin{pmatrix}
3 &amp; -9/5 \\ -1 &amp; 13/5
\end{pmatrix}
\begin{pmatrix}
1 \\ 1
\end{pmatrix}=
\begin{pmatrix}
6/5 \\ 8/5
\end{pmatrix}=
2 \begin{pmatrix}
3/5 \\ 4/5
\end{pmatrix}
\]</span>
Per tant:
<span class="math display">\[
U=\begin{pmatrix}
4/5 &amp; 3/5 \\ -3/5 &amp; 4/5
\end{pmatrix}
\]</span>
I una descomposició d’<span class="math inline">\(A\)</span> en valors singulars és:</p>
<p><span class="math display">\[
\begin{pmatrix}
3 &amp; -9/5 \\ -1 &amp; 13/5
\end{pmatrix}
=\begin{pmatrix}
4/5 &amp; 3/5 \\ -3/5 &amp; 4/5
\end{pmatrix}
\begin{pmatrix}
3\sqrt{2} &amp; 0 \\ 0 &amp; \sqrt{2}
\end{pmatrix}
\begin{pmatrix}
1/\sqrt{2} &amp; -1/\sqrt{2} \\ 1/\sqrt{2} &amp; 1/\sqrt{2}
\end{pmatrix}.
\]</span>
A la Figura <a href="ortogonalitat.html#fig:circ-ellipse" reference-type="ref" reference="fig:circ-ellipse">2</a> podem veure com es modifica la
circumferència unitat quan apliquem la matriu <span class="math inline">\(A\)</span>. Veiem que a la
direcció <span class="math inline">\(\smat{4\\-3}\)</span> hi ha el primer eix principal amb semilongitud
<span class="math inline">\(3\sqrt{2}\)</span> i a la direcció <span class="math inline">\(\smat{3\\4}\)</span> l’altre eix, amb semilongitud
<span class="math inline">\(\sqrt{2}\)</span>.</p>
<figure id="fig:circ-ellipse">
<figcaption>
Deformació de la circumferència unitat per l’aplicació
lineal <span class="math inline"><em>f</em><sub><em>A</em></sub></span>.<span id="fig:circ-ellipse" label="fig:circ-ellipse"></span>
</figcaption>
</figure>
</div>
</div>
<div id="classificació-de-formes-bilineals-simètriques-sobre-mathbbrn" class="section level2 hasAnchor" number="4.10">
<h2><span class="header-section-number">4.10</span> Classificació de formes bilineals simètriques sobre <span class="math inline">\(\mathbb{R}^n\)</span><a href="ortogonalitat.html#classificació-de-formes-bilineals-simètriques-sobre-mathbbrn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A aquesta secció l’objectiu és classificar les formes bilineals. Definim
abans què vol dir que siguin equivalents:</p>
<div class="definition">
<p><span id="def:unlabeled-div-253" class="definition"><strong>Definició 4.9  </strong></span>Diem que dues formes bilineals
<span class="math inline">\(\phi_1,\phi_2\colon \K^n\times\K^n\to \K\)</span> amb matrius corresponents
<span class="math inline">\([\phi_1]\)</span> i <span class="math inline">\([\phi_2]\)</span> <em>són equivalents</em> si existeix una base <span class="math inline">\(\calc\)</span>
de <span class="math inline">\(\K^n\)</span> tal que
<span class="math display">\[
[\phi_1]=[\phi_2]_\calc .
\]</span>
Dit d’una altra manera,
si existeix una matriu invertible <span class="math inline">\(\cals\in M_n(\K)\)</span> (la matriu del
canvi de base, que té per columnes els vectors de <span class="math inline">\(\calc\)</span>) tal que</p>
<p><span class="math display">\[
[\phi_1]=\cals^T [\phi_2] \cals.
\]</span>
Escriurem <span class="math inline">\(\phi_1\sim \phi_2\)</span>.</p>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-254" class="lemma"><strong>Lema 4.7  </strong></span>Ser equivalents com a formes bilineals de <span class="math inline">\(\K^n\times \K^n\)</span> a <span class="math inline">\(\K\)</span> és
una relació d’equivalència. O sigui:</p>
<ul>
<li><p><span class="math inline">\(\phi \sim \phi\)</span> per a tot <span class="math inline">\(\phi\)</span> forma bilineal (reflexiva),</p></li>
<li><p><span class="math inline">\(\phi_1\sim \phi_2 \Leftrightarrow \phi_2 \sim \phi_1\)</span> per a totes
<span class="math inline">\(\phi_1, \phi_2\)</span> formes bilineals (simètrica) i</p></li>
<li><p><span class="math inline">\(\phi_1\sim \phi_2\)</span> i <span class="math inline">\(\phi_2\sim \phi_3\)</span> implica
<span class="math inline">\(\phi_1\sim \phi_3\)</span> per a totes <span class="math inline">\(\phi_1,\phi_2,\phi_3\)</span> formes
quadràtiques (transitiva).</p></li>
</ul>
</div>
<div class="proof">
<p><span id="unlabeled-div-255" class="proof"><em>Prova</em>. </span>Cal veure en cada cas quina és la matriu <span class="math inline">\(\cals\)</span> del canvi de
base:</p>
<ul>
<li><p>Per veure que <span class="math inline">\(\phi\sim \phi\)</span>, considerem <span class="math inline">\(\cals=\1_n\)</span>,</p></li>
<li><p>Si <span class="math inline">\(\phi_1\sim \phi_2\)</span> tenim que existeix <span class="math inline">\(\cals\)</span> una matriu de
canvi de base (i per tant invertible) tal que:
<span class="math display">\[
[\phi_1]=\cals^T [\phi_2] \cals.
\]</span>
Però aquesta igualtat també es
pot escriure com:
<span class="math display">\[
[\phi_2]=(\cals^{-1})^T [\phi_1] \cals^{-1}
\]</span>
i
per tant <span class="math inline">\(\phi_2\sim \phi_1\)</span>.</p></li>
<li><p>Per hipòtesis tenim que existeixen <span class="math inline">\(\cals_1\)</span> i <span class="math inline">\(\cals_2\)</span> tals que:
<span class="math display">\[
[\phi_1]=\cals_1^T [\phi_2] \cals_1
    \text{ i }
    [\phi_2]=\cals_2^T [\phi_3] \cals_2.
\]</span>
Llavors:
<span class="math display">\[
[\phi_1]=\cals_1^T \cals_2^T [\phi_3] \cals_2 \cals_1=
    (\cals_2 \cals_1)^T [\phi_3] (\cals_2 \cals_1)
\]</span>
i tenim
<span class="math inline">\(\phi_1\sim \phi_3\)</span>.</p></li>
</ul>
</div>
<p>A partir d’ara considerem que <span class="math inline">\(\K=\R\)</span> i l’objectiu és classificar les
formes bilineals <span class="math inline">\(\phi\colon \R^n\times \R^n \to \R\)</span>:</p>
<div class="theorem">
<p><span id="thm:class-formbilR" class="theorem"><strong>Teorema 4.10  </strong></span>Tota forma bilineal
simètrica <span class="math inline">\(\phi\colon \R^n\times \R^n\to \R\)</span> és equivalent a una forma
bilineal que té per matriu una matriu diagonal amb coeficients (a la
diagonal): <span class="math inline">\(1\)</span> a les <span class="math inline">\(r\)</span> primeres files, <span class="math inline">\(-1\)</span> a les <span class="math inline">\(s\)</span> següents i <span class="math inline">\(0\)</span> a
les <span class="math inline">\(t\)</span> últimes (<span class="math inline">\(n=r+s+t\)</span>). O sigui una matriu de la forma:</p>
<p><span class="math display">\[
\begin{pmatrix}
1      &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
       &amp; \ddots &amp;   &amp;   &amp; \cdots &amp;   &amp;   &amp; \cdots &amp;  \\
0      &amp; \cdots &amp; 1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
0      &amp; \cdots &amp; 0 &amp; -1 &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
       &amp;        &amp;   &amp;    &amp; \ddots &amp;   &amp;   &amp; \cdots &amp;   \\
0      &amp; \cdots &amp; 0 &amp;  0 &amp; \cdots &amp; -1 &amp; 0 &amp; \cdots &amp; 0 \\
0      &amp; \cdots &amp; 0 &amp;  0 &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
       &amp;        &amp;   &amp;    &amp; \cdots &amp;   &amp;   &amp; \ddots &amp;   \\
0      &amp; \cdots &amp; 0 &amp;  0 &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}
\]</span>
A més, si definim la parella <span class="math inline">\((r,s)\)</span> com la <em>signatura
de <span class="math inline">\(\phi\)</span></em> (<span class="math inline">\(\sign(\phi)\)</span>), dues formes bilineals bilineals
<span class="math inline">\(\phi_1,\phi_2\colon \R^n\times \R^n\to \R\)</span> són equivalents si i només
si <span class="math inline">\(\sign(\phi_1)=\sign(\phi_2)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-256" class="proof"><em>Prova</em>. </span>Considerem <span class="math inline">\([\phi]\)</span> la matriu simètrica <span class="math inline">\(n\times n\)</span> sobre <span class="math inline">\(\R\)</span>
de la forma bilineal <span class="math inline">\(\phi\)</span>. Pel Teorema espectral, existeix una matriu
ortogonal <span class="math inline">\(Q\)</span> i una matriu diagonal <span class="math inline">\(D\)</span> amb valors
<span class="math inline">\(\lambda_1, \dots, \lambda_n\)</span> tal que:
<span class="math display">\[
D=Q^T [\phi] Q \,.
\]</span>
Ara fem
els canvis següents:</p>
<ul>
<li><p>Podem reordenar els elements de la diagonal de <span class="math inline">\(D\)</span> reordenant les
columnes de <span class="math inline">\(Q\)</span>: si <span class="math inline">\(Q&#39;\)</span> és la matriu que resulta d’intercanviar les
columnes <span class="math inline">\(j\)</span> i <span class="math inline">\(k\)</span> de <span class="math inline">\(Q\)</span>, llavors, <span class="math inline">\(Q&#39;\)</span> continua sent ortogonal i
el producte:
<span class="math display">\[
(Q&#39;)^T[\phi]Q&#39;
\]</span>
també és diagonal, però
intercanviant les posicions <span class="math inline">\(i\)</span> i <span class="math inline">\(j\)</span>, o sigui, els valors
<span class="math inline">\(\lambda_i\)</span> i <span class="math inline">\(\lambda_j\)</span>.<br />
Aprofitem aquesta propietat per reordenar la diagonal de <span class="math inline">\(D\)</span> i posar
primer els <span class="math inline">\(\lambda_i&gt;0\)</span>, llavors els <span class="math inline">\(\lambda_i&lt;0\)</span> i finalment els
<span class="math inline">\(\lambda_i=0\)</span>. Per tant, podem suposar que tenim <span class="math inline">\(Q\)</span> una matriu
ortogonal tal que
<span class="math display">\[
D=Q^T [\phi] Q \,.
\]</span>
amb <span class="math inline">\(D\)</span> una matriu diagonal
tal que a la diagonal té els <span class="math inline">\(r\)</span> primers coeficients positius, els
<span class="math inline">\(s\)</span> següents negatius i els <span class="math inline">\(t\)</span> últims zero.</p></li>
<li><p>Considerem <span class="math inline">\(A\)</span> la matriu diagonal següent, definida a partir dels
coeficients de <span class="math inline">\(D\)</span>: el coeficient <span class="math inline">\(a_{ii}\)</span> es defineix com:
<span class="math display">\[
a_{ii}=\begin{cases}
    1/\sqrt{\lambda_i} &amp; \text{si $1\leq i \leq r$,} \\
    1/\sqrt{-\lambda_i} &amp; \text{si $r &lt; i \leq r+s$,} \\
    1 &amp; \text{si $r+s &lt; i \leq n$.}
    \end{cases}
\]</span>
Tenim que <span class="math inline">\(A^TDA\)</span> és una matriu diagonal amb <span class="math inline">\(r\)</span>
uns a les primeres files, <span class="math inline">\(s\)</span> menys uns a les següents i zero a les
últimes.<br />
Per tant:
<span class="math display">\[
[\phi] \sim A^TQ^T [\phi] QA = (QA)^T [\phi] QA
\]</span>
i
<span class="math inline">\((QA)^T [\phi] QA\)</span> és diagonal i com diu l’enunciat del teorema.</p></li>
</ul>
<p>De moment hem vist que, com que “ser equivalent a té la propietat
transitiva, si <span class="math inline">\(\phi_1\)</span> i <span class="math inline">\(\phi_2\)</span> tenen la mateixa signatura, les dues
són equivalents a una mateixa forma bilineal i per tant
<span class="math inline">\(\phi_1\sim \phi_2\)</span>.<br />
Cal veure el recíproc, o el que és equivalent, que dues matrius
diagonals equivalents <span class="math inline">\(D\)</span> i <span class="math inline">\(D&#39;\)</span> amb <span class="math inline">\((r,s,t)\)</span> i <span class="math inline">\((r&#39;,s&#39;,t&#39;)\)</span>
coeficients <span class="math inline">\(1\)</span>, <span class="math inline">\(-1\)</span> i <span class="math inline">\(0\)</span> respectivament, llavors
<span class="math inline">\((r,s,t)=(r&#39;,s&#39;,t&#39;)\)</span>.<br />
Observem que n’hi ha prou amb veure que <span class="math inline">\(r\leq r&#39;\)</span>. Efectivament,
intercanviant els rols de <span class="math inline">\(D\)</span> i <span class="math inline">\(D&#39;\)</span>, conclourem que <span class="math inline">\(r=r&#39;\)</span>. Com que
<span class="math inline">\(t=\dim\ker(D)=\dim\ker(D&#39;)=t&#39;\)</span> i <span class="math inline">\(r+s+t=n=r+s+t&#39;\)</span>, en deduïm les
igualtats <span class="math inline">\(s=s&#39;\)</span> i <span class="math inline">\(t=t&#39;\)</span>.</p>
<p>Per veure que <span class="math inline">\(r\leq r&#39;\)</span>, considerem bases ortonormals
<span class="math inline">\(\mathcal{B}=(u_1,\ldots, u_n)\)</span> i <span class="math inline">\(\mathcal{B&#39;}=(u&#39;_1,\ldots,u&#39;_n)\)</span>
d’<span class="math inline">\(\R^n\)</span>, respecte les quals la forma <span class="math inline">\(\phi\)</span> és, respectivament, <span class="math inline">\(D\)</span> i
<span class="math inline">\(D&#39;\)</span>. Considerem els subespais <span class="math inline">\(V\)</span> i <span class="math inline">\(V&#39;\)</span> donats per</p>
<p><span class="math display">\[
V = \langle u_1,\ldots, u_r\rangle,\quad V&#39;=\rangle u&#39;_1,\ldots, u&#39;_{r&#39;}\rangle,
\]</span></p>
<p>i construirem una aplicació lineal injectiva <span class="math inline">\(V\to V&#39;\)</span>. Sigui
<span class="math inline">\(g \colon \R^n\to V&#39;\)</span> la projecció ortogonal al subespai <span class="math inline">\(V&#39;\)</span>, i sigui
<span class="math inline">\(f \colon V \to V&#39;\)</span> la restricció de <span class="math inline">\(g\)</span> a <span class="math inline">\(V\)</span>. Vegem doncs que <span class="math inline">\(f\)</span> és
injectiva. Sigui <span class="math inline">\(v\in\ker f\)</span>. D’una banda, com que <span class="math inline">\(v\in V\)</span> i <span class="math inline">\(\phi|_V\)</span>
té matriu identitat, tenim <span class="math inline">\(\phi(v,v) \geq 0\)</span> amb igualtat si i només si
<span class="math inline">\(v=0\)</span>. Però com que
<span class="math inline">\(v\in \ker f \subseteq \langle u&#39;_{r&#39;+1},\ldots u&#39;_n\rangle\)</span>, tenim
<span class="math inline">\(\phi(v,v)\leq 0\)</span>, ja que la restricció de <span class="math inline">\(\phi\)</span> a aquest subespai té
matriu semidefinida negativa. Concloem que <span class="math inline">\(\varphi(v,v)=0\)</span> i per tant
<span class="math inline">\(v=0\)</span>, com volíem veure.</p>
</div>
<div class="example">
<p><span id="exm:class-form-bil" class="example"><strong>Exemple 4.15  </strong></span>Considerem la
forma bilineal <span class="math inline">\(\phi\colon \R^4\times\R^4\to\R\)</span> donada per la matriu:</p>
<p><span class="math display">\[
[\phi]=\left(\begin{array}{rrrr}
1 &amp; 5 &amp; -1 &amp; 3 \\
5 &amp; 1 &amp; 3 &amp; -1 \\
-1 &amp; 3 &amp; 1 &amp; 5 \\
3 &amp; -1 &amp; 5 &amp; 1
\end{array}\right).
\]</span>
Una manera de classificar-la és de calcular el
polinomi característic i estudiar el signe dels valors on s’anul·la:</p>
<p><span class="math display">\[
p_{[\phi]}(x)=\det(A-x\1_4)=x^{4} - 4 x^{3} - 64 x^{2} + 256 x .
\]</span>
I
<span class="math inline">\(p_{[\phi]}(x)\)</span> s’anul·la als valors <span class="math inline">\(\{-8,0,4,8\}\)</span>, per tant té
signatura <span class="math inline">\((2,1)\)</span> i és equivalent a la forma bilineal que té per matriu:</p>
<p><span class="math display">\[
\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix} .
\]</span></p>
</div>
<p>Acabem aquesta secció aprofitant aquesta classificació per fer la
corresponent de les formes quadràtiques.</p>
<div class="definition">
<p><span id="def:unlabeled-div-257" class="definition"><strong>Definició 4.10  </strong></span>Una <em>forma quadràtica</em> sobre un cos <span class="math inline">\(\K\)</span> és una aplicació
<span class="math inline">\(q\colon \K^n\to \K\)</span> que es pot expressar com:</p>
<p><span class="math display">\[
q(x_1, \dots , x_n)=\sum_{1\leq i , j \leq n} \lambda_{ij}x_i x_j
\]</span></p>
<p>amb <span class="math inline">\(\lambda_{ij}\in \K\)</span>.</p>
<p>També es pot escriure com:
<span class="math display">\[
q(x_1, \dots , x_n)= \vec x^T A \vec x ,
\]</span></p>
<p>on <span class="math inline">\(\vec x=\smat{x_1 \\ \vdots \\ x_n}\)</span> i <span class="math inline">\(A\)</span> és la matriu (simètrica)
que té per coeficients</p>
<p><span class="math display">\[
a_{ij}=\begin{cases} \lambda_{ii} &amp; \text{si $i=j$,} \\ \frac{\lambda_{ij}+\lambda_{ji}}{2} &amp; \text{si $i\neq j$.}\end{cases}
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-258" class="remark"><em>Observació</em>. </span>Amb el que hem vist, tenim una bijecció entre les matrius simètriques
(que es poden pensar com formes bilineals en una base donada) i les
formes quadràtiques. Escriurem <span class="math inline">\([q]\)</span> per denotar la matriu simètrica
corresponent a la forma quadràtica <span class="math inline">\(q\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-259" class="example"><strong>Exemple 4.16  </strong></span>La forma quadràtica <span class="math inline">\(q\colon \R^2 \to \R\)</span> definida per
<span class="math inline">\(q(x_1,x_2)=ax_1^2+bx_1x_2+cx_2^2\)</span> correspon a la matriu simètrica</p>
<p><span class="math display">\[
\begin{pmatrix}
a &amp; b/2 \\ b/2 &amp; c
\end{pmatrix} .
\]</span>
Podem recuperar la forma quadràtica fent:</p>
<p><span class="math display">\[
q(x_1,x_2)=\begin{pmatrix} x_1 &amp; x_2 \end{pmatrix}\begin{pmatrix}
a &amp; b/2 \\ b/2 &amp; c
\end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}=ax_1^2 + bx_1x_2+cx_2^2 .
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-260" class="example"><strong>Exemple 4.17  </strong></span>Considerem les formes quadràtiques <span class="math inline">\(q_1\)</span> i <span class="math inline">\(q_2\)</span> de <span class="math inline">\(\R^2\)</span> a <span class="math inline">\(\R\)</span>
definides com <span class="math inline">\(q_1(x_1,x_2)=x_1x_2\)</span> i <span class="math inline">\(q_2(y_1,y_2)=y_1^2-y_2^2\)</span>.
Observem que si fem el canvi de base:</p>
<p><span class="math display">\[
\begin{pmatrix} y_1 \\ y_2  \end{pmatrix} =
\begin{pmatrix} 1/2 &amp; 1/2 \\ 1/2 &amp; -1/2 \end{pmatrix}
\begin{pmatrix} x_1 \\ x_2  \end{pmatrix}
\]</span>
es poden escriure:</p>
<p><span class="math display">\[
y_1=\frac12x_1+\frac12x_2 \text{ i } y_2=\frac12x_1-\frac12x_2
\]</span>
i
tenim</p>
<p><span class="math display">\[
q_2(y_1,y_2)=(\frac12x_1+\frac12x_2)^2 - (\frac12x_1-\frac12x_2)^2=x_1x_2 .
\]</span></p>
<p>Per tant, tenen la mateixa forma. Aquesta igualtat també es pot veure en
forma matricial observant que si <span class="math inline">\(\cals\)</span> és la matriu del canvi de base,
tenim <span class="math inline">\([q_1]=\cals^T [q_2] \cals\)</span>, on <span class="math inline">\([q_1]\)</span> i <span class="math inline">\([q_2]\)</span> són les matrius
simètriques corresponents a les formes bilineals <span class="math inline">\(q_1\)</span> i <span class="math inline">\(q_2\)</span>
respectivament:
<span class="math display">\[
\begin{pmatrix} 0 &amp; 1/2 \\ 1/2 &amp; 0 \end{pmatrix}=
\begin{pmatrix} 1/2 &amp; 1/2 \\ 1/2 &amp; -1/2 \end{pmatrix}^T
\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{pmatrix}
\begin{pmatrix} 1/2 &amp; 1/2 \\ 1/2 &amp; -1/2 \end{pmatrix}
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-261" class="definition"><strong>Definició 4.11  </strong></span>Diem que dues formes quadràtiques <span class="math inline">\(q_1,q_2\colon \K^n\to \K\)</span> <em>són
equivalents</em>, amb matrius si existeix un canvi de base <span class="math inline">\(\cals\)</span> a <span class="math inline">\(\K^n\)</span>
tal que
<span class="math display">\[
[q_1]=\cals^T [q_2] \cals.
\]</span>
Escriurem <span class="math inline">\(q_1\sim q_2\)</span>.</p>
</div>
<p>Com que els canvis de base afecten de la mateixa manera que afectaven a
les formes bilineals (veure Lema
<a href="ortogonalitat.html#lem:canvi-base-forma-bil">4.6</a>), obtenim que “<em>ser equivalent
a</em>” també es una relació d’equivalència a les formes quadràtiques i
l’anàleg al Teorema
<a href="ortogonalitat.html#thm:class-formbilR">4.10</a>:</p>
<div class="theorem">
<p><span id="thm:class-formQuadR" class="theorem"><strong>Teorema 4.11  </strong></span>Tota forma
quadràtica <span class="math inline">\(q\colon \R^n\to \R\)</span> és equivalent a una forma bilineal del
tipus:</p>
<p><span class="math display">\[
q_1(x_1,\dots, x_n)=x_1^2+ \cdots + x_r^2 - x_{r+1}^2-\cdots -x_{r+s}^2 .
\]</span></p>
<p>A més, si definim la parella <span class="math inline">\((r,s)\)</span> com la <em>signatura de <span class="math inline">\(q\)</span></em>
(<span class="math inline">\(\sign(q)\)</span>), dues formes quadràtiques <span class="math inline">\(q_1,q_2\colon \R^n\to \R\)</span> són
equivalents si i només si <span class="math inline">\(\sign(q_1)=\sign(q_2)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-262" class="example"><strong>Exemple 4.18  </strong></span>Classifiquem la forma quadràtica <span class="math inline">\(q\colon \R^4 \to \R\)</span>:</p>
<p><span class="math display">\[
q(x_1,x_2,x_3,x_4)=x_1^2+10 x_1x_2 -2 x_1x_3+6x_1x_4 + x_2^2 -6x_2x_3-2x_2x_4+x_3^3+10x_3x_4-x_4^2.
\]</span></p>
<p>Que té per matriu:
<span class="math display">\[
\left(\begin{array}{rrrr}
1 &amp; 5 &amp; -1 &amp; 3 \\
5 &amp; 1 &amp; 3 &amp; -1 \\
-1 &amp; 3 &amp; 1 &amp; 5 \\
3 &amp; -1 &amp; 5 &amp; 1
\end{array}\right)
\]</span>
Per tant, podem aprofitar els càlculs de l’Exemple
<a href="ortogonalitat.html#exm:class-form-bil">4.15</a>, i tenim que és equivalent a:</p>
<p><span class="math display">\[
q(y_1,y_2,y_3,y_4)=y_1^2+y_2^2-y_3^2.
\]</span></p>
</div>
<div id="exercicis-recomanats-3" class="section level3 hasAnchor" number="4.10.1">
<h3><span class="header-section-number">4.10.1</span> Exercicis recomanats<a href="ortogonalitat.html#exercicis-recomanats-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Els exercicis que segueixen són útils per practicar el material
presentat. La numeració és la de <span class="citation">[<a href="#ref-Bret" role="doc-biblioref">1</a>]</span>.</p>
<dl>
<dt>Secció 5.1:</dt>
<dd>
<p>12, 16, 18.</p>
</dd>
<dt>Secció 5.2:</dt>
<dd>
<p>14, 34, 38.</p>
</dd>
<dt>Secció 5.3:</dt>
<dd>
<p>5-11, 13-20, 32.</p>
</dd>
<dt>Secció 5.4:</dt>
<dd>
<p>2, 8, 10.</p>
</dd>
<dt>Secció 5.5:</dt>
<dd>
<p>4, 10, 16, 22.</p>
</dd>
<dt>Secció 8.1:</dt>
<dd>
<p>6, 12, 16, 22.</p>
</dd>
<dt>Secció 8.2:</dt>
<dd>
<p>4, 10, 18, 22.</p>
</dd>
<dt>Secció 8.3:</dt>
<dd>
<p>6, 16, 18, 20.</p>
</dd>
</dl>

<p></p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-Bret" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Bretscher O (1997) Linear algebra with applications. Prentice Hall Eaglewood Cliffs, NJ</div>
</div>
<div id="ref-NaXa" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Nart E, Xarles X (2016) Apunts d’<span>à</span>lgebra linial. Servei de Publicacions de la Universitat Aut<span>ò</span>noma de Barcelona</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="diagonalització.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliografia.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": true,
"all": {}
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mmasdeu/algebralineal/edit/main/src//4_Ortogonalitat.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["algebralineal.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
