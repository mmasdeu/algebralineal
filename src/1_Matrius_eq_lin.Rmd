# Matrius i equacions lineals
El contingut d'aquesta secció el podem trobar a [@Bret Tema 1] i a
[@NaXa Tema 2].

## Matrius

:::{.definition}
Si $m$ i $n$ són dos nombres naturals, una *matriu $m\times n$ amb
entrades a $\K$* és una taula rectangular d'elements de $\K$ amb $m$
files i $n$ columnes. Denotem $M_{m\times n}(\K)$ al conjunt de matrius
que tenen $m$ files i $n$ columnes i els seus elements són de $\K$.
:::

:::{.notation}
Denotarem amb lletres majúscules el nom de les matrius i amb la mateixa
lletra i subíndexs cadascun dels coeficients: si $A$ és una matriu,
anomenarem $a_{ij}$ al nombre de la fila $i$, columna $j$. En el
producte de matrius, a vegades utilitzarem la notació $(AB)_{ij}$ per a
fer referència al coeficient de la posició $(i,j)$ després de fer el
producte.
:::

:::{.example}
Si $A=\big(\begin{smallmatrix}
	1 & 3 & 0 \\ 0 & -1 & 1
	\end{smallmatrix}\big) \in M_{2\times3}(\Q)$, llavors $a_{11}=1$,
$a_{12}=3$, $a_{13}=0$, $a_{21}=0$, $a_{22}=-1$ i $a_{23}=1$.
:::

A continuació fixem algunes notacions i definicions de casos
particulars:

-   Una *matriu quadrada* és una matriu amb el nombre de columnes igual
    al nombre de files. Denotarem per $M_n(\K)=M_{n\times n}(\K)$.

-   Un element està a la *diagonal d'una matriu quadrada* si la posició
    que ocupa té el mateix nombre de fila que de columna: si la matriu
    és $A$, els elements de la diagonal són els $a_{ii}$.

-   Una *matriu diagonal* és una matriu quadrada on els únics elements
    no nuls estan a la diagonal: $A$, matriu quadrada, és diagonal si
    $a_{ij}=0$ $\forall i\neq j$.

-   La *matriu identitat $n\times n$* és una matriu diagonal on tots els
    elements de la diagonal valen $1$ (i per tant els altres valen $0$).
    La denotem per $\1_n$ la matriu identitat $n\times n$.

-   En general, escriurem els vectors per columnes: un *vector de
    $\K^n$* és una matriu amb $n$ files i $1$ columna.

-   Direm que una matriu quadrada $A$ és *triangular superior* si tots
    els coeficients per sota de la diagonal valen $0$, o sigui,
    $a_ {ij}=0$ si $i>j$.

-   Direm que una matriu quadrada $A$ és *triangular inferior* si tots
    els coeficients per sobre de la diagonal valen $0$, o sigui,
    $a_ {ij}=0$ si $i<j$.

-   Donada una matriu $A \in M_{m\times n}(\K)$, definim la *transposada
    d'$A$* i la denotem per $A^T$ com la matriu de $M_{n\times m}(\K)$
    que té per columnes les files d'$A$, o sigui, que a la posició
    $(i,j)$ té el coeficient $a_{ji}$. Tenim la propietat:
    $$(A^T)^T=A \,.$$

-   Diem que una matriu $A$ és *simètrica* si $A=A^T$ (en particular, ha
    de ser quadrada). Per exemple, la matriu identitat $\1_n$ és
    simètrica.

:::{.example}
Considerem un sistema d'equacions amb $m$ equacions i $n$ incògnites:
\begin{align*}
	a_{11}x_1+a_{12}x_2+ \cdots + a_{1n}x_n &= b_1 \\
	a_{21}x_1+a_{22}x_2+ \cdots + a_{2n}x_n &= b_2 \\
	&\vdots \\
	a_{m1}x_1+a_{m2}x_2+ \cdots + a_{mn}x_n &= b_m
\end{align*}

D'aquí podem treure la matriu associada als coeficients del sistema:
$$A=
	\begin{pmatrix}
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} 
	\end{pmatrix}$$ el vector de termes independents: $$B=
	\begin{pmatrix}
	b_1 \\ b_2 \\ \vdots \\ b_m
	\end{pmatrix}$$ o bé escriure-ho tot en una sola matriu (matriu
ampliada), on habitualment separem els termes independents:
$$\begin{amatrix}{4}
	a_{11} & a_{12} & \cdots & a_{1n} & b_1 \\
	a_{21} & a_{22} & \cdots & a_{2n} & b_2 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} & b_m 
	\end{amatrix}$$
:::

## Operacions amb matrius. Matriu invertible {#subsec:opmat}
 Considerem $\lambda \in \K$ i
$A,B \in M_{m\times n}(\K)$. Les primeres operacions que podem fer són
les que corresponen a veure $M_{m\times n} (\K)$ com $\K$-espai
vectorial (més endavant veurem què vol dir):

-   Definim la matriu $\lambda A$ com la matriu que té per coeficients
    $\lambda a_{ij}$.

-   Definim la matriu $A+B$ com la matriu que té per coeficients
    $a_{ij}+b_{ij}$.

:::{.example}
$$\begin{pmatrix} -1 & 1 & 0 \\ 1 & 2 & 1 \end{pmatrix} +
	\begin{pmatrix} 0 & -1 & 2 \\ -1 & 3 & 2 \end{pmatrix} =
	\begin{pmatrix} -1 & 0 & 2 \\ 0 & 5 & 3 \end{pmatrix}$$
:::

:::{.example}
$$2 \begin{pmatrix} -1 & 1 & 0 \\ 1 & 2 & 1 \end{pmatrix} =
	\begin{pmatrix} -2 & 2 & 0 \\ 2 & 4 & 2 \end{pmatrix}$$
:::

Aquestes definicions, més les propietats dels elements de $\K$
impliquen:

1.  Existeix una matriu $\0_{mn}$ complint $A+\0_{mn}=A$,
    $\forall A \in M_{m\times n}(\K)$ ($\0_{mn}$ té tots els coeficients
    zero).

2.  $0 A = \0_{mn}$, $\forall A \in M_{m\times n}(\K)$.

3.  $A+B=B+A$, $\forall A,B \in M_{m\times n}(\K)$.

4.  $1A=A$, $\forall A \in M_{m\times n}(\K)$.

5.  $\lambda (\mu A)= (\lambda \mu) A$, $\forall \lambda,\mu \in \K$ i
    $\forall A \in M_{m\times n}(\K)$

Abans de la definició del producte de matrius és convenient utilitzar el
llenguatge següent:

::: {.definition #dependlineal}
 Diem que *un vector
$\vec{w}$ és combinació lineal de $\{\vec{v}_1,\dots,\vec{v}_n\}$* si
existeixen escalars $\lambda_1, \dots , \lambda_n$ tals que
$w=\lambda_1\vec{v}_1+\cdots+\lambda_n\vec{v}_n$.

Si un dels vectors de $\{\vec{v}_1,\dots,\vec{v}_n\}$ es pot escriure
com a combinació lineal dels altres, diem que *la família de vectors
$\{\vec{v}_1,\dots,\vec{v}_n\}$ és linealment dependent*.

En cas contrari, diem que *la família de vectors
$\{\vec{v}_1,\dots,\vec{v}_n\}$ és linealment independent*.
:::

Parlarem de files (o columnes) linealment dependents o independents
d'una matriu $A$ pensades com a vectors amb tantes components com
columnes (o files) tingui $A$.

A més, a més, podem definir:

:::{.definition}
Si $A \in M_{m\times n}(\K)$, $B \in M_{n\times r}(\K)$ (o sigui, el
nombre de columnes de $A$ és igual al nombre de files de $B$) podem
definir el *producte $AB$* com la matriu $C\in M_{m\times r}$ que té per
coeficients: $$c_{ij}=\sum_{k=1}^{n} a_{ik}b_{kj} \,.$$ La matriu $C$ és
pot pensar que té per columnes combinacions lineals de columnes de $A$
($B$ ens diu quines són aquestes combinacions lineals): la columna $j$
de la matriu $C$ és: $$\begin{pmatrix}
	c_{1j}\\c_{2j}\\ \vdots \\ c_{mj} 
	\end{pmatrix} =
	b_{1j}	\begin{pmatrix}
	a_{11}\\a_{21}\\ \vdots \\ a_{m1} 
	\end{pmatrix} +
	b_{2j}	\begin{pmatrix}
	a_{12}\\a_{22}\\ \vdots \\ a_{m2} 
	\end{pmatrix} + \cdots +
	b_{nj}	\begin{pmatrix}
	a_{1n}\\a_{2n}\\ \vdots \\ a_{mn} 
	\end{pmatrix}$$ Anàlogament, la matriu $C$ es pot pensar que té per
files combinacions lineals de files de $B$ ($A$ ens diu quines són
aquestes combinacions lineals): la fila $i$ de la matriu $C$ és (separem
amb comes per a que quedi més clar): $$(c_{i1},c_{i2},\cdots,c_{ir})=
	a_{i1} (b_{11},b_{12},\dots,b_{1r})+
	a_{i2} (b_{21},b_{22},\dots,b_{2r})+ \cdots +
	a_{in} (b_{n1},b_{n2},\dots,b_{nr})$$
:::

::: {.example #prodmat}

$$\begin{pmatrix} -1 & 1 & 0 \\ 1 & 2 & 1 \end{pmatrix}
	\begin{pmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{pmatrix}
	= 	\begin{pmatrix} 2 & 2 \\ 12 & 16 \end{pmatrix}$$
$$\begin{pmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{pmatrix}
	\begin{pmatrix} -1 & 1 & 0 \\ 1 & 2 & 1 \end{pmatrix}
	= 	\begin{pmatrix} 1 & 5 & 2 \\ 1 & 11 & 4 \\ 1 & 17 & 6 \end{pmatrix}$$
:::

:::{.example}
Podem escriure el sistema d'equacions \begin{align*}
	a_{11}x_1+a_{12}x_2+ \cdots + a_{1n}x_n &= b_1 \\
	a_{21}x_1+a_{22}x_2+ \cdots + a_{2n}x_n &= b_2 \\
	&\vdots \\
	a_{m1}x_1+a_{m2}x_2+ \cdots + a_{mn}x_n &= b_m
\end{align*} com $AX=B$, on $$A=
	\begin{pmatrix}
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} 
	\end{pmatrix},\quad
	X=
	\begin{pmatrix}
	x_1 \\ x_2 \\ \vdots \\ x_n
	\end{pmatrix} \text{ i}\quad
	B=
	\begin{pmatrix}
	b_1 \\ b_2 \\ \vdots \\ b_m
	\end{pmatrix}.$$
:::

:::{.proposition}
El producte de matrius té les propietats següents:

1.  Element neutre: si $A \in M_{m\times n}(\K)$, $\1_m A = A \1_n = A$.

2.  Propietat associativa: si $A \in M_{m\times n}(\K)$,
    $B \in M_{n\times r}(\K)$, $C \in M_{r\times s}(\K)$, llavors
    $$(AB)C=A(BC).$$

3.  Distributiva respecte el producte: si $A \in M_{m\times n}(\K)$,
    $B, C \in M_{n\times r}(\K)$ i $D \in M_{r\times s}(\K)$, llavors
    $A(B+C)=AB+AC$ i $(B+C)D=BD+CD$.
:::

::: {.proof}
Escriure les fórmules amb els coeficients i surt. Fem la
propietat associativa com exemple: volem comparar el coeficient a la
posició $(i,j)$ d'$(AB)C$ amb el d'$A(BC)$, que denotem $((AB)C)_{ij}$ i
$(A(BC))_{ij}$ respectivament:
$$((AB)C)_{ij}=\sum_{k=1}^r(AB)_{ik}c_{kj}=\sum_{k=1}^r(\sum_{l=1}^na_{il}b_{lk})c_{kj}=\sum_{k=1}^r\sum_{l=1}^na_{il}b_{lk}c_{kj}$$
Mentre que:
$$(A(BC))_{ij}=\sum_{l=1}^na_{il}(BC)_{lj}=\sum_{l=1}^na_{il}(\sum_{k=1}^rb_{lk}c_{kj})=\sum_{l=1}^n\sum_{k=1}^ra_{il}b_{lk}c_{kj}$$
I els dos resultats són el mateix ja que podem commutar els sumatoris.
:::

:::{.remark}
El producte de matrius, en general, no és commutatiu (veure l'Exemple
 \@ref(exm:prodmat)).
:::

:::{.proposition}
Si $A\in M_{m\times n}(\K)$ i $B\in M_{n\times r}(\K)$, llavors tenim la
relació següent entre productes i transposades $$(AB)^T=B^T A^T$$
:::

::: {.proof}
Escrivim les fórmules dels coeficients:
$$((AB)^T)_{ij}=(AB)_{ji}=\sum_{k=1}^n a_{jk}b_{ki}$$ mentre que
$$(B^TA^T)_{ij}=\sum_{k=1}^n (B^T)_{ik}(A^T)_{kj}=\sum_{k=1}^n b_{ki}a_{jk}$$
i són iguals per la propietat commutativa del producte a $\K$.
:::

:::{.definition}
Si considerem $\vec{v}$ i $\vec{w}$ vectors de $\K^n$, que escrivim com
una columna cadascun, definim el *producte escalar
$\vec{v}\cdot\vec{w}$* com: $$\vec{v}\cdot \vec{w}=\vec{v}^T \vec{w}$$
Per tant, si les coordenades són: $$\vec{v}=\begin{pmatrix}
	v_1 \\ v_2 \\ \vdots \\ v_n 
	\end{pmatrix}, 
	\vec{w}=\begin{pmatrix}
	w_1 \\ w_2 \\ \vdots \\ w_n 
	\end{pmatrix}
	\text{ llavors }
	\vec{v}\cdot\vec{w}=\sum_{i=1}^n v_iw_i \,.$$
:::

Volem definir la inversa d'una matriu quadrada, però com que el producte
no és commutatiu, hauríem de parlar d'inversa per l'esquerra o per la
dreta.

:::{.definition}
Diem que una *matriu quadrada $A \in M_{n}(\K)$ és invertible* si
existeix una matriu $B \in M_n(\K)$ tal que $AB=BA=\1_n$.\
:::

::: {.theorem #invuniq}
 Siguin $A$ i $B$ matrius quadrades
$n\times n$. Llavors $AB=\1_n$ si i només si $BA=\1_n$. En particular,
si això passa aleshores $A$ és invertible i la inversa, que és única, la
denotarem $A^{-1}$.
:::

::: {.proof}
La primera part la demostrarem quan tinguem el concepte de rang
d'una matriu (Corol·lari
 \@ref(cor:invuniq)). Vegem ara que si $B$ és tal que $AB=\1_n$, i
$C$ tal que $CA=\1_n$, llavors $B=C$:
$$C = C \1_n = C(AB)=(CA)B=\1_n B=B \,.$$ Això implica que la inversa és
única: Suposem $B'$ tal que $AB'=\1_n$, llavors, pel raonament d'abans,
$B'=C=B$.
:::

:::{.proposition}
1.  Si $A \in M_{n\times n}(\K)$ és invertible, llavors $A^T$ també ho
    és i $(A^T)^{-1}=(A^{-1})^T$.

2.  Si $A,B \in M_{n\times n}(\K)$ són matrius invertibles, llavors el
    producte $AB$ també ho és i $(AB)^{-1}=B^{-1}A^{-1}$.
:::

::: {.proof}
Demostrem primer (a): com que ens proposen una inversa, tant
sols cal comprovar que ho és:
$$(A^T)(A^{-1})^T=(A^{-1}A)^T=\1_n^T=\1_n \,.$$ per tant, $(A^{-1})^T$
és inversa d'$A^T$ (com que $A^T$ és quadrada, pel Teorema
 \@ref(thm:invuniq), tant sols cal comprovar-ho per un dels
costats).

Demostrem ara (b): com que $A$ i $B$ són invertibles, existeixen les
matrius inverses $A^{-1}$ i $B^{-1}\in M_{n\times n}(\K)$
respectivament. Llavors:
$$(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=A\1_n A^{-1}=A A^{-1}=\1_n$$ Igual
que (a), això ja demostra que $B^{-1}A^{-1}$ és inversa d'$AB$.
:::

:::{.remark}
D'aquí es dedueix que si $A_1, \dots, A_r \in M_{n\times n}(\K)$ són
invertibles, el seu producte també ho és i
$(A_1\cdots A_r)^{-1}=A_r^{-1}\cdots A_1^{-1}$.
:::

## Transformacions elementals en matrius {#subsec:trans-el}

Considerem una matriu organitzada per files (encara que tot el que farem
aquí també es pot fer per columnes) i definim les *transformacions
elementals*:

::: {.center}
:::

:::{.remark}
Si considerem la matriu d'un sistema d'equacions i apliquem qualsevol de
les transformacions elementals, no modifiquem les solucions.
:::

:::{.remark}
Les tres transformacions elementals es poden desfer mitjançant una
transformació elemental:

1.  Multiplicar una de les files per $1/\lambda\neq 0$.

2.  Sumar a una de les files $-\mu$ vegades una altra fila.

3.  Intercanviar dues files.
:::

Aquestes transformacions elementals es poden fer (i desfer) multiplicant
per una matriu invertible $P$ per l'esquerra. Suposem que $A$ és una
matriu amb $m$ files:

1.  si apliquem aquest canvi a la fila $i$ d'$\1_m$ tindrem una matriu
    $P$ on hem modificat l'$1$ de la fila $i$ per $\lambda\neq 0$.
    Llavors $PA$ té els mateixos valors que $A$, però amb la fila $i$
    multiplicada per $\lambda$. En aquest cas, $P^{-1}$ és una matriu
    identitat amb un $1/\lambda$ a la posició $(i,i)$.

2.  si sumem a la fila $i$ de la matriu $\1_n$ $\mu$ vegades la fila
    $k\neq i$ tindrem una matriu $P$ tal que $PA$ té a la fila $i$ la
    fila $i$ d'$A$ més $\mu$ vegades la fila $k$ d'$A$. En aquest cas,
    $P^{-1}$ és una matriu amb $1$ a la diagonal, $0$ fora, excepte la
    posició $(k,i)$, que val $-\mu$.

3.  si intercanviem les files $i$ i $k$ de la matriu $\1_n$ ($i\neq k$)
    obtenim una matriu $P$ tal que $PA$ és el resultat d'intercanviar
    les files $i$ i $k$ d'$A$. En aquest cas, $P^{-1}=P$.

Amb aquests raonaments hem demostrat:

:::{.proposition}
Si considerem la matriu identitat $\1_m$ i li apliquem transformacions
elementals per files, la matriu $P$ que obtenim és invertible. Si
apliquem exactament les mateixes transformacions elementals a una altra
matriu $A\in M_{m\times n}(\K)$, la matriu que resulta és exactament
$PA$.
:::

:::{.definition}
Diem que *dues matrius $A$ i $B$ són equivalents per files* si es pot
passar d'$A$ a $B$ mitjançant transformacions elementals per files.
Escriurem $A \sim B$.
:::

::: {.proposition #relequiv}
 Si $A$, $B$ i $C$ són matrius
de dimensions iguals, aleshores es té:

-   $A \sim A$ (reflexiva),

-   $A \sim B$ si i només si $B \sim A$ (simètrica),

-   $A \sim B$ i $B \sim C$ implica $A \sim C$ (transitiva).
:::

::: {.proof}
La primera és **T2** per a qualsevol fila i $\mu=0$.

La segona és que la inversa d'una transformació elemental és una
transformació elemental (i les composem en ordre invers): si $P_1$,
...$P_r$ són les transformacions elementals que apliquem a $A$ per
obtenir $B$, resulta que: $$P_r \cdots P_1 A=B$$ llavors
$$A= P_1^{-1} \cdots P_r^{-1} B$$ Però si $P_i$ és una transformació
elemental, $P_i^{-1}$ també i per tant $B\sim A$.

La tercera és per definició de $\sim$, ja que passem de $A$ a $C$ fent
primer els canvis elementals que transformen $A$ en $B$ i després els
que transformen $B$ en $C$.
:::

:::{.remark}
Si tenim un conjunt $S$ i una relació $\sim$ entre els seus elements
complint les propietats de la Proposició
 \@ref(prp:relequiv) (reflexiva, simètrica i transitiva) diem que
és una *relació d'equivalència*.
:::

:::{.definition}
Diem que una matriu $A$ està *en forma esglaonada (per files)* si
compleix que:

1.  El primer element no nul de cada fila val $1$, i l'anomenem *pivot*.

2.  Si una fila conté un pivot a la columna $j$, les files superiors
    també tenen un pivot a una columna $j'<j$.

3.  Totes les entrades per sota d'un pivot valen $0$.

Diem que una matriu $A$ està *en forma reduïda (per files)* si està
esglaonada i a més compleix que:

1.  Si una columna té un pivot, aquest és l'únic element no nul de la
    seva columna.
:::

:::{.example}
De les matrius següents: $$A=\begin{pmatrix}
	1 & 2 & 0 \\ 0 & 0 & 1
	\end{pmatrix},
	B=\begin{pmatrix}
	1 & 0 & 1 \\ 0 & 0 & 1
	\end{pmatrix},
	C=\begin{pmatrix}
	0 & 1 & 0 \\ 1 & 0 & 1
	\end{pmatrix} \text{ i }
	D = \begin{pmatrix}
	1&2&3\\
	0&1&5
	\end{pmatrix}$$ les matrius $A$, $B$ i $D$ estan esglaonades per files
(i la matriu $C$ no). De les matrius esglaonades per files, $A$ està en
forma reduïda i les altres no: per sobre del segon pivot no hi ha zero.
:::

Considerem ara l'algorisme següent (**Mètode de Gauss** o **Mètode de
Gauss-Jordan**), que aplica canvis elementals per files a una matriu $A$
fins que obtenim una matriu en forma reduïda per files. A aquest mètode
també li diem **triangular la matriu per files**.

::: {.center}
:::

Com que la matriu té un nombre finit de files, aquest algorisme sempre
acaba. A més, acabem de demostrar que:

:::{.theorem}
Tota matriu $A\in M_{m\times n}(\K)$ és equivalent a una matriu reduïda.
:::

:::{.example}
Considerem la matriu: $$A=\begin{pmatrix}
	0 & 1 & 3 & 0 \\  1 & -2 & -5 & 4\\ 2 & -3 & -7 & 7
	\end{pmatrix}$$ Apliquem el canvi **T3**, canviant la primera fila per
la segona, obtenint: $$\begin{pmatrix}
	1 & -2 & -5 & 4\\0 & 1 & 3 & 0 \\   2 & -3 & -7 & 7
	\end{pmatrix}$$ Apliquem el canvi **T2**, restant a la tercera fila $2$
cops la primera: $$\begin{pmatrix}
	1 & -2 & -5 & 4\\0 & 1 & 3 & 0 \\   0 & 1 & 3 & -1
	\end{pmatrix}$$ Com que a la segona fila, la primera posició ja és $1$,
podem utilitzar-la directament de pivot i sumar-la $2$ cops a la primera
fila, i restar-la a la tercera: $$\begin{pmatrix}
	1 & 0 & 1 & 4\\0 & 1 & 3 & 0 \\   0 & 0 & 0 & -1
	\end{pmatrix}$$ Les dues primeres files ja estan en forma reduïda, pel
que podem considerar la tercera fila, i multiplicar-la per $-1$ per a
que l'única posició no nua sigui un pivot: $$\begin{pmatrix}
	1 & 0 & 1 & 4\\0 & 1 & 3 & 0 \\   0 & 0 & 0 & 1
	\end{pmatrix}$$ Finalment restem la tercera fila a la primera
multiplicada per $4$: $$\begin{pmatrix}
	1 & 0 & 1 & 0\\0 & 1 & 3 & 0 \\   0 & 0 & 0 & 1
	\end{pmatrix}$$
:::

## Criteri d'invertibilitat. Rang d'una matriu

Considerem primer el resultat següent:

::: {.theorem #critinv}
 Donada una matriu quadrada
$A \in M_{n\times n}(\K)$, les condicions següents són equivalents:

1.  $A$ és equivalent a $\1_n$.

2.  $A$ és invertible.
:::

::: {.proof}
Vegem primer (a) implica (b): si $A$ és equivalent a $\1_n$,
existeix $P$ tal que $PA=\1_n$. Com que $P$ és invertible, existeix
$P^{-1}$ i podem fer:
$$PA=\1_n \Rightarrow P ^{-1}PA=P^{-1} \Rightarrow A=P^{-1} \Rightarrow AP = P^{-1}P=\1_n$$
per tant $P$ és la inversa d'$A$.

Vegem (b) implica (a): suposem que $A$ no és equivalent a $\1_n$.
Llavors, forçosament, quan l'esglaonem hi haurà una fila sense pivot, i
per tant, tot zeros. Llavors obtenim $PA=A'$, amb $P$ invertible i $A'$
una matriu que té l'última fila tot zeros. Si $A$ fos invertible,
voldria dir que existeix $Q$ tal que $AQ=\1_n$, llavors, però llavors:
$$AQ=\1_n \Rightarrow PAQ=P \Rightarrow A'Q=P \Rightarrow A'(QP^{-1})=\1_n$$
Observem ara que si $A'$ té l'última fila tot zeros $A'(QP^{-1})$ també,
i per tant no pot ser $\1_n$.
:::

::: {.remark #noinv}
 Si considerem $A \in M_{n\times n}(\K)$
que es pot subdividir en 4 submatrius: $$A=\left(\begin{array}{c|c}
	B & C \\ \hline \0 & D
	\end{array}\right)$$ on la matriu $\0$ està formada per zeros i toca la
diagonal (conté un coeficient amb coordenades $(i,i)$), llavors $A$ no
és invertible.
:::

::: {.proof}
Primer observem que per a que una matriu sigui invertible,
l'esglaonament ha de fer que a totes les seves columnes (i files) hi
hagi un pivot (es dedueix del Teorema
 \@ref(thm:critinv)).

Si fos invertible podríem fer els canvis elementals per files i obtenir
$\1_n$. Com que la submatriu $\0$ toca la diagonal, llavors $B$ té més
columnes que files i $D$ més files que columnes. Quan esglaonem la
matriu $A$, el pivot de les primeres columnes ha de ser a $B$ (sota hi
ha zeros) i per tant no hi pot haver més pivots que files a $B$ (a les
columnes de $B$), per tant hi ha columnes de $A$ sense pivot, i per tant
no pot ser invertible.
:::

Podem utilitzar el concepte de matriu reduïda equivalent per a definir
el rang d'una matriu $A$. Cal tenir en compte que necessitarem demostrar
que la definició no depèn de quina matriu reduïda equivalent a $A$
considerem.

:::{.proposition}
Si $A$ i $B$ són dues matrius en forma reduïda que són equivalents,
llavors $A=B$.
:::

::: {.proof}
Suposem que $A$ i $B$ són diferents. Considerem submatrius $A'$
i $B'$ formades per la primera columna on difereixin, així com per totes
les columnes a l'esquerra d'aquesta que continguin pivots. Observem que
$A'$ i $B'$ segueixen essent equivalents, mitjançant les mateixes
transformacions elementals associades a $A$ i $B$. Podem interpretar
$A'$ i $B'$ com matrius augmentades de sistemes lineals equivalents,
posem en $r$ incògnites. Si aquests són compatibles, aleshores
necessàriament els termes independents han de coincidir i, per tant
obtenim una contradicció. Si aquests dos sistemes són incompatibles,
això significa que l'última columna conté zeros en les primeres files
(tantes com pivots) i necessàriament tindrem que aquesta última columna
tindrà un pivot a l'entrada $r$-èssima. Per tant, obtenim una nova
contradicció.
:::

:::{.notation}
D'aquí és dedueix que donada una matriu $A\in M_{n\times n}(\K)$,
existeix una sola matriu reduïda equivalent a $A$, i l'anomenem
$\rref(A)$ (*reduced row-echelon form*) .
:::

Ara ja podem definir el rang d'una matriu:

:::{.definition}
Donada una matriu $A$, definim el *rang d'$A$* com el nombre de files
diferents de zero (igual al nombre de pivots) de $\rref(A)$.
:::

:::{.corollary}
Si fem transformacions elementals a una matriu $A\in M_{m\times n}(\K)$,
el rang de la matriu resultant és el mateix. Això també ho podem
enunciar dient que si $P\in M_{m\times m}(\K)$ és una matriu invertible
i $A\in M_{m\times n}(\K)$, llavors $\Rang(A)=\Rang(PA)$.
:::

:::{.corollary}
Una matriu quadrada $A\in M_{n\times n}(\K)$ és invertible si i només si
té rang $n$.
:::

::: {.proof}
Considerem el Teorema
 \@ref(thm:critinv), i per tant és invertible si i només si és
equivalent a $\1_n$, per tant, si i només si té rang $n$.
:::

El Mètode de Gauss ens dona una manera de calcular la inversa d'una
matriu: suposem que ens donen una matriu $A \in M_{n\times n}(\K)$.
Considerem la matriu formada per una matriu identitat $n\times n$ al
costat de la matriu $A$:
$$\left(\begin{array}{c|c}A & \1_n \end{array}\right) \in M_{n\times 2n}(\K)$$
Si apliquem canvis per files a aquesta matriu (cada fila té $2n$
coeficients) obtindrem matrius que podem escriure com:
$$\left(\begin{array}{c|c}A'& P \end{array}\right) \in M_{n\times 2n}(\K)$$
amb $A',P\in M_{n\times n}(\K)$ i que compleixen que $PA=A'$.

Com a cas particular tenim que, si $A$ és invertible, podem arribar a la
situació \begin{align*}
(\#eq:inversa)
\left(\begin{array}{c|c}\1_n & P \end{array}\right) \in M_{n\times 2n}(\K)
\end{align*} i tindrem que $P=A^{-1}$.

Si la matriu $A$ no fos invertible, el
Teorema \@ref(thm:critinv) ens diu que no seria possible arribar a la
situació de l'Equació
 \@ref(eq:inversa).

:::{.example}
Considerem la matriu: $$A=\begin{pmatrix}
	1 & 2 & 6 \\ 0 & -1 & -8 \\ 5 & 6 & 0
	\end{pmatrix}$$ Escrivim la matriu amb una còpia de la matriu identitat
a la dreta: $$\left(\begin{array}{rrr|rrr}
	1 & 2 & 6 & 1 & 0 & 0 \\
	0 & -1 & -8& 0 & 1 & 0 \\
	5 & 6 & 0 & 0 & 0 & 1
	\end{array}\right)$$ Esglaonem la matriu resultant segon el mètode de
Gauss: $$\left(\begin{array}{rrr|rrr}
	1 & 2 & 6 & 1 & 0 & 0\\
	0 & -1 & -8 & 0 & 1 & 0\\
	0 & -4 & -30 & -5 & 0 & 1
	\end{array}\right)
	\rightsquigarrow
	\left(\begin{array}{rrr|rrr}
	1 & 2 & 6 & 1 & 0 & 0\\
	0 & 1 & 8 & 0 & -1 & 0\\
	0 & -4 & -30 & -5 & 0 & 1
	\end{array}\right)$$ $$\left(\begin{array}{rrr|rrr}
	1 & 0 & -10  & 1 & 2 & 0\\
	0 &1 & 8 & 0 & -1 & 0\\
	0 & 0 & 2 & -5 & -4 & 1
	\end{array}\right)
	\rightsquigarrow
	\left(\begin{array}{rrr|rrr}
	1 & 0 & 6 & 1 & 0 & 0\\
	0 & 1 & 8 & 0 & -1 & 0\\
	0 & 0 & 1 & -5/2 & -2 & 1/2
	\end{array}\right)$$ $$\rightsquigarrow\left(\begin{array}{rrr|rrr}
	1 & 0 & 0 & -24 & -18 & 5\\
	0 & 1 & 0 & 20 & 15 & -4\\
	0 & 0 & 1 & -5/2 & -2 & 1/2
	\end{array}\right)$$ Per tant: $$A^{-1}=\begin{pmatrix}
	-24 & -18 & 5 \\
	20 & 15 & -4 \\
	-5/2 & -2 & 1/2
	\end{pmatrix}$$
:::

:::{.exercise}
Tot el que hem fet per files té el seu anàleg per columnes. Una manera
senzilla d'adaptar totes les definicions i resultats és dir que la
transposada compleix la definició per files. Per exemple:

> La matriu $A$ és *en forma reduïda per columnes* si $A^T$ és en forma
> reduïda per files.

(a) Enuncieu l'anàleg per columnes de cada resultat que s'ha vist a
    aquest capítol per files.

(b) Demostreu que $\Rang(A)=\Rang(A^T)$.
:::

:::{.exercise}
Considerem $A\in M_{m\times n}(\K)$ i hi afegim una fila, obtenint
$A'\in M_{(m+1)\times n}(\K)$. Demostreu $\Rang(A)=\Rang(A')$ si i només
si la fila que hem afegit és combinació lineal de les files d'$A$.
:::

:::{.exercise}
Demostreu que el rang d'una matriu $A$ és el nombre màxim de files (o
columnes) linealment independents que conté $A$.
:::

## Resolució de sistemes d'equacions lineals

Recordem la notació d'un sistema d'equacions: \begin{align*}
a_{11}x_1+a_{12}x_2+ \cdots + a_{1n}x_n &= b_1 \\
a_{21}x_1+a_{22}x_2+ \cdots + a_{2n}x_n &= b_2 \\
&\vdots \\
a_{m1}x_1+a_{m2}x_2+ \cdots + a_{mn}x_n &= b_m
\end{align*} Que també escrivim com $AX=B$, on $$A=
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} 
\end{pmatrix}\text{, }
X=
\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix} \text{ i }
B=
\begin{pmatrix}
b_1 \\ b_2 \\ \vdots \\ b_m
\end{pmatrix}$$ Volem esbrinar si el sistema té solució o no, i, en cas
de tenir-ne, saber quantes en té i calcular-les.

Una primera interpretació és considerar que tenim una solució $X$ i fer
el càlcul següent: $$\begin{pmatrix}
b_1 \\ b_2 \\ \vdots \\ b_m
\end{pmatrix} = B = AX =
x_1 \begin{pmatrix}
a_{11} \\ a_{21} \\ \vdots \\ a_{m1}
\end{pmatrix} +
x_2 \begin{pmatrix}
a_{12} \\ a_{22} \\ \vdots \\ a_{m2}
\end{pmatrix} + \cdots +
x_n \begin{pmatrix}
a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}
\end{pmatrix}$$ Per tant, una solució ens dóna $B$ com a combinació
lineal de les columnes d'$A$, i al revés: si podem escriure $B$ com a
combinació lineal de les columnes d'$A$, tenim una solució.

Argumentant amb el rang per columnes, ja tenim un criteri per saber si
un sistema d'equacions lineals té solució o no:

:::{.proposition}
El sistema d'equacions: \begin{align*}
	a_{11}x_1+a_{12}x_2+ \cdots + a_{1n}x_n &= b_1 \\
	a_{21}x_1+a_{22}x_2+ \cdots + a_{2n}x_n &= b_2 \\
	&\vdots \\
	a_{m1}x_1+a_{m2}x_2+ \cdots + a_{mn}x_n &= b_m
\end{align*} té solució si i només si: $$\Rang \begin{pmatrix}
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} 
	\end{pmatrix} =
	\Rang \begin{pmatrix}
	a_{11} & a_{12} & \cdots & a_{1n} & b_1 \\
	a_{21} & a_{22} & \cdots & a_{2n} & b_2 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} & b_m 
	\end{pmatrix}$$
:::

::: {.proof}
Denotem per $A$ la matriu associada al sistema i per
$\overline{A}$ la matriu ampliada. Esglaonem per files la matriu
$\overline{A}$ fins a tenir una matriu reduïda $P\overline{A}$ i fem
exactament les mateixes transformacions a la matriu $A$, obtenint $PA$.
Com que les primeres $n$ columnes d'$\overline{A}$ són precisament les
d'$A$, tenim que $PA$ també és una matriu en forma reduïda, i tants sols
difereixen de que $P\overline{A}$ té una columna més.

Si $\Rang(A)=\Rang(\overline{A})$, vol dir que a l'última columna de
$P\overline{A}$ no hi ha cap pivot, i per tant es pot escriure l'última
columna de $P\overline{A}$ com a combinació lineal de les $n$ primeres,
obtenint una solució del sistema.

Si $\Rang(A)\neq\Rang(\overline{A})$, per força
$\Rang(\overline{A})=\Rang(A)+1$ i l'última columna té un pivot. La fila
on hi ha el pivot de l'última columna correspon a l'equació $0=1$, que
no té solució.
:::

El resultat anterior també es pot enunciar dient que si esglaonem per
files la matriu ampliada del sistema, no queda cap fila on l'únic
element no nul sigui a la columna de termes independents.

Com que les transformacions elementals per files a la matriu ampliada no
modifiquen les solucions del sistema, suposem que hem esglaonat la
matriu inicial del sistema. Llavors, obtenint una matriu reduïda
$\overline{A}'$:

(a) El sistema té solució si i només si l'última columna
    d'$\overline{A}'$ no té cap pivot. Si el sistema té solució, diem
    que el *sistema és compatible*, i quan no té solució, diem que és un
    *sistema incompatible*.

(b) Suposem que el sistema té solució, llavors cada pivot ens permet
    aïllar la variable corresponent a la seva columna.

(c) Continuem suposant que el sistema té solució: les columnes (de la
    matriu sense ampliar) que no tenen pivot definiran el que anomenem
    *paràmetres lliures*. N'hi haurà tants com $k:=n-\Rang(A)$, on $n$
    és el número de incògnites. En aquest cas direm que *la solució té
    dimensió $k$*.\
    En el cas particular que $k=0$ diem que té solució única i diem que
    és un *sistema compatible determinat*.\
    Si $k>0$ diem que és un *sistema compatible indeterminat amb $k$
    paràmetres lliures*.

::: {.example #SCI}
 Considerem el sistema d'equacions:

\begin{align*}
	x - y + 2z + 3t &= 21 \\
	-x+2y+z+5t &= 26\\
	3x+y-2z+t &= -9\\
	3x+2y+z+9t &= 38
\end{align*}

Considerem la matriu ampliada i esglaonem: $$\begin{amatrix}{4}
	1 & -1 & 2 & 3 & 21 \\
	-1 & 2 & 1 & 5 & 26 \\
	3 & 1 & -2 & 1 & -9\\
	3 & 2 & 1 & 9 & 38
	\end{amatrix}
	\rightsquigarrow
	\begin{amatrix}{4}
	1 & -1 & 2 & 3 & 21 \\
	0 & 1 & 3 & 8 & 47 \\
	0 & 4 & -8 & -8 & -72\\
	0 & 5 & -5 & 0 & -25
	\end{amatrix}$$ $$\begin{amatrix}{4}
	1 & 0 & 5 & 11 & 68 \\
	0 & 1 & 3 & 8 & 47 \\
	0 & 0 & -20 & -40 & -260\\
	0 & 0 & -20 & -40 & -260
	\end{amatrix}\rightsquigarrow
	\begin{amatrix}{4}
	1 & 0 & 5 & 11 & 68 \\
	0 & 1 & 3 & 8 & 47 \\
	0 & 0 & -20 & -40 & -260\\
	0 & 0 & 0 & 0 & 0
	\end{amatrix}$$ $$\begin{amatrix}{4}
	1 & 0 & 5 & 11 & 68 \\
	0 & 1 & 3 & 8 & 47 \\
	0 & 0 & 1 & 2 & 13\\
	0 & 0 & 0 &  0 & 0
	\end{amatrix}
	\rightsquigarrow
	\begin{amatrix}{4}
	1 & 0 & 0 & 1 & 3 \\
	0 & 1 & 0 & 2 & 8 \\
	0 & 0 & 1 & 2 & 13\\
	0 & 0 & 0 &  0 & 0
	\end{amatrix}$$ Com que el rang de la matriu associada és 3 i el de
l'ampliada també, el sistema és compatible. Com que tenim 4 incògnites,
té 4-3=1 paràmetre lliures (per tant, sistema compatible indeterminat).
Amb l'esglaonament que hem fet, la $t$ és el paràmetre lliure i podem
escriure la solució: $$\begin{pmatrix}
	x \\ y \\ z \\ t
	\end{pmatrix} =
	\begin{pmatrix}
	3 \\ 8 \\ 13 \\ 0
	\end{pmatrix}
	+t
	\begin{pmatrix}
	-1 \\ -2 \\ -2 \\ \phantom{-}1
	\end{pmatrix} \text{ amb $t\in\K$.}$$
:::

:::{.example}
Per tal d'aprofitar els càlculs del sistema anterior, tant sols fem una
petita modificació a l'últim terme independent: \begin{align*}
	x - y + 2z + 3t &= 21 \\
	-x+2y+z+5t&=26\\
	3x+y-2z+t&=-9\\
	3x+2y+z+9t&=39
\end{align*} Considerem la matriu ampliada i esglaonem (són els
mateixos passos d'abans, pel que tant sols escrivim la primera i última
matriu): $$\begin{amatrix}{4}
	1 & -1 & 2 & 3 & 21 \\
	-1 & 2 & 1 & 5 & 26 \\
	3 & 1 & -2 & 1 & -9\\
	3 & 2 & 1 & 9 & 38
	\end{amatrix}
	\rightsquigarrow	
	\begin{amatrix}{4}
	1 & 0 & 0 & 1 & 3 \\
	0 & 1 & 0 & 2 & 8 \\
	0 & 0 & 1 & 2 & 13\\
	0 & 0 & 0 &  0 & 1
	\end{amatrix}$$ si som estrictes en el concepte de reduïda, falta
utilitzar l'$1$ de l'última fila per a posar zeros a l'última columna.
En qualsevol cas, veiem que la matriu ampliada té rang 4, mentre que
l'associada té rang 3, pel que el sistema és incompatible.
:::

Considerem ara el cas particular en que $B=\0_m$ (un vector format per
zeros).

:::{.definition}
(a) Diem que el sistema d'equacions lineals $AX=B$ *és homogeni* si
    $B=\0_m$.

(b) Si $AX=B$ és un sistema, parlem de *sistema homogeni associat* al
    sistema $AX=\0_m$.
:::

Tenim els resultats següents:

(a) Si el sistema és homogeni, llavors $X=\0_n$ és una solució, per tant
    el sistema és compatible.

(b) Si $X$ és solució del sistema homogeni i $\lambda\in\K$, llavors
    $\lambda X$ també és solució del sistema: si ho pensem com a
    multiplicació de matrius, tenim la igualtat
    $A(\lambda X)=\lambda (AX)=\lambda \0_m=\0_m$, per tant $\lambda X$
    també és solució.

(c) Si $X$ i $Y$ són solucions d'un sistema homogeni, llavors $X+Y$
    també és solució: tornem a escriure-ho en forma matricial:
    $A(X+Y)=AX+AY=\0_m+\0_m=\0_m$.

(d) Si $AX=B$ és un sistema, amb $X$ i $Y$ una solucions, llavors $X-Y$
    és una solució del sistema homogeni associat:
    $A(X-Y)=AX-AY=B-B=\0_m$.

(e) Si $AX=B$ és un sistema i $X$ una solució particular, qualsevol
    altre solució $Y$ es pot escriure com $Y=X+Z$, amb $Z$ solució del
    sistema homogeni associat: això es dedueix de l'apartat anterior: si
    $X$ i $Y$ són solucions, llavors $Z=Y-X$ és solució de l'homogeni.
    Si $X$ és solució del sistema i $Z$ de l'homogeni associat, llavors
    $AY=A(X+Z)=AX+AZ=B+\0_m=B$.

:::{.example}
Considerem el sistema d'equacions de l'Exemple
 \@ref(exm:SCI):
\begin{align*}
	x - y + 2z + 3t &= 21 \\
	-x+2y+z+5t&=26\\
	3x+y-2z+t&=-9\\
	3x+2y+z+9t&=38
\end{align*} Veiem que el sistema homogeni associat és
\begin{align*}
	x - y + 2z + 3t &= 0\\
	-x+2y+z+5t&=0\\
	3x+y-2z+t&=0\\
	3x+2y+z+9t&=0
\end{align*} i té per solució $$\begin{pmatrix}
	x \\ y \\ z \\ t
	\end{pmatrix} =
	t \begin{pmatrix}
	-1 \\ -2 \\ -2 \\ \phantom{-}1
	\end{pmatrix} \text{ amb $t\in\K$.}$$ Tenint en compte que una solució
particular és $(x,y,z)=(3,8,13)$, podem escriure: $$\begin{pmatrix}
	x \\ y \\ z \\ t
	\end{pmatrix} =
	\begin{pmatrix}
	3 \\ 8 \\ 13 \\ 0
	\end{pmatrix}+
	t \begin{pmatrix}
	-1 \\ -2 \\ -2 \\ \phantom{-}1
	\end{pmatrix} \text{ amb $t\in\K$.}$$
:::

## Exercicis recomanats 

Els exercicis que segueixen són útils per practicar el material
presentat. La numeració és la de [@Bret].

Secció 1.1:

:   6, 12, 16.

Secció 1.2:

:   10, 18.

Secció 1.3:

:   6, 8, 18, 20, 24, 28.
